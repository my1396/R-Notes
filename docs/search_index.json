[["index.html", "R Notes Chapter 1 About", " R Notes Menghan Yuan 2025-08-26 Chapter 1 About This is a sample book written in Markdown. You can use anything that Pandoc‚Äôs Markdown supports; for example, a math equation \\(a^2 + b^2 = c^2\\). "],["1.1-usage.html", "1.1 Usage", " 1.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. "],["1.2-render-book.html", "1.2 Render book", " 1.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select ‚ÄúAll formats‚Äù if you‚Äôd like to use multiple formats from the same book source files. Or build the book from the R console: bookdown::render_book() To render this example to PDF as a bookdown::pdf_book, you‚Äôll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. "],["1.3-preview-book.html", "1.3 Preview book", " 1.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in ‚ÄúPreview book‚Äù, or from the R console: bookdown::serve_book(dir = &quot;.&quot;, output_dir = &quot;_book&quot;, preview = TRUE, quiet = FALSE) You pass the root directory of the book to the¬†dir¬†argument, and this function will start a local web server so you can view the book output using the server. The default URL to access the book output is¬†http://127.0.0.1:4321. Depending on your IDE, the url will be opened in either internal or external web browser. The server will listen to changes in the book root directory: whenever you modify any files in the book directory,¬†serve_book()¬†can detect the changes, recompile the Rmd files, and refresh the web browser automatically. If you set preview = FALSE, the function will recompile the book, which can take a longer time. quiet = TRUE will suppress output (e.g., the knitting progress) in the console. Even if the compiling messages are distracting, they tell you the status of the book rendering, so you may want to keep the default anyway. To stop the server, run servr::daemon_stop(1) or restart your R session. bookdown::serve_book() is better than using live preview in VS Code because the html viewer does not blink continuously when you edit and save the .Rmd files. Besides, live preview do not refresh contents automatically. üí°¬†Tip:¬† Use¬†serve_book()¬†while working on your book to see live changes, then run¬†render_book()¬†once you are ready to publish. serve_book() and preview_chapter are slow as they recompile the entire book when you save any file. Suggested action: rmarkdown::render_site(\"onefile.Rmd\") to render the current active file. Go to docs/ and right-click onefile.html &gt; select Show Preview to open in browser. ‚úÖ The web preview will not update automatically (no flashing), so you need to re-render the file when you make changes. Bibliography # automatically create a bib database for R packages knitr::write_bib(c( .packages(), &#39;bookdown&#39;, &#39;knitr&#39;, &#39;rmarkdown&#39; ), &#39;packages.bib&#39;) "],["2-rstudio.html", "Chapter 2 RStudio", " Chapter 2 RStudio RStudio shortcuts Command Palette: shift+cmd+P, all shortcuts can be accessed via the Command Palette. keyboard combination function opt + _ insert assignment operator &lt;- ESC or ctrl + C exit + prompt shift + cmd + M Add magrittr‚Äôs pipe operator ‚Äú%&gt;%‚ÄùAfter R4.1, you can set this too native pipe |&gt; ctrl + [/] indent or unindent cmd + D delete one row cmd + 1 move cursor to console window cmd + 2 move cursor to editor window ctrl + shift + S add 80 hyphens --- to signal a new chapter (Addin) ctrl + shift + = add 80 equals === to signal a new Chapter (Addin) shift + cmd +N new R script cmd + \\(\\uparrow\\) / \\(\\downarrow\\) in console, get a list of command history shift + \\(\\uparrow\\) / \\(\\downarrow\\) select one line up/down fn + F2 view() an object, don‚Äôt select the object cmd + shift + 1 activate X11() window ctrl (+ shift) + tab next (last) tab in scriptor (this applies to all apps); hit ctrl first, then shift if necessary, last tab Source keyboard combination function cmd + return Run current line/selection opt + return Run current line/selection (retain cursor position) Rmd related keyboard combination function cmd + shift + K Knit rmd cmd + opt + C run current code chunk in Rmd cmd + opt + I insert code chunks in Rmd, i.e., ```{r} and ``` Q: How to print output in console rather than inline in Rmd? A: Choose the gear ‚öôÔ∏è in the editor toolbar and choose ‚ÄúChunk Output in Console‚Äù. Q: How to insert Emojis in Rmd? A: There are several options (only work for html output): You can type directly a lot of Emojis, such as Ô∏èüôè and ü§£. Try this first, if it doesn‚Äôt show properly, then try the following solutions. If the emoji can show in the script, then you can use it directly. Using a html tag, e.g., &lt;span&gt; ‚öôÔ∏è &lt;/span&gt; will show like this ‚öôÔ∏è This seems to be the most straightforward solution to me. ‚úÖ Note that the emoji won‚Äôt disply correctly in your Rmd file, but when you render the Rmd and deploy to html pages, the emoji will show properly. Using Hexadecimal code. (You need to look up the code somewhere, which is a hassle. ‚ùå) We can add emojis to an HTML document by using their hexadecimal code. These code starts with &amp;#x and ends with ; to specify browser that these are hexadecimal codes. For example, &lt;p&gt;Smily face &lt;span&gt;&amp;#x1F600;&lt;/span&gt; &lt;/p&gt; will give you Smily face üòÄ Go to this site: https://emojipedia.org/emoji/ Grab the codepoint for the emoji you want (e.g., U+1F600 for grinning face) Replace U+ with &amp;#x so it becomes &amp;#x1F600, and add a semicolon ; at the end. Finally, enclose that into an html tag, e.g., &lt;span&gt;. With RStudio Visual mode. (You need to change mode back and forth. ‚ùå) First change to the Visual mode. To insert an emoji, you can use either the Insert menu or the requisite markdown shortcut plus auto-complete: I am personally NOT a fan of Visual Mode because it changes your source code silently ‚Ä¶ Set working directory # get the dir name of the current script dir_folder &lt;- dirname(rstudioapi::getSourceEditorContext()$path) setwd(dir_folder) # set as working dir RStudio projects are associated with R working directories. You can create an RStudio project: In a brand new directory In an existing directory where you already have R code and data By cloning a version control (Git or Subversion) repository Why using R projects: I don‚Äôt need to use setwd at the start of each script, and if I move the base project folder it will still work. I have a personal package with a custom project, which creates my folders just the way I like them. This makes it so that the basic locations for data, outputs and analysis is the same across my work. Double-click on a .Rproj file to open a fresh instance of RStudio, with the working directory and file browser pointed at the project folder. Q: What is an R session? And when do I use it? A: Multiple concurrent sessions can be useful when you want to: Run multiple analyses in parallel Keep multiple sessions open indefinitely Participate in one or more shared projects Launch a new project-less RStudio session # run in console rstudioapi::terminalExecute(&quot;open -n /Applications/RStudio.app&quot;, show = FALSE) -n Open a new instance of the application(s) even if one is already running. rstudioapi::terminalExecute(command, workingDir = NULL, env = character(), show = TRUE) tells R to run the system command in quotes. command System command to be invoked, as a character string. workingDir Working directory for command env Vector of name=value strings to set environment variables show If FALSE, terminal won‚Äôt be brought to front The rstudioapi package provides an interface for interacting with the RStudio IDE with R code. Usingrstudioapi, you can: Examine, manipulate, and save the contents of documents currently open in RStudio, Create, open, or re-open RStudio projects, Prompt the user with different kinds of dialogs (e.g.¬†for selecting a file or folder, or requesting a password from the user), Interact with RStudio terminals, Interact with the R session associated with the current RStudio instance. Set up Development Tools https://cran.r-project.org/bin/macosx/tools/ install Xcode command line tools sudo xcode-select --install install GNU Fortran compiler Using Apple silicon (aka arm64, aarch64, M1) Macs Fortran compiler Go to https://www.xquartz.org/, download the .dmg and run the installer. Verify that build tools are installed and available by opening an R console and running install.packages(&quot;pkgbuild&quot;) pkgbuild::check_build_tools() Insert Code Session To insert a new code section you can use the Code -&gt; Insert Section command. Alternatively, any comment line which includes at least four trailing dashes (-), equal signs (=), or pound signs (#) automatically creates a code section. Define your own shortcuts https://www.statworx.com/ch/blog/defining-your-own-shortcut-in-rstudio/ https://www.r-bloggers.com/2020/03/defining-your-own-shortcut-in-rstudio/ Install the shortcut packages. Add code session separators, --- or ===. install.packages( # same path as above &quot;~/Downloads/shoRtcut_0.1.0.tar.gz&quot;, # indicate it is a local file repos = NULL) install.packages( # same path as above &quot;~/Downloads/shoRtcut2_0.1.0.tar.gz&quot;, # indicate it is a local file repos = NULL) Set up keyboard shortcuts in RStudio for the two packages. Now go to Tools &gt; Modify Keyboard Shortcuts and search for ‚Äúdashes‚Äù. Here you can define the keyboard combination by clicking inside the empty Shortcut field and pressing the desired key-combination on your keyboard. Click Apply, and that‚Äôs it! The shortcuts I use are: ctrl + shift + S (hyphen) to insert --- and ctrl + shift + = (equal sign) to insert ===. Tips and Tricks In Rmd files, send the R code chunk output to the console. By default, RStudio enables inline output (Notebook mode) on all R Markdown documents. You can disable notebook mode by clicking the gear button in the editor toolbar, and choosing Chunk Output in Console. To use the console by default for all your R Markdown documents: Tools -&gt; Options -&gt; R Markdown -&gt; Uncheck Show output inline for all R Markdown documents. To add comments to a function, you can type ‚ÄúRoxygen comment‚Äù into the Command Palette (shift+cmd+P) while the cursor is in a function and it will automatically add a template structure for writing a comment about your function. Keyboard shortcut: shift+opt+cmd+R Snippets are a way to make a shortcut for inserting text based on a ‚Äúcode‚Äù. To find the snippets and edit them, use the Palette (Cmd-Shift-P) and type ‚Äúedit snippets‚Äù. There you will find some predefined snippets. You can also create your own. For instance, when in an R script (or code chunk), typing ‚Äúfun‚Äù followed by pressing Tab, a template for a function will be inserted that looks like: name &lt;- function(variables) { } You can just fill in the name of the function, then press Tab to move to the variables, change the name, then press Tab again to move to the function code area and write your function without moving your fingers from the keyboard. Show argument definitions as you type functions. When you type an existing R function such as round(, not only does tab give you the options, but there‚Äôs an explanation beneath each variable, telling you its role in the function: "],["2.1-dark-theme.html", "2.1 Dark Theme", " 2.1 Dark Theme https://community.rstudio.com/t/fvaleature-req-word-background-highlight-color-in-find-and-spellcheck/18578/3 https://rstudio.github.io/rstudio-extensions/rstudio-theme-creation.html https://docs.posit.co/ide/user/ide/guide/ui/appearance.html#creating-custom-themes-for-rstudio Theme repositories rstudiothemes: https://github.com/max-alletsee/rstudio-themes rsthemes: https://www.garrickadenbuie.com/project/rsthemes/ RStudio and Editor themes are two differnt things RStudio theme applies to the IDE‚Äôs framework; including Modern (default), Classic, Sky, and Dark. The Sky theme is similar to the Modern theme, except for the tab and toolbar headers. Ê∑°Ê∑°ÁöÑËìùËâ≤ The dark theme is a superset to the Modern and Sky themes that is activated whenever the Editor theme uses a dark palette. Editor theme applies to the source pane. A useful tool to customize your editor theme: https://tmtheme-editor.glitch.me/#!/editor/theme/Monokai Embeded themes can be found here: https://github.com/rstudio/rstudio/tree/87e129853121106a87e92df416363f39da95f82e/src/cpp/session/resources/themes Useful elements: .ace_marker-layer .ace_selection Changes the color and style of the highlighting for the currently selected line or block of lines. .ace_marker-layer .ace_bracket Changes the color and style of the highlighting on matching brackets. Recommended highlight color: rgba(255, 0, 0, 0.47) If you really like one of the default themes RStudio provides, but you want to tweak some small things, you can go the theme directory and change the element‚Äôs appearance. RStudio‚Äôs default editor theme directory on Mac: Right click RStudio.app, ‚ÄúShow Package Contents‚Äù to navigate to the application folder. /Applications/RStudio.app/Contents/Resources/resources/themes/ambiance.rstheme (deprecated) New editor theme directory: /Applications/RStudio.app/Contents/Resources/app/resources/themes/ambiance.rstheme You may also find the default themes on GitHub repo: https://github.com/rstudio/rstudio/tree/master/src/cpp/session/resources/themes If you want to install or create a completely new theme, use the Custom theme (user-defined) folder: ~/.config/rstudio/themes/idle_fingers_2.rstheme on mac viridis-theme /* yaml tag */ .ace_meta.ace_tag { color: #2499DA; } /* quoted by $...$ and code chunk options */ .ace_support.ace_function { color: #55C667; } See HERE for common selectors you can use. A collection of screenshots of default RStudio themes: https://www.trifields.jp/list-of-rstudio-editor-themes-2520 Q: The margin line is too bright. A: Change the .ace_print-margin element. .ace_print-margin { width: 1px; background: #e8e8e8; } #e8e8e8 is the culprit here, and should be darkened. I changed it to #2F3941. Source: https://github.com/rstudio/rstudio/issues/3420#issuecomment-453154475 Install custom themes Using rstudiothemes pkg Go to the repository to see which theme you want to use. Then install the theme. Themes can be applied to RStudio via ‚ÄúTools‚Äù - ‚ÄúGlobal Options‚Äù - ‚ÄúAppearance‚Äù - ‚ÄúAdd Theme‚Äù. # install the pseudo-package from this Github repository devtools::install_github(&quot;max-alletsee/rstudio-themes&quot;) library(rstudiothemes) # ... then load the library # example 1: bulk-install all light themes install_rstudio_themes(theme = &quot;all_light&quot;) # example 2: install two specific light themes install_rstudio_themes(theme = c(&quot;Ayu Light&quot;, &quot;Github {rsthemes}&quot;)) # examplease 3: install one specific dark theme install_rstudio_themes(theme = &quot;49th Parallel&quot;) Using rstudioapi package‚Äôs ‚ÄúaddTheme‚Äù function # create temporary download directory theme_49th_parallel &lt;- fs::path_temp(&quot;49th_parallel-RStudio&quot;, ext = &quot;rstheme&quot;) # download theme from github download.file(&quot;https://raw.githubusercontent.com/wvictor14/rstudio_themes/master/49th%20Parallel.rstheme&quot;, theme_49th_parallel) # apply the theme rstudioapi::addTheme(theme_49th_parallel, apply = TRUE) "],["2.2-update-r.html", "2.2 Update R", " 2.2 Update R Q: How to tell which version of R you are running? A: In the R terminal, type R.version. sessioninfo::session_info() print information about the current R session, including the R version, packages loaded, and where they were loaded from. The key thing to be aware of is that when you update R, if you just download the latest version from the website, you will lose all your packages! ‚ùå Q: How to understand the version number? A: The version number is in the format major.minor.patch, e.g.¬†4.3.1. The first number is the major version, the second is the minor version, and the third is the patch version. Major versions are released every 18 months, minor versions are released every 6 months, and patches are released as needed. 2.2.1 Update R on Mac On Mac, can use updater The package re-installs the packages and does not copy them from the previous R installation library. R packages for even minor R releases (e.g.¬†R 4.1 to R 4.2) may not be compatible, which is why its important to re-install the packages and not copy them. Q: What updateR does? A: {updateR}¬†restores old libraries from previous version with the following actions, depending the type of releases: For major releases (R 3.x -&gt; R 4.x),¬†reinstall¬†all the packages; For minor releases (R 3.5.x -&gt; R 3.6.x), users may choose between reinstall or¬†copy and pasteall the file folders under /Library/Frameworks/R.framework/Versions/[old\\_version]/library to /Library/Frameworks/R.framework/Versions/[new\\_version]/library; For patch releases (R 4.0.1 -&gt; R 4.0.2), no actions will be taken. Usage: Find the current location of R by running &gt; .libPaths() [1] &quot;/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library&quot; Install R from https://cran.r-project.org/. Install packages. 3.1 Open your new version of R and install the updater package with install.packages(\"updater\"). 3.2 Install the previous libraries with updater::install_pkgs. updater::install_pkgs(lib.loc = c(&quot;&lt;location(s) saved in Step 1&gt;&quot;)) Use example &gt; library(updater) &gt; install_pkgs(lib.loc = c(&quot;/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library&quot;)) ‚îÄ‚îÄ Importing Package Information ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Ñπ Found 377 packages ‚Üí CRAN: abind, AER, alphavantager, anytime, AsioHeaders, askpass, assertthat, backports, base64enc, bayestestR, bdsmatrix, BH, bigD, bit, bit64, bitops, blob, bookdown, ‚Ä¶, zip, and zoo ‚Üí GitHub: addinexamples, ggplot2, httpgd, rstudiothemes, xltabr, and xtsExtra ‚Üí R-Forge: IntroCompFinR ‚Üí Unknown: shoRtcut and shoRtcut2 ‚îÄ‚îÄ Installing Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Ñπ Packages in repositories CRAN, R-Forge, and Unknown will be installed from CRAN (&lt;https://cran.rstudio.com/&gt;) Do you want to proceed? [y/N]: Type y to proceed with the installation of packages. A progress bar and ETA will show the Estimated Time Remaining until the installation is finished. It will take a while to install all the packages, depending on your internet speed and the number of packages you have installed. For a moderate user, it may take about 10 minutes to install all the packages. Refs: Tutorial: https://cran.r-project.org/web/packages/updater/refman/updater.html GitHub, @Daniel D. Sjoberg : https://github.com/ddsjoberg/updater 2.2.2 Update R on Windows On Windows use installr The easiest way to update R and not cause yourself a huge headache is to use the installr package. When you use the updateR() function, a series of dialogue boxes will appear. These should be fairly self-explanatory but there is a full step-by-step guide available for how to use installr, simply select ‚ÄúYes‚Äù when it asks if you would like to copy your packages from the older version of R. # Install the installr package install.packages(&quot;installr&quot;) # Load installr library(installr) # Run the update function updateR() 2.2.3 Caveats about updating R Be aware that some package updates may cause your previous code to stop working. For this reason, we recommend updating all your packages once at the beginning of each academic year (or semester) ‚Äì don‚Äôt do it before an assessment or deadline just in case! Q: When should I update R? A: When I see many packages print a warning message that they are built under a newer version of R, I will update R. Every 6 months or so would be quite sufficient. "],["2.3-packages-management.html", "2.3 Packages Management", " 2.3 Packages Management 2.3.1 Load packages Q: What is the difference btw library(package) and require(package)? A: library(package) returns an error if the package doesn‚Äôt exist. require(package) returns FALSE if the package is not found and TRUE if the packages is loaded. require is designed for use inside other functions, such as using the value it returns in some error checking loop, as it outputs a warning and continues if the package is not found. Q: How to reload a package after updating? A: Call detach(package:pkg, unload = TRUE) or unloadNamespace first, then use library(pkg) to reload. If you use library on a package whose namespace is loaded, it attaches the exports of the already loaded namespace. So detaching and re-attaching a package may not refresh some or all components of the package, and is inadvisable. The most reliable way to completely detach a package is to restart R. For example, if we want to detach ggplot2 package, we can use detach(package:ggplot2, unload=TRUE) requireNamespace can be used to test if a package is installed and loadable because it comes back with either TRUE (if found the pkg) or FALSE (if failed to find the pkg). &gt; !requireNamespace(&quot;ggplot2&quot;) [1] FALSE &gt; !requireNamespace(&quot;ggplot3&quot;) Loading required namespace: ggplot3 Failed with error: ‚Äòthere is no package called ‚Äòggplot3‚Äô‚Äô [1] TRUE To see whether need to install some packages: # install the package if it is not available if (!requireNamespace(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) # or equivalently if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) You can also use require(devtools) to check whether the required package is available, but note that it will load the package as a side effect. Alternatively, # short command &quot;ggplot2&quot; %in% installed.packages() # full command &quot;ggplot2&quot; %in% rownames(installed.packages()) installed.packages() Finds details of all packages installed in the specified library path lib.loc. Returns a matrix of package names, library paths and version numbers. &gt; installed.packages() %&gt;% class() [1] &quot;matrix&quot; &quot;array&quot; &gt; installed.packages() %&gt;% str() chr [1:355, 1:16] &quot;abind&quot; &quot;alphavantager&quot; &quot;anytime&quot; &quot;askpass&quot; &quot;assertthat&quot; &quot;backports&quot; &quot;base&quot; ... - attr(*, &quot;dimnames&quot;)=List of 2 ..$ : chr [1:355] &quot;abind&quot; &quot;alphavantager&quot; &quot;anytime&quot; &quot;askpass&quot; ... ..$ : chr [1:16] &quot;Package&quot; &quot;LibPath&quot; &quot;Version&quot; &quot;Priority&quot; ... The following code can be used to load packages for your project and set up the working environment. # load the pkg, if not found, install then load require(dplyr) || {install.packages(&quot;dplyr&quot;); require(dplyr)} require(odbc) || {install.packages(&quot;odbc&quot;); require(odbc)} require(DBI) || {install.packages(&quot;DBI&quot;); require(DBI)} If using library(), will return error if some package is not installed and interrupt the program. If it is a list of packages you want to check, use lapply to loop through all packages. ## First specify the packages of interest packages = c(&quot;MASS&quot;, &quot;nlme&quot;) ## Now load or install&amp;load all package.check &lt;- lapply( packages, FUN = function(x) { if (!require(x, character.only = TRUE)) { install.packages(x, dependencies = TRUE) library(x, character.only = TRUE) } } ) You can then use search() to determine whether all the packages have loaded. search() [1] &quot;.GlobalEnv&quot; &quot;package:nlme&quot; &quot;package:MASS&quot; [4] &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; [7] &quot;package:datasets&quot; &quot;renv:shims&quot; &quot;package:utils&quot; [10] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; Q: dplyr has many conflicts with plyr. A: Specify pkg using ::. Or set library priority by changing the order in which you load the packages. # load dplyr last so that it has priority library(plyr) library(dplyr) with the {needs} package library(needs) # prioritize the functions in dplyr prioritize(dplyr) Q: How to unload a package without restarting R? A: detach(\"package:ggplot2\", unload=TRUE) or uncheck the checkbox button in Packages pane. Q: How to remove a package? A: Use remove.packages(\"dplyr\") or you can use the package manager pane, click the X mark on the right side of the selected package. 2.3.2 Install packages Packages can be installed from GitHub, GitLab, BioConductor, and any repository listed in getOption(\"repos\"). This would typically be from CRAN and any other secondary repositories that may be set. Note that package names are case-sensitive, they should match exactly the GitHub repository name. Each time you install an R package from the R command line, you are asked which CRAN mirror, or server, R should use. To set the repository and avoid having to specify this during every package installation, you can add the following line to your .Rprofile file: options(repos = c(CRAN=&quot;https://cloud.r-project.org&quot;)) This code snippet sets the R package repository to the 0-Cloud CRAN mirror at the start of each R session. 0-Cloud automatically redirects to servers worldwide, currently sponsored by Posit. The principle is to choose a mirror location close to you. Install R packages from source # From local tarball install.packages( # indicate path of the package source file &quot;~/Documents/R/UserPackages/shoRtcut2_0.1.0.tar.gz&quot;, # indicate it is a local file repos = NULL) # From github install.packages(&quot;Rcpp&quot;, repos=&quot;https://rcppcore.github.io/drat&quot;) Install from GitHub devtools::install_github(repo, ref=&quot;HEAD&quot;, subdir = NULL) repo repository address in the format username/repo[/subdir][@ref|#pull]. Alternatively, you can specify subdir and/or ref using the respective parameters. If both are specified, the values in repo take precedence. ref Desired git reference. Could be a commit, tag, or branch name, or a call to github_pull() or github_release(). Defaults to \"HEAD\", which means the default branch on GitHub and for git remotes. Ex # install version 3.5.1 install_github(&quot;tidyverse/ggplot2&quot;, ref=&quot;ggplot2 3.5.1&quot;) # install your own package devtools::install_github(&quot;my1396/shoRtcut&quot;) # insert dash devtools::install_github(&quot;my1396/shoRtcut2&quot;) # insert equal sign Install package from source file .tar install.packages(&quot;~/Downloads/greenbrown_2.4.3.tar&quot;, repos = NULL, type=&quot;source&quot;) Check installed packages # print all installed packages rownames(installed.packages()) # check if `ggplot2` is installed &quot;ggplot2&quot; %in% rownames(installed.packages()) installed.packages(lib.loc=NULL, priority=NULL) lib.loc character vector describing the location of R library trees to search through priority used to select packages; \"high\" is equivalent to c(\"base\", \"recommended\") # list all bases packages from your `R.Version` &gt; rownames(installed.packages(priority=&quot;base&quot;)) [1] &quot;base&quot; &quot;compiler&quot; &quot;datasets&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;grid&quot; [7] &quot;methods&quot; &quot;parallel&quot; &quot;splines&quot; &quot;stats&quot; &quot;stats4&quot; &quot;tcltk&quot; [13] &quot;tools&quot; &quot;utils&quot; # what R loads on startup &gt; c(getOption(&quot;defaultPackages&quot;), &quot;base&quot;) [1] &quot;datasets&quot; &quot;utils&quot; &quot;grDevices&quot; &quot;graphics&quot; &quot;stats&quot; &quot;methods&quot; &quot;base&quot; getOption(\"defaultPackages\") is what R loads on startup although the basepackage is not counted. Check package version packageVersion(&quot;ggplot2&quot;) # check package version Q: How do I know if I have the latest version? A: You can go to GitHub repo to check release notes. You will find the latest version of packages there. 2.3.3 Update packages Update an individual package Using install.packages install.packages(&quot;ggplot2&quot;) # update one specific package Using update.packages update.packages(oldPkgs = &quot;ggplot2&quot;) Note that you need to specify oldPkgs explicitly as it is a named argument. Update ALL outdated packages ## update all installed packages in a stated library location, default to `.libPaths()` update.packages(lib.loc = .libPaths(), ask = TRUE) update.packages updates ALL outdated packages in a stated library location. That library location is given by the first argument (if not supplied it works on all known library locations for the current R session). It will ask you for every package if you want to update. To just say yes to everything, use ask = FAlSE. update.packages(ask = FALSE) Unfortunately this won‚Äôt update packages installed by devtools::install_github() Troubleshooting Q: I ran update.packages(\"ggplot2\"), but nothing happened. No output on console, no error, nothing. A: The first argument specifies the library location you want to search through (and update packages therein). update.packages(\"ggplot2\") means you want to update the packages in library location ggplot2, which is most unlikely to exist on your R installation. Q: I tried to update ggplot2 with install.packages(\"ggplot2\"), but nothing happened. A: If ggplot2 is already loaded, then you can‚Äôt install ggplot2 in the current session now. If you need to, save any objects you can‚Äôt easily recreate, and quit out of R. Then start a new R session, immediately run install.packages(\"ggplot2\"), then once finished, load the package and reload in any previously saved objects. More about update.packages: update.packages(lib.loc = NULL, repos = getOption(\"repos\"), ask = TRUE): First a list of all packages found in lib.loc is created and compared with those available at the repositories. If ask = TRUE (the default) packages with a newer version are reported and for each one the user can specify if it should be updated. If so the packages are downloaded from the repositories and installed in the respective library path (or instlib if specified). You can specify one specific package to update using update.packages(oldPkgs = \"ggplot2\"). It will check updates only for that package and ask you if you want to update. The easiest way to update an individual package is just to use install.packages. It is a one step command, compared to update.packages, which first checks and then asks. update.packages returns NULL invisibly. Be aware that some package updates may cause your previous code to stop working. For this reason, we recommend updating all your packages once at the beginning of each academic year (or semester) ‚Äì don‚Äôt do it before an assessment or deadline just in case! Reinstall all Packages after R update R packages are missing after updating. So you have to save the installed packages and re-install them after updating. Alternatively, updater automatically reinsatll R pakages. ‚úÖ Here is how to do it manually. ## get packages installed packs &lt;- as.data.frame(installed.packages(.libPaths()[1]), stringsAsFactors = F) # Save to local f_name &lt;- &quot;~/Documents/R/packages.csv&quot; rownames(packs) write.csv(packs, f_name, row.names = FALSE) packs &lt;- read_csv(f_name) packs ## Re-install packages using install.packages() after updating R install.packages(packs$Package) R library path /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library use find.package(\"ggplot2\") to find the location to where the given package is found. alternatively, you can run .libPaths() .libPaths() without an argument will return a list of all the places R will currently look for loading a package when requested. .libPaths(\"a/b/c\") with an argument will add that new directory (\"a/b/c\") to the ones R was already using. If you use that directory often enough, you may wish to add that call to .libPaths(\"a/b/c\") in your .Rprofile startup file in your home directory. 2.3.4 Put your R package on GitHub Reference: https://jennybc.github.io/2014-05-12-ubc/ubc-r/session2.4_github.html Change to the package directory Initialize the repository with git init Add and commit everything with git add . stage changes; git status optional check staged changes, but yet to submit; and git commit submit staged changes. Create a new repository on GitHub Connect your local repository to the GitHub one # add repo name &quot;origin&quot; to the remote repo at the URL git remote add origin https://github.com/username/reponame Push everything to github # rename the current local branch to &quot;main&quot; git branch -M main # creates a remote branch &quot;origin&quot; and sets it upstream of the &quot;main&quot; branch git push -u origin main FAQ Q: What are package vignettes? A: It‚Äôs important to write good and clear documentation, but users don‚Äôt often read it; at best they‚Äôll look at the examples, so be sure to include informative examples. In my experience, what users really want are instructive tutorials demonstrating practical uses of the software with discussion of the interpretation of the results. In R packages, such tutorials are called ‚Äúvignettes.‚Äù "],["2.4-using-git-with-rstudio.html", "2.4 Using Git with RStudio", " 2.4 Using Git with RStudio Before you start coding, make sure that you are on the correct branch. You may check from the Git tab on the Environment, History, Connections, ‚Ä¶ pane you can also see from the status bar on the very top of the window. The words are formatted as ‚ÄúProjection Name ‚Äì Branch ‚Äì RStudio‚Äù. Choose a License for your repo Q: Which open source license is appropriate for my project? A: See https://opensource.guide/legal/#which-open-source-license-is-appropriate-for-my-project. Q: How to add a license to my repo? A: Follow the instructions here. "],["2.5-github-copilot.html", "2.5 GitHub Copilot", " 2.5 GitHub Copilot Copilot offers autocomplete-style suggestions as you code as ‚Äúghost text.‚Äù GitHub Copilot primarily relies on the context in the file you are actively editing. Any comments, code, or other context provided within the active document will be used as a ‚Äúprompt‚Äù that Copilot will then use to provide a suggested completion. To expand the scope of the context used by Copilot beyond just the active document, there is a setting to also index and read from other R, Python, or SQL files in the current project. This setting can be toggled on or off in the Tools &gt; Global Options &gt; Copilot &gt; ‚ÄúIndex project files with GitHub Copilot‚Äù setting. At times, normal R autocomplete and Copilot may seem to conflict with each other. In these cases, it is best to review the Copilot suggestion and determine if it is appropriate for the current context. Accept the suggestion by pressing Tab. You can ignore the suggestion and continue typing or force the normal R autocomplete to show by pressing Ctrl+Space. You can set up normal R autocomplete using Options &gt; Code &gt; Completion. Issue: Ctrl+Space crash with spotlight shortcut. ‚ùóÔ∏è Only show Copilot suggestions when evoke manually using Ctrl + Backslash (\\). The Copilot suggestions can be very distracting and clutter your script. For general advice on how to use Copilot, please see: RStudio Copilot User Guide How to use GitHub Copilot: Prompts, tips, and use cases "],["2.6-save-r-workspace.html", "2.6 Save R Workspace", " 2.6 Save R Workspace If you want to saves all objects in your work space, use save.image(). It will creates an image of your current variables and functions, and saves them to a file called .RData. When R next loads, objects stored in this image are by default restored. This sounds convenient, however, you do NOT want to do this because this corrupt reproducibility of your project. ‚ùå You want to start from a clean slate very time. ‚úÖ It is suggested change RStudio Global Options to not ‚Äúrestore .RData into workspace at startup‚Äù, and never ‚Äúsave workspace to .RData on exit‚Äù. In case you do feel the need to save the workspace, use the following cmd. save.image(file = \".RData\", version = NULL, ascii = FALSE, compress = !ascii, safe = TRUE) ## save current workspace ## f_name &lt;- &quot;RImage/TCR_2023-05-09.RData&quot; f_name save.image(f_name) # load(f_name) Q: Can I save the loaded packages in the current session/workspace? A: The workspace is for objects like data and functions. Starting R with particular packages loaded is what your .Rprofile file is for, and you can have a different one in each directory. But I‚Äôd recommend not saving anything between r sessions and instead recreate it all using code. This is much more likely to lead to reproducible results. History When you quit a project, .Rhistory is automatically written to the project directory unless you opt out to. It contains a history of all of the commands that you have sent to the R console in this session. "],["2.7-pane-layout.html", "2.7 Pane Layout", " 2.7 Pane Layout Pop out an editor Click the Show in New Window button in any source editor tab. To return a document to the main window, click the Return to Main Window button on the editor toolbar. Environment Pane By default, the Environment pane is located in the top-right and includes the Environment, History, Connections, Build, and Version Control System (VCS) tabs. Version Control System (VCS) The VCS tab will change based on the version control system you have enabled for that session. For example, using Git will change the tab name to Git and provide some common commands for viewing diffs, committing changes, pull and push ‚Ä¶ Output pane The Output pane displays various outputs such as plots, HTML content, or on-disk files. It contains the Files, Plots, R Packages, Help, Viewer, and Presentation tabs. Ref: RStudio Pane Layout Global Options that make coding easier Syntax highlight and matched parentheses. Under ‚ÄúTools -&gt; Global Options -&gt; Code -&gt; Display‚Äù, under Syntax section, check the boxes for highlight R function calls and use rainbow parentheses. The second is especially useful to mark matching opening and closing brackets. Show whitespace characters. In ‚ÄúTools -&gt; Global Options -&gt; Code -&gt; Display‚Äù, check ‚ÄúShow whitespace characters‚Äù. This will let you see spaces and newlines in the editor. Q: How to show Toolbar? A: View &gt; Show Toolbar. References: https://coding-club.rostools.org/posts/tips-and-tricks/ "],["2.8-options.html", "2.8 Options", " 2.8 Options getOption(x) Allow the user to set and examine a variety of global options which affect the way in which R computes and displays its results. Use getOption to check default values of global options. x a character string holding an option name, must be quoted in quotes Can only query one option at a time. If multiple options are given, will return the value of the first option. options(...) query and modify global options. ... any options can be defined, using name = value. Note that you do NOT need to quote your option name here! options() with no arguments returns a list with the current values of the options. options(\"name\") can be used to examine options‚Äô current value too; return a list, whereas getOption(\"name\") returns the value only. Note that you need to quote the option name when you do queries. You can query more than one options at a time. &gt; options(&quot;width&quot;, &quot;digits&quot;) $width [1] 90 $digits [1] 7 &gt; getOption(&quot;width&quot;, &quot;digits&quot;) [1] 90 ?options to get the help page of global options. To check which options are available and their definitions. Use examples ## Two ways checking default option values &gt; options(&quot;width&quot;) $width [1] 81 &gt; getOption(&quot;width&quot;) [1] 81 ## Change option values # use name=value &gt; options(width=80, digits=15) # set print width, digits to print for numeric values using name=value paris # use a named list &gt; options(list(width=80, digits=15)) Commonly used global options: Option Description width Controls the maximum number of columns on a line used in printing vectors, matrices and arrays, and when filling by cat. Defaults to 80.Don‚Äôt change this if you want to print more columns. Use options(tibble.width=400) instead. pillar.sigfig Tibbles print numbers with three significant digits by default, switching to scientific notation if the available space is too small.options(pillar.sigfig = 4) to increase the number of digits printed out. options(\"RStudioConsoleRender.viewer_mode\"=\"viewer_pane\") Changes the display location for R Markdown rendering output from the default behavior to the Viewer pane in RStudio. Display Options: ‚Äúviewer_pane‚Äù: Shows output in RStudio‚Äôs Viewer pane (bottom-right panel) ‚Äúwindow‚Äù: Opens output in a separate popup window ‚Äúbrowser‚Äù: Opens output in your default web browser How to Check the Current Default: You can verify the current setting (or default if not set) by running: getOption(&quot;RStudioConsoleRender.viewer_mode&quot;) If it returns NULL, then the system is using the built-in default behavior, which is typically the popup window approach. "],["2.9-r-startup.html", "2.9 R Startup", " 2.9 R Startup Sys.getenv(x) get the values of the environment variables. Returns a vector of the same length as x. x a character vector Environment Variables examples: &gt; Sys.getenv(c(&quot;HOME&quot;, &quot;R_HOME&quot;, &quot;R_PAPERSIZE&quot;, &quot;R_PRINTCMD&quot;)) HOME R_HOME &quot;/Users/menghan&quot; &quot;/Library/Frameworks/R.framework/Resources&quot; R_PAPERSIZE R_PRINTCMD &quot;a4&quot; &quot;lpr&quot; Rstudio doesn‚Äôt load Rprofile or Renviron I store my Rprofile and Renviron in non-default places (i.e.¬†~/.config/R). When opening R in a normal shell, my environment is loaded perfectly fine. When opening Rstudio, it doesn‚Äôt load my options, settings or paths. Have to wrap your option settings in rstudio.sessionInit https://damien-datasci-blog.netlify.app/post/2020-12-31-pimp-your-r-startup-message/ Open .Rprofile usethis::edit_r_profile() wrap up your options in the following snippet setHook(&quot;rstudio.sessionInit&quot;, function(newSession) { # any code included here will be run at the start of each RStudio session options(buildtools.check = function(action) TRUE ) }, action = &quot;append&quot;) Understanding R‚Äôs startup https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/ https://docs.posit.co/ide/user/ide/guide/environments/r/managing-r.html usethis is a workflow package: it automates repetitive tasks that arise during project setup and development, both for R packages and non-package projects. 2.9.1 .Rprofile What is .Rprofile? .Rprofile is a startup file to set options and environment variables. .Rprofile files can be either at the user or project level. User-level .Rprofile files live in the base of the user‚Äôs home directory, and project-level .Rprofile files live in the base of the project directory. R will source only one .Rprofile file. If there is a project-level .Rprofile, the user-level file will NOT be sourced, i.e., the project-level config file take priority. So if you have both a project-specific .Rprofile file and a user .Rprofile file that you want to use, you explicitly source the user-level .Rprofile at the top of your project-level .Rprofile with source(\"~/.Rprofile\"). .Rprofile files are sourced as regular R code, so setting environment variables must be done inside a Sys.setenv(key = \"value\") call. Quitting R will erase the default theme setting. If you load ggplot2 in a future session it will revert to the default gray theme. If you‚Äôd like for ggplot2 to always use a different theme (either yours or one of the built-in ones), you can set a load hook and put it in your .Rprofile file. For example, the following hook sets the default theme to be theme_minimal() every time the ggplot2 package is loaded. setHook(packageEvent(&quot;ggplot2&quot;, &quot;onLoad&quot;), function(...) ggplot2::theme_set(ggplot2::theme_bw())) Of course, you can always override this default theme by adding a theme object to any of your plots that you construct in ggplot2. 2.9.2 .Renviron .Renviron is a user-controllable file that can be used to create environment variables. This is especially useful to avoid including credentials like API keys inside R scripts. This file is written in a key-value format, so environment variables are created in the format: Key1=value1 Key2=value2 ...additional key=value pairs And then Sys.getenv(\"Key1\") will return \"value1\" in an R session. Like with the .Rprofile file, .Renviron files can be at either the user or project level. If there is a project-level .Renviron, the user-level file will not be sourced. The usethis package includes a helper function for editing .Renviron files from an R session with usethis::edit_r_environ(). The .Renviron file is most useful for defining sensitive information such as API keys (such as GitHub, Twitter, or Posit Connect) as well as R specific environment variables like the history size (R_HISTSIZE=100000) and default library locations R_LIBS_USER. Rcpp compilation breaks in R 4.1.0 devtools::build(&quot;my_package&quot;) Error: Could not find tools necessary to compile a package Call `pkgbuild::check_build_tools(debug = TRUE)` to diagnose the problem. In RStudio, I am continually prompted to install additional build tools and I can‚Äôt install the build tool. \\(\\rightarrow\\) Bypass the option options(buildtools.check = function(action) TRUE). Turns out R was pointing to an old clang version in my Makevars. I just deleted it using in Terminal sudo rm ~/.R/Makevars Install SDK command line tool Download from developer.apple.com. Software development kit. https://developer.apple.com/download/all/ R compiler tools for cpp on MacOS https://thecoatlessprofessor.com/programming/cpp/r-compiler-tools-for-rcpp-on-macos/ install OpenMP enabled clang from the terminal https://rpubs.com/Kibalnikov/776164 "],["2.10-vs-code.html", "2.10 VS Code", " 2.10 VS Code vscode-R is the R Extension for Visual Studio Code. The extension is mainly focused on providing language service based on static code analysis and user interactivity between VS Code and R sessions. You can run R in VS Code. Simply open the folder containing your R scripts in VS Code, and then open the command palette (Cmd+Shift+P) and type ‚ÄúR: Create R terminal‚Äù. This will start an R session in the terminal. By default, this will close the currently open folder. If you want multiple windows each with their own folder, you first open a new window (Ctrl + Shift + N) and then open the folder in that new window. rstudioapi::restartSession() will restart the R session. Command Palette, type ‚ÄúR: Interrupt R‚Äù to interrupt the current R session. 2.10.0.1 Keyboard shortcuts Shortcuts Function cmd + / comment shift + cmd + M or shift + ctrl + M user defined; %&gt;% opt + - user defined; &lt;- For commonly used general keyboard shortcuts (not limited to R), see HERE. Suggested keyboard shortcuts for R in VS Code. For user defined shortcuts, you can add them in the keybindings.json file. Q: How to run R code interactively? A: Create an R terminal via command R: Create R Terminal in the Command Palette. Once an R terminal is ready, you could either select the code or put the cursor at the beginning or ending of the code you want to run, press (Ctrl + Enter), and then code will be sent to the active R terminal. If you want to run an entire R file, open the file in the editor, and press Ctrl+Shift+S and the file will be sourced in the active R terminal. Q: Why using VS Code for R programming instead of RStudio? A: Several reasons: Better integration with Copilot, making it easier to write code with AI assistance. More responsive and powerful engineering tools such as symbol highlight, find references, rename symbol, etc. integrated to the IDE. VS Code has a lot of extensions that can enhance your R programming experience, such as Markdown Preview Enhance, Live Server, and GitLens. Git support is better in VS Code, making it easier for version control and collaboration. 2.10.1 languageserver package The R language server implements the Language Server Protocol (LSP) and provides a set of language analysis features such as completion, providing function signatures, extended function documentation, locating function implementations, occurrences of a symbol or object, rename symbol, and code diagnostics and formatting. The R language server statically analyzes R code, and vscode-R interfaces with it to provide the core of this extension‚Äôs functionality. The R language server is implemented by the languageserver package which performs static code analysis with the latest user documents in R and Rmd languages. Therefore, it does not rely on an active R session and thus does not require the code to be executed. vscode-R settings 2.10.1.1 Highlight Features: styler The language server provides code formatting through the through the styler package in R. See here for configuration. Main usage: select the code block you want to format, right-click and select Format Selection. Note that this only works in the Edit Mode when Vim is enabled. Alternatively, right-click at anywhere in the code editor and select Format Document to format the entire document. Rename symbols Place the cursor on the symbol you want to rename, right-click and select Rename Symbol. A dialog will pop up, allowing you to enter the new name for the symbol. A refactoring preview will be shown, allowing you to review the changes before applying them. Find References Right-click on an R object, select Find All References to find all references to the object in the current workspace. The results will be shown in the References view in the Activity Bar on the left side of the window. select Go To References open a popup showing all of the uses of the object within the current document. 2.10.1.2 lintr R code linting (diagnostics) is provided by lintr via language server and is enabled by default. It provides syntax error warnings as well as style guidelines. Configuration To configure the behavior of lintr, you should create/edit the global lintr config file at ~/.lintr. Alternatively, you can also create/edit a project-specific lintr config file at ${workspaceFolder}/.lintr. Do not forget the new empty line at the end of the file. To be sure that the file is correctly set up you can use: read.dcf(&quot;.lintr&quot;) # Should give no error If the file is not available in the workspace you can add it with: options(lintr.linter_file=&quot;Path/to/file/.lintr&quot;) You can also add this line in your .Rprofile to not have to run it everytime. ref ‚Ü©Ô∏é Visit Individual linters for a complete list of supported linters. Visit the Configuring linters for a complete guide to customizing lintr config regarding the list of linters and file or line exclusions. Q: How to disable lintr? A: Set \"r.lsp.diagnostics\": false. Then in command palette, type ‚ÄúDeveloper: Reload Window‚Äù for the changes to take effect. 2.10.2 Radian Q: There is no syntax highlighting in the R terminal. How to fix it? A: Install Radian, an improved R console REPL interface that corrects many limitations of the official R terminal and supports many features such as syntax highlighting and auto-completion. In the terminal, run $pipx install radian installed package radian 0.6.15, installed using Python 3.13.5 These apps are now globally available - radian ‚ö†Ô∏è Note: &#39;/Users/menghan/.local/bin&#39; is not on your PATH environment variable. These apps will not be globally accessible until your PATH is updated. Run `pipx ensurepath` to automatically add it, or manually modify your PATH in your shell&#39;s config file (e.g. ~/.bashrc). done! ‚ú® üåü ‚ú® $pipx ensurepath /Users/menghan/.local/bin has been been added to PATH, but you need to open a new terminal or re-login for this PATH change to take effect. Alternatively, you can source your shell&#39;s config file with e.g. &#39;source ~/.bashrc&#39;. You will need to open a new terminal or re-login for the PATH changes to take effect. Alternatively, you can source your shell&#39;s config file with e.g. &#39;source ~/.bashrc&#39;. N.B. If you have zsh as your shell, you need to run source ~/.zshrc instead of source ~/.bashrc. To find where radian is installed, you can run: which radian /Users/menghan/.local/bin/radian This is the path to the radian executable. After adding Radian to your PATH, you can invoke it in the terminal by simply typing radian. $radian R version 4.3.1 (2023-06-16) -- &quot;Beagle Scouts&quot; Platform: aarch64-apple-darwin20 (64-bit) r$&gt; Then, in VS Code, you will need to set Radian as the default R terminal. You can also configure other settings for R. { &quot;r.rterm.mac&quot;: &quot;/Users/menghan/.local/bin/radian&quot;, &quot;r.bracketedPaste&quot;: true, &quot;r.sessionWatcher&quot;: true, } r.rterm.mac: Path to the Radian executable. r.bracketedPaste: Enables bracketed paste mode, which allows pasting code without executing it immediately. This is useful if you want to paste multiple lines of code into the console at once. r.sessionWatcher: Enables session watcher to monitor the R session. Specifically, Show value of session symbols on hover Show plot output on update and plot history Show htmlwidgets, documentation and shiny apps in WebView r.alwaysUseActiveTerminal: always send code to active terminal rather than vscode-R; helps me to start an R session which is terminated when VSCode exits. r.plot.useHttpgd: Use the httpgd package for viewing plots in a VS Code window or in the browser. See Extension Settings for a full list of settings of vscode-R that can be set in VSCode‚Äôs settings.json file. 2.10.2.1 Configuration radian can be customized by specifying the below options in various locations: $HOME/.config/radian/profile .radian_profile in the working directory Example of a radian profile # either `&quot;emacs&quot;` (default) or `&quot;vi&quot;`. options(radian.editing_mode = &quot;vi&quot;) # enable various emacs bindings in vi insert mode options(radian.emacs_bindings_in_vi_insert_mode = TRUE) # show vi mode state when radian.editing_mode is `vi` options(radian.show_vi_mode_prompt = TRUE) options(radian.vi_mode_prompt = &quot;\\033[0;34m[{}]\\033[0m &quot;) # custom key bindings options( radian.escape_key_map = list( list(key = &quot;-&quot;, value = &quot; &lt;- &quot;), ), radian.ctrl_key_map = list( list(key = &quot;right&quot;, value = &quot; %&gt;% &quot;) ) ) VI support by radian: options(radian.editing_mode = \"vi\"): set the default editing mode to vi. options(radian.show_vi_mode_prompt = TRUE): This option will show the current vi mode in the prompt when using radian in vi mode. The prompt will be colored blue and will display the current mode. [ins]: insert mode [nav]: normal mode 2.10.3 FAQ Q: I cannot see my R Objects in the global environment. A: when you click on ‚ÄúR: (not attached)‚Äù on the bottom bar or type .vsc.attach() into the terminal, your objects should start showing up in your global environment. Q: How to hide variables in OUTLINE view? A: OUTLINE view by default shows all variables in the current R script, making it difficult to locate your true sections. To hide variables, go to command palette, type ‚ÄúOutline: Show‚Äù, there is a list of objects that you can choose to show or hide (you can choose to set this for workspace or the user). Here is my current setting: { &quot;outline.showArrays&quot;: false, &quot;outline.showBooleans&quot;: false, &quot;outline.showClasses&quot;: true, &quot;outline.showConstants&quot;: false, &quot;outline.showFields&quot;: false // this hides most variables you actually don&#39;t want to see } Note that some answers mention that you should use \"outline.showVariables\": false, but this does NOT work for me. Instead, I use \"outline.showFields\": false to hide most variables. See HERE for a complete list of icons and their meanings in the OUTLINE view. The following tables shows the icons that you most commonly see in the OUTLINE view. Icon Name Symbol type Methods and Functions method, function, constructor Variables variable Fields field Words text Constants constant Classes class Structures struct Modules module Properties and Attributes property Constant applies to Quarto sections. Need to set \"outline.showConstants\": true, to show sections properly in the OUTLINE view. 2.10.4 Plot Viewer httpgd: A graphics device for R that is accessible via network protocols. This package was created to make it easier to embed live R graphics in integrated development environments and other applications. httpgd is required by the interactive plot viewer of the R extension for VS Code. Enable r.plot.useHttpgd in VS Code settings. { &quot;r.plot.useHttpgd&quot;: true } r.plot.useHttpgd: use the httpgd-based plot viewer instead of the base VSCode-R plot viewer This will allow you to view plots in a separate window or in the browser, which is useful for interactive visualizations. The httpgd plot viewer supports auto-resizing, light/dark theme mode, plot history, hiding and zooming. Getting started with httpgd library(httpgd) # Start the httpgd graphics device hgd() # Open the plot viewer in your browser hgd_browse() # Create a figure x = seq(0, 3 * pi, by = 0.1) plot(x, sin(x), type = &quot;l&quot;) # Close the graphics device dev.off() Q: Plot viewer is missing. A: Run hgd() in R and get the url to the viewer. Use the command palette to run ‚ÄúR Plot: Open httpgd Url‚Äù. It will let you enter the url address, fill it in and hit Enter to open the plot viewer. Alternatively, if you want to view in external browser, you can copy the url and paste it in your browser. hgd Initialize device and start server. hgd_browse Open the plot viewer in your browser. Universal graphics device (unigd) is a package that provides a set of functions to manage the plot viewer, such as: unigd::ugd_clear() Clear all pages in the plot viewer. unigd::ugd_remove(page = 2) Remove the second page Ref: Using httpgd in VSCode: A web-based SVG graphics device, @Kun Ren 2.10.5 R Debugger To be added ‚Ä¶ ref: Debugging R in VSCode, @Kun Ren 2.10.6 Emulating¬†rstudioapi¬†functions The VSCode-R extension is compatible with a subset of RStudio Addins via an {rstudioapi} emulation layer. Nearly all of the document inspection and manipulation API is supported, allowing RStudio add-ins and packages that rely on¬†rstudioapi¬†to function within VS Code. This emulation is achieved by ‚Äúduck punching‚Äù or ‚Äúmonkey patching‚Äù the¬†rstudioapi¬†functions within the R session running in VS Code.¬†This means the original¬†rstudioapi¬†functions are replaced with custom implementations that communicate with VS Code instead of RStudio. To enable RStudio Addins, you may need to add¬†options(vsc.rstudioapi = TRUE)¬†to your¬†~/.Rprofile¬†file.¬†This ensures the¬†rstudioapi¬†emulation is loaded when your R session starts. getOption(\"vsc.rstudioapi\")¬†will return¬†TRUE¬†if the emulation is enabled. Use RStudio Addins from VS Code How to use your RStudio Addins in VS Code after enabling the emulation: Use the command palette (Ctrl+Shift+P) and type ‚ÄúR: Launch RStudio Addin‚Äù. You will see a list of available RStudio add-ins that you can run directly from VS Code. Choose the add-in you want to run, and it will execute in the current R session. You can also bind a keyboard shortcut to launch the RStudio Addin picker (command id: r.launchAddinPicker): { &quot;key&quot;: &quot;ctrl+shift+A&quot;, // launch RStudio Addin &quot;command&quot;: &quot;r.launchAddinPicker&quot;, &quot;when&quot;: &quot;editorTextFocus &amp;&amp; (editorLangId == &#39;markdown&#39; || editorLangId == &#39;r&#39; || editorLangId == &#39;rmd&#39; || editorLangId == &#39;quarto&#39;)&quot; }, This will allow you to quickly access and run RStudio add-ins without needing to open the command palette each time. To launch a specific RStudio addin, you can map a direct keybinding to the addin R functions. The function can be found in inst/rstudion/addins.dcf¬†file of the addin-providing-package‚Äôs source. Look for the keyword Binding in the file to find the function name. The package name is the repository name of the addin-providing package. Use example: Here I want to invoke two RStudio addins: shoRtcut::set_new_chapter() and shoRtcut2::set_new_chapter2(). Add the following keybindings to your keybindings.json file: { &quot;description&quot;: &quot;Pad line with dashes&quot;, &quot;key&quot;: &quot;ctrl+shift+S&quot;, &quot;command&quot;: &quot;r.runCommand&quot;, &quot;when&quot;: &quot;editorTextFocus &amp;&amp; (editorLangId == &#39;markdown&#39; || editorLangId == &#39;r&#39; || editorLangId == &#39;rmd&#39; || editorLangId == &#39;quarto&#39;)&quot;, &quot;args&quot;: &quot;shoRtcut:::set_new_chapter()&quot; }, { &quot;description&quot;: &quot;Pad line with equals&quot;, &quot;key&quot;: &quot;ctrl+shift+=&quot;, &quot;command&quot;: &quot;r.runCommand&quot;, &quot;when&quot;: &quot;editorTextFocus &amp;&amp; (editorLangId == &#39;markdown&#39; || editorLangId == &#39;r&#39; || editorLangId == &#39;rmd&#39; || editorLangId == &#39;quarto&#39;)&quot;, &quot;args&quot;: &quot;shoRtcut2:::set_new_chapter2()&quot; }, Now you can use Ctrl+Shift+S to pad a line with dashes, and Ctrl+Shift+= to pad a line with equals. ref: RStudio addin support in VSCode-R 2.10.7 Work with Rmd You can edit Rmd with either of the two Language Modes: R Markdown: there is a knit button to provide preview but no live preview. If using RStudio, you can only get a preview after rendering Render the site using: rmarkdown::render_site() function, which can be slow for large sites. Render the document using: rmarkdown::render(\"0103-RStudio-VSCode.Rmd\") function, which is equivalent to clicking the ‚ÄúKnit‚Äù button in RStudio. Note that Knit button generate output html in the docs/ directory; it uses your styles settings in the _output.yml file. rmarkdown::render generates output html in the same directory as the Rmd file. It does not apply any settings from _output.yml file, so you need to specify any headers you want to load, e.g., mathjax macros. How to decide which Language Mode to use? A rule of thumb is: If your Rmd has lots of R code you need to run interactively, use R Markdown. If you want to write a static report with minimal R code, use Markdown. At all cases, it is quite easy to switch between the two modes by changing the Language Mode in the bottom right corner of VS Code. So you can choose either one that suits you best. Markdown: there is no knit button but you can have a live preview using the Markdown Preview Enhance extension. Instead, you need to type the command rmarkdown::render() in the R console or in an R script to render the Rmd file. 2.10.7.1 Render book Render the site rmarkdown::render_site(input = \".\", output_format = \"all\") rmarkdown::render_site() without arguments will render all formats by default. specify output_format to render a specific format, e.g., bookdown::gitbook, bookdown::pdf_book, etc. # Render the site, equivalent to clicking the &quot;Build Book&quot; button in RStudio # All output formats specified in the `_output.yml` file will be rendered. rmarkdown::render_site() # Render a specific output format, e.g., bookdown::gitbook rmarkdown::render_site(output_format = &#39;bookdown::gitbook&#39;) Render a single document has two options: rmarkdown::render_site(file_name) looks for the _output.yml file in the root directory of the site and applies the settings specified in that file. See HERE for more details. Recommended for its automatic formatting. ‚úÖ rmarkdown::render(input, output_format = NULL) renders any single Rmd file without applying any settings from _output.yml file. See HERE for more details. You need to specify any headers you want to load, e.g., mathjax macros. More hassle ‚Üí less recommended. # Render the document, equivalent to clicking the &quot;Knit&quot; button in RStudio # This will apply any global settings for your website and generate the output html in the `docs/` directory. rmarkdown::render_site(&quot;0103-RStudio-VSCode.Rmd&quot;) # Render a single Rmd file rmarkdown::render(&quot;0103-RStudio-VSCode.Rmd&quot;) 2.10.7.2 View book To view the static site in the docs/ directory. I installed the VSCode extension Live Preview. All I need to do is select one of the .html files, right-click the preview button in the code editor, and there it is. I can also just navigate to http://127.0.0.1:3000/docs/ in my browser. It even updates as I add chapters and redo the render_site() command. If Live Preview is not loading the latest changes, try ‚ÄúDeveloper: Reload Window‚Äù in the command palette. A most reliable way is just to open the docs/xxx.html file in your browser. This way, not only will it open the file you clicked, but it will also open the entire site. An additional benefit is that your site won‚Äôt blink when you make changes or build the site. I found the constant blinking of the Live Preview blinding. Using static files, you simply refresh the browser every time you rebuild the site. ref vscode-R Wiki: R Markdown 2.10.8 Extensions R Tools provides support for the R language, including syntax checking, completions, code formatting, formatting as you type, tooltips, linting. Open the Command Palette and type ‚ÄòR:‚Äô to see list of available commands and shortcuts. 2.10.9 Jupyter Notebooks Jupyter¬†(formerly IPython Notebook) is an open-source project that lets you easily combine Markdown text and executable Python source code on one canvas called a¬†notebook. Install IRkernel install.packages(&quot;IRkernel&quot;) IRkernel::installspec() Reload window. Open command palette and type ‚ÄúJupyter: Create New Blank Notebook‚Äù to create a new Jupyter notebook. Click on the button right below ellipsis in upper right corner to choose kernel Select Jupyter Kernels &gt; R to use the R kernel. It should look something like this: 2.10.9.1 Code cell modes Three code modes in Jupyter notebooks: Unselected: When no bar is visible, the cell is unselected. selected: When a cell is selected, it can be in command mode or in edit mode. Command mode: solid vertical bar on the left side of the cell. The cell can be operated on and accepts keyboard commands. Hit Enter to enter edit mode, or click on the cell to enter edit mode. Edit mode: a solid vertical bar is joined by a border around the cell editor. Press Escape to return to command mode, or click outside the cell to return to command mode. 2.10.9.2 .ipynb keyboard shortcuts: Command Palette, type ‚ÄúPreferences: Open Keyboard Shortcuts‚Äù to open the keyboard shortcuts editor. You can search for ‚Äújupyter‚Äù to find all Jupyter-related commands and their shortcuts. Run code cells | Shortcut | Function | | ‚Äî‚Äî‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî | | Ctrl+Enter | runs the currently selected cell. | | Shift+Enter | runs the currently selected cell and inserts a new cell immediately below (focus moves to new cell). | | Opt+Enter | runs the currently selected cell and inserts a new cell immediately below (focus remains on current cell). | | Run Cells Above | Command Pallette, type ‚ÄúNotebook: Execute Above Cells‚Äù | Insert code cells Shortcut Function ctrl+; A Press Ctrl+;, release, then A.Insert a new code cell above the current one. ctrl+; B Add a new code cell below the selected one. Change Cell to Code Shortcut Function ctrl+; Y Change cell to code ctrl+; M Change cell to markdown Miscellaneous Shortcut Function ctrl+; X or dd Delete selected cells shift + ‚Üë/‚Üì Select consecutive multiple cells L command mode; toggle line numbers R Undo last change 2.10.9.3 Issues Copilot Bug Issue: Copilot suddenly suggests new edit, and cannot close Diff Editor. The cell output not distinguishable from the code input text has a dark grey background color, which is hard to read. You can customize your active Visual Studio Code¬†color theme¬†with the workbench.colorCustomizations¬†and¬† editor.tokenColorCustomizations user¬†setting. E.g., &quot;workbench.colorCustomizations&quot;: { &quot;editor.foreground&quot;: &quot;#ffffff&quot; }, workbench.colorCustomizations allows you to set the colors of VS Code UI elements such as list &amp; trees (File Explorer, suggestions widget), diff editor, Activity Bar, notifications, scroll bar, split view, buttons, and more. For a list of all customizable colors, see the¬†Theme Color Reference. editor.tokenColorCustomizations allows you to set the colors of syntax highlighting in the source code editor, such as text, comments, keywords, strings, numbers, and more. Use the¬†scope inspector¬†tool to investigate what tokens are present in a source file. Syntax Highlight Guide¬†for more information on how to customize syntax highlighting. Developer Tools Command Palette ‚Üí type ‚ÄúDeveloper: Toggle Developer Tools‚Äù, this will open the Developer Tools window. It is useful for styling the VS Code UI, such as changing the colors of the text and background in the editor, or customizing the colors of the Jupyter notebook cells. debugging issues with VS Code, such as checking for errors in the console References: R in Visual Studio Code Set up vscode-R: https://renkun.me/2019/12/26/writing-r-in-vscode-interacting-with-an-r-session/ https://francojc.github.io/posts/r-in-vscode/ VS Code for R on macOS Getting started with httpgd: https://nx10.github.io/httpgd/articles/getting-started.html Bookdown in VS code: https://www.bendirt.com/bookdown/ Jupyter Notebooks in VS Code "],["3-rmd.html", "Chapter 3 Rmd", " Chapter 3 Rmd R Markdown is a powerful tool for combining analysis and reporting into the same document. R Markdown has grown substantially from a package that supports a few output formats, to an extensive and diverse ecosystem that supports the creation of books, blogs, scientific articles, websites, and even resumes. Nice documentations rmarkdown package CRAN Package CRAN page Reference manual bookdown package CRAN Package CRAN page Reference manual R markdown: The definitive guide. provides detailed references; GitHub repo HERE. R markdown cookbook: concise and covers essential functions, with examples. Authoring Books with R Markdown: with a focus on bookdown. Q: What is the difference between Rmd and R script? A: An R script (.R) is used for developing and troubleshooting code; a place where you can store reusable code fragments. An R Markdown file (.Rmd) is used to integrate R commands with explanatory text and output, making it useful for creating reports. Quick takeaways: Can still use horizontal separator ctrl + shift + S for dashed lines and ctrl + shift + = for equals Headers must have one empty line above and below to separate it from other text "],["3.1-yaml-metadata.html", "3.1 YAML metadata", " 3.1 YAML metadata Q: What is YAML? A: YAML is a human-friendly data serialization language for all programming languages. YAML stands for ‚ÄúYet Another Markup Language.‚Äù Q: What does YAML do? A: It is placed at the very beginning of the document and is read by each of Pandoc, rmarkdown, and knitr. Provide metadata of the document using key: value. Located at the top of the file. Adheres to the YAML format and is delimited by lines containing three three dashes (---). Indentation is essential as it indicates hierarchy. YAML are also called header or front matter. See R Markdown YAML metadata (header) tutorial with examples by hao203 HERE for commonly used YAML metadata (header) in different R Markdown output formats. There is NO official documentation for R Markdown YAML frontmatter because the YAML frontmatter is a collection of metadata and each individual piece of data might or not be used by a piece of software from your tool chain. That is, the behavior of YAML depends on your user platform. For instance, the following metadata editor_options: chunk_output_type: console is used exclusively by RStudio to have the code block output ‚Äúbe shown in the R console instead of inside the source editor‚Äù. This option might be ignored by VSCode or Emacs. Where do the YAML fields in rmd come from? Pandoc R markdown Output functions, e.g., rmarkdown::pdf_document Custom Pandoc templates R markdown extension packages, e.g., blogdown YAML can set values of the template variables, such as title, author, and date of the document. The output field is used by rmarkdown to apply the output format function rmarkdown::html_document() in the rendering process. There are two types of output formats in the rmarkdown package: documents (e.g., pdf_document), and presentations (e.g., beamer_presentation). Supported output format examples: html_document, pdf_document. R Markdown documents (html_documents) and R Notebook documents (html_notebook) are very similar; in fact, an R Notebook document is a special type of R Markdown document. The main difference is using R Markdown document (html_documents) you have to knit (render) the entire document each time you want to preview the document, even if you have made a minor change. However, using an R Notebook document (html_notebook) you can view a preview of the final document without rendering the entire document. Troubleshooting Issue: bookdown always output html, even if specified to pdf. Cause: If it produces HTML, the output format must have been provided somewhere. Fix: Check if you have a _output.yml under the root directory of your book project. If you do, you may delete it. Then bookdown will use the output field that you specified in the YAML frontmatter of your Rmd document. If there are two output formats, rmarkdown::render() defaults to use the first output type. If you want another, specify the type, e.g., rmarkdown::render(\"0100-RStudio.Rmd\", 'pdf_document'). bookdown wrappers of base markdown format bookdown output formats allow numbering and cross-referencing figures/tables/equations. It takes the format html_document2, in general, markdown_document2 is a wrapper for the base format markdown_document. With the bookdown output format, you can cross-reference sections by their ID‚Äôs using the same syntax when sections are numbered. Other bookdown output format examples for single documents: bookdown::pdf_document2, bookdown::beamer_presentation2, bookdown::tufte_html2, bookdown::word_document2. See Page 12 of the reference manual for a complete list of supported formats by bookdown. What bookdown is very powerful for is that it compiles books. The main difference between rendering in R Markdown and bookdown is that a book will generate multiple HTML pages by default. Book formats: HTML: bookdown::gitbook Built upon rmarkdown::html_document. bookdown::html_book bookdown::tufte_html_book PDF: bookdown::pdf_book e-book: bookdown::epub_book 3.1.1 Top-level YAML metadata Many aspects of the LaTeX template used to create PDF documents can be customized using top-level YAML metadata (note that these options do NOT appear underneath the output section, but rather appear at the top level along with title, author, and so on). For example: --- title: &quot;Crop Analysis Q3 2013&quot; output: pdf_document fontsize: 11pt geometry: margin=1in --- A few available metadata variables are displayed in the following (consult the Pandoc manual for the full list): Top-level YAML Variable Description lang Document language code fontsize Font size (e.g., 10pt, 11pt, or 12pt) papersize Defines the paper size (e.g., a4paper, letterpaper) documentclass LaTeX document class (e.g., article, book, and report) classoption A list of options to be passed to the document class, e.g., you can create a two-column document with the twocolumn option. geometry Options for geometry package (e.g., margin=1in set all margins to be 1 inch) mainfont, sansfont, monofont, mathfont Document fonts (works only with xelatex and lualatex) linkcolor, urlcolor, citecolor Color for internal links (cross references), external links (link to websites), and citation links (bibliography) linestretch Options for line spacing (e.g.¬†1, 1.5, 3). Pandoc User‚Äôs Guide: https://www.uv.es/wiki/pandoc_manual_2.7.3.wiki?21 classoption onecolumn, twocolumn - Instructs LaTeX to typeset the document in one column or two columns. twoside, oneside: Specifies whether double or single sided output should be generated. The classes‚Äô article and report are single sided and the book class is double sided by default. Note that this option concerns the style of the document only. The option two side does NOT tell the printer you use that it should actually make a two-sided printout. The difference between single-sided and double-sided documents in LaTeX lies in the layout of the page margins and the orientation of the text on the page. Single-sided documents are printed on only one side of the page, with the text and images aligned to the right-hand side of the page. This type of layout is often used for brochures, flyers, and other types of promotional materials. Double-sided documents are printed on both sides of the page, with the text and images alternating between right-hand and left-hand margins. This type of layout is often used for books, reports, and other types of long-form documents. A twoside document has different margins and headers/footers for odd and even pages. The layout of a twoside book Q: Why Inner margin is narrow? A: The reason for this is that with two pages side by side, you actually have only THREE margins - the left, right and middle. The middle margin is made up from the inside margins of both pages, and so these are smaller because they add together to make the middle margin. If they were bigger, then you would end up with too much whitespace in the middle. o - outside margin i - inside margin b - binding offset Before binding: ------------------ ----------------- |oooo|~~~~~~|ii|b| | | |~~~~~~| | | |~~~~~~| | | | | |~~~~~~| | | |~~~~~~| | | | | |~~~~~~| | | |~~~~~~| | | | | |~~~~~~| | | |~~~~~~| | | | | |~~~~~~| | | |~~~~~~| | | | | |~~~~~~| | ------------------ ----------------- After binding: ------------------------------- |oooo|~~~~~~|ii|ii|~~~~~~|oooo| |oooo|~~~~~~|ii|ii|~~~~~~|oooo| |oooo|~~~~~~|ii|ii|~~~~~~|oooo| |oooo|~~~~~~|ii|ii|~~~~~~|oooo| |oooo|~~~~~~|ii|ii|~~~~~~|oooo| |oooo|~~~~~~|ii|ii|~~~~~~|oooo| ------------------------------- landscape - Changes the layout of the document to print in landscape mode. openright, openany - Makes chapters begin either only on right hand pages or on the next page available. This does not work with the article class, as it does not know about chapters. The report class by default starts chapters on the next page available and the book class starts them on right hand pages. In PDFs, you can use code, typesetting commands (e.g., \\vspace{12pt}), and specific packages from LaTeX. The header-includes option loads LaTeX packages. Note that header-includes is a top-level option that align with output. --- output: pdf_document header-includes: - \\usepackage{fancyhdr} --- \\pagestyle{fancy} \\fancyhead[LE,RO]{Holly Zaharchuk} \\fancyhead[LO,RE]{PSY 508} # Problem Set 12 Common header-includes: Chinese/Japanese support --- output: pdf_document header-includes: - \\usepackage{ctex} --- Alternatively, use extra_dependencies to list a character vector of LaTeX packages. This is useful if you need to load multiple packages: --- title: &quot;Untitled&quot; output: pdf_document: extra_dependencies: [&quot;bbm&quot;, &quot;threeparttable&quot;] --- If you need to specify options when loading the package, you can add a second-level to the list and provide the options as a list: --- title: &quot;Untitled&quot; output: pdf_document: extra_dependencies: caption: [&quot;labelfont={bf}&quot;] hyperref: [&quot;unicode=true&quot;, &quot;breaklinks=true&quot;] lmodern: null --- Here are some examples of LaTeX packages you could consider using within your report: pdfpages: Include full PDF pages from an external PDF document within your document. caption: Change the appearance of caption subtitles. For example, you can make the figure title italic or bold. fancyhdr: Change the style of running headers of all pages. Some output options are passed to Pandoc, such as toc, toc_depth, and number_sections. You should consult the Pandoc documentation when in doubt. --- output: pdf_document: toc: true keep_tex: true --- keep_tex: true if you want to keep intermediate TeX. Easy to debug. Defaults to false. To learn which arguments a format takes, read the format‚Äôs help page in R, e.g.¬†?html_document. Parameters We can include variables and R expressions in this header that can be referenced throughout our R Markdown document. For example, the following header defines start_date and end_date parameters, which will be reflected in a list called params later in the R Markdown document. --- title: My RMarkdown author: Yihui Xie output: html_document params: start_date: &#39;2020-01-01&#39; end_date: &#39;2020-06-01&#39; --- To access a parameter in our R code, call params$&lt;parameter name&gt;, e.g., params$start_date and params$end_date. Should I use quotes to surround the values? Whenever applicable use the unquoted style since it is the most readable. Use quotes when the value can be misinterpreted as a data type or the value contains a :. # values need quotes foo: &#39;{{ bar }}&#39; # need quotes to avoid interpreting as `dict` object foo: &#39;123&#39; # need quote to avoid interpreting as `int` object foo: &#39;yes&#39; # avoid interpreting as `boolean` object foo: &quot;bar:baz:bam&quot; # has colon, can be misinterpreted as key # values need not quotes foo: bar1baz234 bar: 123baz Ref: R Markdown anatomy, R Markdown Cookbook https://rmarkdown.rstudio.com/lesson-6.html File options Some aspects of markdown output can be customized via global, project, or file-level options, including: How to wrap / break lines (fixed column, sentence-per-line, etc.). Where to write footnotes (below the current paragraph or section, or at the end of the document). Whether to use the visual mode markdown writer when saving markdown from source mode (to ensure consistency between documents saved from either mode). Global and project options that affect the way markdown is written can also be customized on a per-file basis. These file specific options can be set using YAML. For instance, you want to set lines wrapping after 72 characters: --- editor_options: markdown: wrap: 72 --- Ref: https://rstudio.github.io/visual-markdown-editing/options.html#file-options 3.1.2 MathJax Options By default, MathJax scripts are included in HTML documents for rendering LaTeX and MathML equations. You can use the mathjax option to control how MathJax is included: Specify \"default\" to use an HTTPS URL from a CDN host (currently provided by RStudio). Specify \"local\" to use a local version of MathJax (which is copied into the output directory). Note that when using \"local\" you also need to set the self_contained option to false. Specify an alternate URL to load MathJax from another location. Specify null to exclude MathJax entirely. For example, to use a local copy of MathJax: --- title: &quot;Habits&quot; output: html_document: mathjax: local self_contained: false --- To use a self-hosted copy of MathJax: --- title: &quot;Habits&quot; output: html_document: mathjax: &quot;http://example.com/MathJax.js&quot; --- To exclude MathJax entirely: --- title: &quot;Habits&quot; output: html_document: mathjax: null --- Ref: https://bookdown.dongzhuoer.com/rstudio/rmarkdown-book/html-document#mathjax-equations 3.1.3 Document dependency By default, R Markdown produces standalone HTML files with no external dependencies, using data:URIs to incorporate the contents of linked scripts, stylesheets, images, and videos. This means you can share or publish the file just like you share Office documents or PDFs. If you would rather keep dependencies in external files, you can specify self_contained: false. Note that even for self-contained documents, MathJax is still loaded externally (this is necessary because of its big size). If you want to serve MathJax locally, you should specify mathjax: local and self_contained: false. One common reason to keep dependencies external is for serving R Markdown documents from a website (external dependencies can be cached separately by browsers, leading to faster page load times). In the case of serving multiple R Markdown documents you may also want to consolidate dependent library files (e.g.¬†Bootstrap, and MathJax, etc.) into a single directory shared by multiple documents. You can use the lib_dir option to do this. For example: --- title: &quot;Habits&quot; output: html_document: self_contained: false lib_dir: libs --- Loading LaTeX packages We can load additional LaTeX packages using the extra_dependencies option within the pdf_document YAML settings. This allows us to provide a list of LaTeX packages to be loaded in the intermediate LaTeX output document, e.g., --- title: &quot;Using more LaTeX packages&quot; output: pdf_document: extra_dependencies: [&quot;bbm&quot;, &quot;threeparttable&quot;] --- If you need to specify options when loading the package, you can add a sub-level to the list and provide the options as a list, e.g., output: pdf_document: extra_dependencies: caption: [&quot;labelfont={bf}&quot;] hyperref: [&quot;unicode=true&quot;, &quot;breaklinks=true&quot;] lmodern: null For those familiar with LaTeX, this is equivalent to the following LaTeX code: \\usepackage[labelfont={bf}]{caption} \\usepackage[unicode=true, breaklinks=true]{hyperref} \\usepackage{lmodern} The advantage of using the extra_dependencies argument over the includes argument introduced in Section 6.1 is that you do not need to include an external file, so your Rmd document can be self-contained. Includes HTML Output You can do more advanced customization of output by including additional HTML content or by replacing the core Pandoc template entirely. To include content in the document header or before/after the document body, you use the includes option as follows: --- title: &quot;Habits&quot; output: html_document: includes: in_header: header.html # inject CSS and JavaScript code into the &lt;head&gt; tag before_body: doc_prefix.html # include a header that shows a banner or logo. after_body: doc_suffix.html # include a footer template: template.html # custom templates --- An example header.html to load a MathJax extension textmacros. &lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ loader: {load: [&#39;[tex]/textmacros&#39;]}, tex: {packages: {&#39;[+]&#39;: [&#39;textmacros&#39;]}} }); &lt;/script&gt; PDF Output For example, to support Chinese characters. You can use includes and preamble.tex (can be any name, contains any pre-loaded latex code you want to run before your main text code, for setting up environment, loading pkgs, define new commands ‚Ä¶ Very flexible.) In the main Rmd: --- output: pdf_document: includes: in_header: latex/preamble.tex before_body: latex/before_body.tex after_body: latex/after_body.tex --- If you want to add anything to the preamble, you have to use the includes option of pdf_document. This option has three sub-options: in_header: loading necessary packages before_body: Styling that has the highest priority (as it will be loaded latest; if you put in in_header, it might be overridden by default settings) Dedication page like ‚ÄúThe books is dedicated to ‚Ä¶‚Äù (Ê≠§‰π¶ÁåÆÁªô‚Ä¶) An example of before_body.tex: % Styling that has the highest priority \\let\\tightlist\\relax % disable `\\tightlist` \\setlength{\\abovedisplayskip}{-5pt} \\setlength{\\abovedisplayshortskip}{-5pt} % book dedication page \\thispagestyle{empty} \\begin{center} ÁåÆÁªô‚Ä¶‚Ä¶ ÂëÉÔºåÁà±Ë∞ÅË∞ÅÂêß \\end{center} The default bookdown uses \\tightlist for all bullet lists, setting itemsep=0pt and parskip=0pt, aim for ‚Äúcompact lists.‚Äù See the following definition: \\providecommand{\\tightlist}{% \\setlength{\\itemsep}{0pt}\\setlength{\\parskip}{0pt}} I personally don‚Äôt like the compact list setting, so I disable it with \\let\\tightlist\\relax. To prevent it from being overridden, I put it in before_body.tex instead of preamble.tex. after_body. Each of them takes one or multiple file paths. The file(s) specified in in_header will be added to the preamble. The files specified in before_body and after_body are added before and after the document body, respectively. \\documentclass{article} % preamble \\begin{document} % before_body % body % after_body \\end{document} In preamble.tex: \\usepackage{xeCJK} \\setCJKmainfont{Noto Sans CJK SC} Alternatively, you can use header-includes but with less flexibility to change options: --- output: pdf_document header-includes: - \\usepackage{ctex} --- Q: includes vs.¬†header-includes, which one is better to use for loading LaTeX packages? A: Another way to add code to the preamble is to pass it directly to the header-includes field in the YAML frontmatter. The advantage of using header-includesis that you can keep everything in one R Markdown document. However, if your report is to be generated in multiple output formats, we still recommend that you use the includes method, because the header-includes field is unconditional, and will be included in non-LaTeX output documents, too. By comparison, the includes option is only applied to the pdf_document format. Ref: https://github.com/hao203/rmarkdown-YAML?tab=readme-ov-file#chinesejapanese-support https://bookdown.org/yihui/rmarkdown-cookbook/latex-preamble.html header-includes Tex style and package loading can also put in header-includes. Ex.1 --- output: pdf_document header-includes: - \\usepackage{fancyhdr} - \\pagestyle{fancy} - \\usepackage{ctex} #TeX package for Chinese - \\fancyhead[L]{MANUSCRIPT AUTHORS} - \\fancyhead[R]{MANUSCRIPT SHORT TITLE} - \\usepackage{lineno} # TeX package for line numbers - \\linenumbers --- Ex.2 --- title: Adding a Logo to LaTeX Title author: Michael Harper date: December 7th, 2018 output: pdf_document header-includes: - \\usepackage{titling} - \\pretitle{\\begin{center} \\includegraphics[width=2in,height=2in]{logo.jpg}\\LARGE\\\\} - \\posttitle{\\end{center}} --- Ex.3 To override or extend some CSS for just one document, include for example: --- output: html_document header-includes: | &lt;style&gt; blockquote { font-style: italic; } tr.even { background-color: #f0f0f0; } td, th { padding: 0.5em 2em 0.5em 0.5em; } tbody { border-bottom: none; } &lt;/style&gt; --- Change Font The default font is \\usepackage{lmodern} in bookdown. Can specify alternative fonts in preamble.tex as follows: \\usepackage{fontspec} \\setmainfont{Charter} Fonts known to LuaTeX or XeTEX may be loaded by their standard names as you‚Äôd speak them out loud, such as Times New Roman or Adobe Garamond. ‚ÄòKnown to‚Äô in this case generally means ‚Äòexists in a standard fonts location‚Äô such as ~/Library/Fonts on macOS, or C:\\Windows\\Fonts on Windows. In LuaTEX, fonts found in the TEXMF tree can also be loaded by name. In XeTEX, fonts found in the TEXMF tree can be loaded in Windows and Linux, but not on macOS. "],["3.2-render-rmd.html", "3.2 Render Rmd", " 3.2 Render Rmd When you click the Knit button (‚áß‚åòK) in RStudio, generally two processes happen: The .Rmd file is fed to knitr, which executes all of the R code chunks and creates a new markdown (.md) document which includes the R code and its output. The .md file is then processed by pandoc which is responsible for creating the finished format, e.g., HTML, PDF, MS_Word. .md files can be directly converted to html, but .md to pdf is time-consuming. It first generates .tex, then call the LaTeX engine to convert to pdf. There is one function that can do the processes mentioned above: rmarkdown::render. 3.2.1 Render a single document rmarkdown::render(input, output_format = NULL, output_file = NULL, output_dir = NULL, output_options = NULL, output_yaml = NULL) Arguments Definition output_format - \"all\" will render all formats define within the file- Name of a format, e.g., html_document, will render to that single format- An output format object, e.g., html_document(toc = TRUE, toc_depth = 2, includes = includes(before_body = \"header.htm\")), where you can pass on the argument output_dir Defaults to the directory of the input .Rmd file. output_options - List of output options that can override the options specified in metadata (e.g could be used to force self_contained or mathjax = \"local\"). - Note that this is only valid when the output format is read from metadata (i.e.¬†not a custom format object passed to output_format).- output_options cannot work together with xxx_document(). output_yaml Paths to YAML files specifying output formats and their configurations. The first existing one is used. If none are found, then the function searches YAML files specified to the output_yaml top-level parameter in the YAML front matter, _output.yml or _output.yaml, and then uses the first existing one. Use examples of render, using output format objects rmarkdown::render(&quot;0208-Rmd-GHpage.Rmd&quot;, bookdown::pdf_document2( latex_engine = &quot;xelatex&quot;, # template = &quot;latex/template.tex&quot;, includes = rmarkdown::includes( in_header = &quot;latex/preamble.tex&quot;, before_body = &quot;latex/before_body.tex&quot;) )) # This does NOT work as `output_options` is only valid when the format is not an output format object &quot;xxx_document()&quot; rmarkdown::render(&quot;0208-Rmd-GHpage.Rmd&quot;, bookdown::pdf_document2( latex_engine = &quot;xelatex&quot;, # template = &quot;latex/template.tex&quot;), output_options = list( includes = rmarkdown::includes( in_header = &quot;latex/preamble.tex&quot;, before_body = &quot;latex/before_body.tex&quot;) )) # render to html rmarkdown::render(&quot;0304-Quarto.Rmd&quot;, bookdown::html_document2( includes = rmarkdown::includes( in_header = &quot;head.html&quot;, before_body = &quot;scripts.html&quot;) )) Note that sometimes the bookdown cross references in Rmd are not rendered when using the Knit button. The rendered html shows Fig. \\@ref(fig:ar-res) (without the backslash). In this case, using rmarkdown::render() with output_format = bookdown::html_document2() might help. You can have more than one output formats for your Rmd. For example, you want both the html and pdf output. When you render the Rmd with rmarkdown::render(), it will use the first output format you specify in the YAML metadata (if it is missing, the default is html_document). If you do not want to use the first one, you can specify the one you want in the second argument, e.g., for an Rmd document input.Rmd with the metadata: output: html_document: toc: true pdf_document: keep_tex: true You can render it to PDF via: # Render to pdf rmarkdown::render(&#39;input.Rmd&#39;, &#39;pdf_document&#39;) # Render multiple formats render(&quot;input.Rmd&quot;, c(&quot;html_document&quot;, &quot;pdf_document&quot;)) # Render all formats defined rmarkdown::render(&#39;input.Rmd&#39;, &#39;all&#39;) RStudio calls the function rmarkdown::render() to render the document in a new R session. RStudio does this to ensure reproducibility. Fast rendering within the current global environment rmarkdown::render(active_document_path, envir=.GlobalEnv) What this does: Uses .GlobalEnv to access all objects already loaded in your workspace Faster rendering since it skips the overhead of starting a new R session All variables, functions, and loaded packages from your current session are available Caveats: ‚ö†Ô∏è Less reproducible - document depends on current session state ‚ö†Ô∏è Potential conflicts - objects in current session might interfere with document 3.2.2 Render multiple documents as a website rmarkdown::render_site(input = \".\", output_format = \"all\") Render all of the R Markdown documents within a directory as a website. There are two requirements for a directory to be rendered as a website: It must contain either an index.Rmd or index.md file. It must contain a site configuration file R Markdown websites: _site.yml. Bookdown websites: output.yml (output formats) and _bookdown.yml (book-specific metadata). Note that the ‚ÄúKnit‚Äù button in RStudio uses rmarkdown::render_site to knit the file in presence of an index.Rmd file in the working directory. Argument Definition input Website directory (or the name of a file within the directory). output_format R Markdown format to convert to (defaults to ‚Äúall‚Äù). encoding Ignored. The encoding is always assumed to be UTF-8. What will happen: All output and supporting files are copied to a ‚Äú_site‚Äù subdirectory of the website directory. This is configurable with output_dir in _site.yml. output_dir: indicates which directory to copy site content into. Refer to Rmd GitHub Pages for more details about rendering websites. Use example: # render the entire site, if input_file is not specified # render all output formats defined in `_output.yml` rmarkdown::render_site() # render the entire site for gitbook format only rmarkdown::render_site(output_format = &quot;bookdown::gitbook&quot;) # render a single file only rmarkdown::render_site(&quot;about.Rmd&quot;) rmarkdown::render_site(\"onefile.Rmd\") is useful when you want to render a single file in the site, e.g., about.Rmd or index.Rmd, without rendering the entire site. It is useful for testing changes to a single file without having to render the entire site. It uses all settings in _output.yml to render the file, such as the output format, output directory, and other options. It is faster than rendering the entire site because it only processes the specified file and its dependencies, rather than all files in the site. You can view the updates in the server http://127.0.0.1:port/. If the server has live reloading enabled, it automatically detects changes to the file and updates the output in real-time. Then you don‚Äôt need to restart or refresh the server to see the changes. "],["3.3-chunk-options.html", "3.3 Chunk Options", " 3.3 Chunk Options If you want to set chunk options globally, call knitr::opts_chunk$set() in a code chunk (usually the first one in the document), e.g., ```{r, label=&quot;setup&quot;, include=FALSE} knitr::opts_chunk$set( comment = &quot;#&gt;&quot;, echo = FALSE, fig.width = 6 ) ``` Chunk options are written in the form tag=value. A special chunk option is the chunk label (e.g., setup in the above example). Only the chunk label does not need a tag (i.e., you can only provide the value). If you prefer the form tag=value, you could also use the chunk option label explicitly, e.g., label=\"setup\". The settings will apply to every chunk in your file as a default. You can overwrite them in individual chunk headers. Full list of chunk options: https://yihui.org/knitr/options/ Chunk options can customize nearly all components of code chunks, such as the source code, text output, plots, and the language of the chunk. Here is a template I often use: ```{r, label=&quot;setup&quot;, include=FALSE} # set default chunk options knitr::opts_chunk$set( echo = TRUE, message = FALSE, warning = FALSE, fig.align = &quot;center&quot;, fig.pos = &quot;H&quot; ) # set default kable options opts &lt;- options(knitr.kable.NA = &quot;&quot;) ``` For demonstration purposes, use echo = TRUE to show the source code in the output document. For production documents, you may want to set echo = FALSE to hide the source code. This way, the output will show. If you want to further hide the output, use include = FALSE. This hides both the source code and the output, but the code is still evaluated, and plots are generated. Other languages are supported in Rmd You can list the names of all available engines via: names(knitr::knit_engines$get()) ## [1] &quot;awk&quot; &quot;bash&quot; &quot;coffee&quot; ## [4] &quot;gawk&quot; &quot;groovy&quot; &quot;haskell&quot; ## [7] &quot;lein&quot; &quot;mysql&quot; &quot;node&quot; ## [10] &quot;octave&quot; &quot;perl&quot; &quot;php&quot; ## [13] &quot;psql&quot; &quot;Rscript&quot; &quot;ruby&quot; ## [16] &quot;sas&quot; &quot;scala&quot; &quot;sed&quot; ## [19] &quot;sh&quot; &quot;stata&quot; &quot;zsh&quot; ## [22] &quot;asis&quot; &quot;asy&quot; &quot;block&quot; ## [25] &quot;block2&quot; &quot;bslib&quot; &quot;c&quot; ## [28] &quot;cat&quot; &quot;cc&quot; &quot;comment&quot; ## [31] &quot;css&quot; &quot;ditaa&quot; &quot;dot&quot; ## [34] &quot;embed&quot; &quot;eviews&quot; &quot;exec&quot; ## [37] &quot;fortran&quot; &quot;fortran95&quot; &quot;go&quot; ## [40] &quot;highlight&quot; &quot;js&quot; &quot;julia&quot; ## [43] &quot;python&quot; &quot;R&quot; &quot;Rcpp&quot; ## [46] &quot;sass&quot; &quot;scss&quot; &quot;sql&quot; ## [49] &quot;stan&quot; &quot;targets&quot; &quot;tikz&quot; ## [52] &quot;verbatim&quot; &quot;theorem&quot; &quot;lemma&quot; ## [55] &quot;corollary&quot; &quot;proposition&quot; &quot;conjecture&quot; ## [58] &quot;definition&quot; &quot;example&quot; &quot;exercise&quot; ## [61] &quot;hypothesis&quot; &quot;proof&quot; &quot;remark&quot; ## [64] &quot;solution&quot; &quot;marginfigure&quot; The engines from theorem to solution are only available when you use the bookdown package, and the rest are shipped with the knitr package. To use a different language engine, you can change the language name in the chunk header from r to the engine name, e.g., ```python x = &#39;hello, python world!&#39; print(x.split(&#39; &#39;)) ``` For engines that rely on external interpreters such as python, perl, and ruby, the default interpreters are obtained from Sys.which(), i.e., using the interpreter found via the environment variable PATH of the system. If you want to use an alternative interpreter, you may specify its path in the chunk option engine.path. For example, you may want to use Python 3 instead of the default Python 2, and we assume Python 3 is at /usr/bin/python3 ```{python, engine.path = &#39;/usr/bin/python3&#39;} import sys print(sys.version) ``` All outputs support markdown syntax. If the output is html, you can write in html syntax. The chunk label for each chunk is assumed to be unique within the document. This is especially important for cache and plot filenames, because these filenames are based on chunk labels. Chunks without labels will be assigned labels like unnamed-chunk-i, where i is an incremental number. Chunk label doesn‚Äôt need a tag, i.e., you only provide the value. If you prefer the form tag=value, you could also use the chunk option label explicitly, e.g., ```{r, label=&#39;my-chunk&#39;} # one code chunk example ``` You may use knitr::opts_chunk$set() to change the default values of chunk options in a document. Commonly used chunk options Complete list here. Or ?opts_chunk to get the help page. Options Definitions echo=TRUE Whether to display the source code in the output document.Use this when you want to show the output but NOT the source code itself. eval=TRUE Whether to evaluate the code chunk. include=TRUE Whether to include the chunk code and output in the output document‚Äîincluding source code, text output, messages, warnings, and plots. If FALSE, nothing will be written into the output document, but the code is still evaluated and plot files are generated if there are any plots in the chunk, so you can manually insert figures later. message=TRUE Whether to preserve messages emitted by message() warning=TRUE Whether to show warnings in the output produced by warning(). results='markup' Controls how to display the text results. When results='asis' that is to write text output as-is, i.e., write the raw text results directly into the output document without any markups.Useful when printing stargazer tables. comment='##' The prefix to be added before each line of the text output. Set comment = '' remove the default ##. collapse=FALSE Whether to, if possible, collapse all the source and output blocks from one code chunk into a single block (by default, they are written to separate blocks). This option only applies to Markdown documents. fig.keep='high' How plots in chunks should be kept. high: Only keep high-level plots (merge low-level changes into high-level plots). none: Discard all plots. all: Keep all plots (low-level plot changes may produce new plots). first: Only keep the first plot. last: Only keep the last plot. If set to a numeric vector, the values are indices of (low-level) plots to keep.If you want to choose the second to the fourth plots, you could use fig.keep = 2:4 (or remove the first plot via fig.keep = -1). fig.align=\"center\" Figure alignment. fig.pos=\"H\" A character string for the figure position arrangement to be used in \\begin{figure}[]. fig.cap Figure caption. results='markup' note plural form for results. results='markup': Defaults to `markup`. Mark up text output with the appropriate environments depending on the output format. results='markup' means to put the text output in fenced code blocks (```), treat output verbatim. ‚Üí Not interpreted as Markdown. Useful when you just want to show plain output, like from print(), cat(), or basic text generation. For example, for R Markdown, if the text output is a character string \"[1] 1 2 3\", the actual output that knitr produces will be: ``` [1] 1 2 3 ``` Ex2 ```{r, results=&#39;asis&#39;} cat(&quot;I&#39;m raw **Markdown** content.\\n&quot;) ``` will be rendered as ``` I&#39;m raw **Markdown** content. ``` Any markdown format like ** will not be rendenered as bold; will be printed as plain source code output. results='asis': Write text output as-is, i.e., write the raw text results directly into the output document without any markups. ‚Üí Output is interpreted as Markdown. Useful when you want to genrates Markdown that will be rendered directly. You must ensure the output is valid Markdown, or it may break the document. Ex1 ```{r, results=&#39;asis&#39;} cat(&quot;I&#39;m raw **Markdown** content.\\n&quot;) ``` Will be rendered as I‚Äôm raw Markdown content. Ex2 ```{r, results=&#39;asis&#39;} cat(&quot;## This is a level-2 header\\n&quot;) cat(&quot;*This will be italicized*\\n&quot;) cat(&quot;**This will be bold**\\n&quot;) ``` Will be rendered as This is a level-2 header This will be italicized This will be bold In summary, all formats will be applied. Other use cases: This can be particularly useful when you want to generate content dynamically from R code. ```{r, results=&#39;asis&#39;} cat(paste0(&quot;- `&quot;, names(iris), &quot;`&quot;), sep = &quot;\\n&quot;) ``` will be rendered as - `Sepal.Length` - `Sepal.Width` - `Petal.Length` - `Petal.Width` - `Species` Use stargazer to generate LaTeX tables. ```{r, include=TRUE, results=&#39;asis&#39;} stargazer(capm_ml, FF_ml, type=&#39;latex&#39;, header=FALSE, digits=4, no.space = TRUE, title=&quot;Regression Results for META&quot;, label = &quot;tab:reg-table&quot;) ``` This will outout LaTeX code to your document, which will be formatted scientifically. Sometime, you encounter the following error messages when you have R codes within enumerate environment. You can‚Äôt use macro parameter character # in horizontal mode. By default, knitr prefixes R output with ##, which can‚Äôt be present in your TeX file. Solution: specify results=\"asis\" in code chunks. results='hold': Hold all pieces of text output in a chunk and flush them to the end of the chunk. results='hide' (or results='FALSE'): Hide text output. collapse = FALSE Whether to merge text output and source code into a single code block in the output. The default FALSE means R expressions and their text output are separated into different blocks. collapse = TRUE makes the output more compact, since the R source code and its text output are displayed in a single output block. The default collapse = FALSE means R expressions and their text output are separated into different blocks. message=TRUE is often used together with warning=TRUE to control whether messages and warnings are displayed in the output document. message=FALSE will suppress messages from the output document, but the code is still evaluated and messages are still printed to the console. Often used to suppress messages from packages that are not relevant to the output document, such as package loading messages. 3.3.1 Global knitr Options First check your configuration (_bookdown.yml) when you are using new_session = FALSE, use index.Rmd to set global options. It will be applied to all files in the book. if you are using new_session = TRUE, you need to apply them per file because each file is knitted independently. You can create a file with the common R code and load this file in each document. Read external scripts into a chunk Two options if you want to read external scripts: Source .R scripts ```{r} source(&quot;setup.R&quot;) ``` You will NOT be able to see the source code by default. You can use source(..., echo = TRUE), but the source code will NOT be properly syntax highlighted. Use the¬†file option of a chunk. This method allows you to see the source code, and it has proper syntax highlighting. ```{r file=&#39;setup.R&#39;} ``` You can load as many scripts as you want, just put them in a character vector: ```{r file=c(&#39;one.R&#39;, &#39;two.R&#39;)} ``` You can even read scripts of other languages. ```{python, file=&#39;script.py&#39;} ``` .Rmd files If we have R Markdown documents that we want to share across several pages in the website, we can include them in the parent .Rmd document using the child chunk option. Note that the child .Rmd files need to be named with a leading _ so they are not compiled as standalone documents during the site rendering. ```{r, child = &quot;_setup.Rmd&quot;, include=FALSE} ``` In summary In the .Rmd files, source an R script and include a child .Rmd Ref: R Markdown Cookbook: ¬ß16.2 Read external scripts into a chunk 3.3.2 Hooks The object knit_hooks in the knitr package is used to set hooks; the basic usage is knitr::knit_hooks$set(name = FUN) (see objects for details) where name is the name of a chunk option (can be arbitrary), and FUN is a function. There are two types of hooks: chunk hooks and A chunk hook is a function that is triggered by a chunk option when the value of this chunk option is not NULL. output hooks Output hooks control over output from your code chunks, such as source code, text output, messages, and plots. Hook functions may have different forms, depending what they are designed to do. Chunk hooks For example, we define a custom chunk hook function for the small_mar option knitr::knit_hooks$set(small_mar = function(before, ...) { if (before) par(mar = c(4, 4, .1, .1)) # smaller top/right margin }) Then this function will be called for a chunk option like the following (small_mar doesn‚Äôt have to be TRUE; it can be any non-NULL value): ```{r, myplot, small_mar=TRUE} hist(rnorm(100), main = &#39;&#39;) ``` Output hooks With the knitr package, you have control over every piece of output from your code chunks, such as source code, text output, messages, and plots. The control is achieved through ‚Äúoutput hooks.‚Äù Available output hook names: names(knitr:::.default.hooks) ## [1] &quot;source&quot; &quot;output&quot; &quot;warning&quot; &quot;message&quot; ## [5] &quot;error&quot; &quot;plot&quot; &quot;inline&quot; &quot;chunk&quot; ## [9] &quot;text&quot; &quot;evaluate.inline&quot; &quot;evaluate&quot; &quot;document&quot; Note that these names of output hooks are reserved by knitr, so you must NOT use these names for your custom chunk hooks. source: processing the source code. output: processing text output. You may obtain the actual hooks from the object knit_hooks via the get() method, e.g., # for meaningful output, the code below should be # executed *inside* a code chunk of a knitr document knitr::knit_hooks$get(&quot;source&quot;) knitr::knit_hooks$get(&quot;output&quot;) # or knitr::knit_hooks$get(c(&#39;source&#39;, &#39;output&#39;)) A custom output hook is registered through the set() method of knit_hooks. Because this method will override the existing default hook, we recommend that you save a copy of an existing hook, process the output elements in your own way, and pass the results to the default hook. When the text output from a code chunk is lengthy, you may want to only show the first few lines. For example, when printing a data frame of a few thousand rows, it may not be helpful to show the full data, and the first few lines may be enough. Below we redefine the output hook so that we can control the maximum number of lines via a custom chunk option out.lines: # save the built-in output hook hook_output &lt;- knitr::knit_hooks$get(&quot;output&quot;) # set a new output hook to truncate text output knitr::knit_hooks$set(output = function(x, options) { if (!is.null(n &lt;- options$out.lines)) { x &lt;- xfun::split_lines(x) if (length(x) &gt; n) { # truncate the output x &lt;- c(head(x, n), &quot;....\\n&quot;) } x &lt;- paste(x, collapse = &quot;\\n&quot;) } hook_output(x, options) }) The basic idea of the above hook function is that if the number of lines of the text output is greater than the threshold set in the chunk option out.lines (stored in the variable n in the function body), we only keep the first n lines and add an ellipsis (....) to indicate the output is truncated. Now we can test the new output hook by setting the chunk option out.lines = 4 on the chunk below: ```{r out.lines = 4} print(cars) ``` ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 .... And you see four lines of output as expected. print(head(cars, 10)) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 ## 7 10 18 ## 8 10 26 ## 9 10 34 ## 10 11 17 Since we have stored the original output hook in hook_output, we can restore it by calling the set() method again: knitr::knit_hooks$set(output = hook_output) Now we print the data frame again. The default behavior is to print the whole data frame. print(head(cars, 10)) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 ## 7 10 18 ## 8 10 26 ## 9 10 34 ## 10 11 17 References: Output hooks: https://bookdown.org/yihui/rmarkdown-cookbook/output-hooks.html Chunk hooks: https://bookdown.org/yihui/rmarkdown-cookbook/chunk-hooks.html "],["3.4-print-verbatim-r-code-chunks.html", "3.4 Print Verbatim R code chunks", " 3.4 Print Verbatim R code chunks verbatim in line code use knitr::inline_expr. --- title: &quot;Test inline expr&quot; output: html_document --- To use `chunk_reveal(&quot;walrus&quot;, title = &quot;## Walrus operator&quot;)` inline, you can wrap it in R inline chunk like this `` `r chunk_reveal(&quot;walrus&quot;, title = &quot;## Walrus operator&quot;)` `` Including verbatim R code chunks inside R Markdown One solution for including verbatim R code chunks (see below for more) is to insert hidden inline R code (`r ''`) immediately before or after your R code chunk. The hidden inline R code will be evaluated as an inline expression to an empty string by knitr. Then wrap the whole block within a markdown code block. The rendered output will display the verbatim R code chunk ‚Äî including backticks. R code generating the four backticks block: output_code &lt;- &quot;````markdown ```{r} plot(cars) ``` \\n````&quot; cat(output_code) Write this code in your R Markdown document: output_code &lt;- &quot;````markdown `r &#39;&#39;````{r} plot(cars) ``` \\n````&quot; cat(output_code) ````markdown `r &#39;&#39;````{r} plot(cars) ``` ```` or output_code &lt;- &quot;````markdown ```{r}`r &#39;&#39;` plot(cars) ``` \\n````&quot; cat(output_code) ````markdown ```{r}`r &#39;&#39;` plot(cars) ``` ```` Knit the document and the code will render like this in your output: ```{r} plot(cars) ``` This method makes use of Markdown Syntax for code. Q: What is the Markdown Syntax for code? A: Inline code use a pair of backticks, e.g., `code`. To use \\(n\\) literal backticks, use at least \\(n+1\\) backticks outside. Note that use a space (‚ê£) to separate your outside backticks from your literal backtick(s). For example, to generate `code`, you use ``‚ê£`code`‚ê£`` (i.e., two backticks + space + one backtick + code + one backtick + space + two backticks). Note that you need to write sequentially. Plain code blocks can be written either After three or more backticks (fenced code blocks), or Can also use tildes (~) Indent the blocks by four spaces (indented code blocks) Special characters do not trigger special formatting, and all spaces and line breaks are preserved. Blank lines in the verbatim text need not begin with four spaces. Note that code blocks must be separated from surrounding text by blank lines. If the code itself contains a row of tildes or backticks, just use a longer row of tildes or backticks at the start and end: ~~~~~~~~~~~~~~~~ ~~~~~~~~~~ code including tildes ~~~~~~~~~~ ~~~~~~~~~~~~~~~~ These begin with a row of three or more tildes (~) and end with a row of tildes that must be at least as long as the starting row. A trick if you don‚Äôt want to type more than three tildes or backticks is that you just use different inner and outer symbols. ~~~markdown ```r print (&quot;hello world&quot;) ``` ~~~ Will be rendered as: ```r print (&quot;hello world&quot;) ``` A shortcut form (without braces) can also be used for specifying the language of the code block: ```haskell qsort [] = [] ``` This is equivalent to: ``` {.haskell} qsort [] = [] ``` haskell is the language class. You can add more classes, such as numberLines for adding line numbers. This shortcut form may be combined with attributes: ```haskell {.numberLines} qsort [] = [] ``` Which is equivalent to: ``` {.haskell .numberLines} qsort [] = [] ``` and &lt;pre id=&quot;mycode&quot; class=&quot;haskell numberLines&quot; startFrom=&quot;100&quot;&gt; &lt;code&gt; primes = filterPrime [2..] where filterPrime (p:xs) = p : filterPrime [x | x &lt;- xs, x `mod` p /= 0] &lt;/code&gt; &lt;/pre&gt; If highlighting is supported for your output format and language, then the code block above will appear highlighted, with numbered lines starting with 100, 101, and go on. primes = filterPrime [2..] where filterPrime (p:xs) = p : filterPrime [x | x Code chunks within enumerate Mind the indentation. Rstudio does not automatically adjust indentation for codes. specify results=\"asis\" if encounter You can‚Äôt use `macro parameter character #‚Äô in horizontal mode. cross references using bookdown (\\@ref{fig:scatter-plot}) might not work. Use latex references \\ref{fig:scatter-plot} (base latex) or \\autoref{fig:scatter-plot} (from hyperref package) markdown language does not work well inside latex environments. A possible workaround is use 1 and indent four spaces for contents that follow. If it is still a pain in the ass, use this solution. Basically, just copy the output from R condole and paste in Rmd. References: https://yihui.org/en/2017/11/knitr-verbatim-code-chunk/ https://support.posit.co/hc/en-us/articles/360018181633-Including-verbatim-R-code-chunks-inside-R-Markdown https://themockup.blog/posts/2021-08-27-displaying-verbatim-code-chunks-in-xaringan-presentations/ Pandoc‚Äôs Markdown: https://pandoc.org/MANUAL.html#fenced-code-blocks "],["3.5-rmd-basics.html", "3.5 Rmd Basics", " 3.5 Rmd Basics To name a chunk, add the name after r, it‚Äôs not necessary to add label='chunk-name', but it is possible to do so if you prefer the form tag=value. The Chunk Label Must be unique within the document. This is especially important for cache and plot filenames, because these filenames are based on chunk labels. Chunks without labels will be assigned labels like unnamed-chunk-i, where i is an incremental number. Avoid spaces (‚ê£), periods ( .), and underscores (_) in chunk labels and paths. If you need separators, you are recommended to use hyphens (-) instead. knitr::opts_chunk$set() changes the default values of chunk options in a document. See here for commonly used chunk options. Unnumbered Sections Add {-} at the end of the section title. # Question 1: Variance and Covariance properties {-} &lt;!-- equivalently, you can use {.unnumbered} --&gt; # Question 1: Variance and Covariance properties {.unnumbered} Note that the section won‚Äôt be numbered but will show in the TOC. If you want to further exclude it from the TOC: # Question 1: Variance and Covariance properties {.unlisted .unnumbered} Headings with # will appear in the file outline, which is a convenient feature. So use this method whenever possible. One exception is level 2 headings in Bookdown: By default Bookdown starts a new page for each level 2 heading. If you want to keep the style without starting a new page, use an html tag. The heading won‚Äôt be numbered or included in TOC. However, a downside is that the heading won‚Äôt show up in the file outline either, making them harder to locate. &lt;h2&gt;YAML metadata&lt;/h2&gt; Add Section ID To add a section ID, use {#section-id} at the end of the section title. This is useful for linking to specific sections within the document or from other documents. Add a section ID # Question 1: Variance and Covariance properties {#variance-covariance} Refer to this section using the ID: [Variance and Covariance properties](#variance-covariance). Bookdown supports cross files linking. By default, Pandoc will generate an ID for all section headers, e.g., a section # Hello World will have an ID hello-world. However, we recommend you to manually assign an ID to a section header to make sure you do not forget to update the reference label after you change the section header. Further attributes of section headers can be set using standard Pandoc syntax. Q: How to cross reference a regular text across files? A: There‚Äôs no built-in @ref() syntax for referencing arbitrary inline text. You can achieve this using html anchors. In the source .Rmd file (say, chapter1.Rmd), write: &lt;span id=&quot;mytext&quot;&gt;This is the important concept you want to reference later.&lt;/span&gt; Then, in another .Rmd file (say, chapter2.Rmd), link to it with: See [this explanation](#mytext) in Chapter 1. This works for HTML output only. Knitting in the global environment rmarkdown::render(&quot;/Users/menghan/Library/CloudStorage/OneDrive-Norduniversitet/EK369E/Seminars/w1.rmd&quot;, envir=.GlobalEnv) Advantages: fast; load and output results in the global environment; easy to inspect afterwards. Rmd has many built-in themes which can be conveniently applied to your html document. See here for a preview for some popular html themes: https://rstudio4edu.github.io/rstudio4edu-book/rmd-themes.html Use the following code in either _site.yml (if R Markdown websites) or _output.yml(if Bookdown websites) to set the theme: html_document: toc: true toc_depth: 2 theme: cerulean highlight: tango theme¬†specifies the theme to use for the presentation (available themes are¬†\"default\",¬†\"simple\",¬†\"sky\",¬†\"beige\",¬†\"serif\",¬†\"solarized\",¬†\"blood\",¬†\"moon\",¬†\"night\",¬†\"black\",¬†\"league\", and¬†\"white\"). highlight¬†specifies the syntax highlighting style. Supported styles include¬†\"default\",¬†\"tango\",¬†\"pygments\",¬†\"kate\",¬†\"monochrome\",¬†\"espresso\",¬†\"zenburn\", and¬†\"haddock\". Pass null to prevent syntax highlighting. .Rmd documents can be edited in either source or visual mode. To switch into visual mode for a given document, use the Source or Visual button at the top-left of the document toolbar (or alternatively the Cmd+Shift+F4 keyboard shortcut). Visual mode Visual mode allows you to preview the effect after having compiled the markdown file. ‚ùóÔ∏èBut it modifies your code silently, be cautions with visual mode. More user-friendly in terms of providing drop down menus for editing. Visual mode supports both traditional keyboard shortcuts (e.g.¬†Cmd + B for bold) as well as markdown shortcuts (using markdown syntax directly). For example, enclose **bold** text in asterisks or type ## and press space to create a second level heading. One bug for Visual mode is that inside bullet points, $ is automatically escaped as \\$. In this case, use cmd+/ and choose inline math to insert an eqn. When type inline equations, first type $ then the equation, then $ at last. Do not type $$ at one time. Otherwise, they will be escaped as regular text. Comments in Rmd In both html and pdf outputs, use the following to write true comments you don‚Äôt want to show in the rendered file. &lt;!-- regular html comment --&gt; Link to an external javascript &lt;SCRIPT language=&quot;JavaScript&quot; SRC=&quot;my_jxscript.js&quot;&gt;&lt;/SCRIPT&gt; Tips: In general, you‚Äôd better leave at least one empty line between adjacent but different elements, e.g., a header and a paragraph. This is to avoid ambiguity to the Markdown renderer. For example, the - in the list below cannot be recognized as a bullet point. You need to add a black line before the bullet list. The result of 5 - 3 is 2. Different flavors of Markdown may produce different results if there are no blank lines. üôàüôà Need to escape @ in the text by \\@ in bookdown, otherwise, it will be interpreted as a citation key, e.g., @author or @citekey. Special characters that have specific meanings in Markdown, such as *, _, #, +, -, should be escaped with a backslash (\\) if you want to display them literally. \\* Without the backslash, this would be a bullet in an unordered list. "],["3.6-citations.html", "3.6 Citations", " 3.6 Citations For an overview of including bibliographies in your output document, you may see Section 2.8 of Xie (2016). The basic usage requires us to specify a bibliography file using the bibliography metadata field in YAML. For example: --- output: html_document bibliography: references.bib --- where the BibTeX database is a plain-text file with the *.bib extension that consists of bibliography entries. How to cite in text: Use @citationkey to cite references in text. To put citations in parentheses, use [@citationkey]. To cite multiple entries, separate the keys by semicolons, e.g., [@key-1; @key-2; @key-3]. To suppress the mention of the author, add a minus sign before @, e.g., [-@citationkey]. Syntax Result @adams1975 concludes that ‚Ä¶ Adams (1975) concludes that ‚Ä¶ @adams1975[p.33] concludes that ‚Ä¶ Adams (1975, p.¬†33) concludes that ‚Ä¶ ‚Ä¶ end of sentence [@adams1975]. ‚Ä¶ end of sentence (Adams, 1975). [see @adams1975,p.33]. ‚Ä¶ end of sentence (see Adams, 1975, p.¬†33). delineate multiple authors with colon: [@adams1975; @aberdeen1958] delineate multiple authors with colon: (Aberdeen, 1958; Adams, 1975) Check Lo and MacKinlay [-@Lo-Mackinlay1988; -@Lo1989] for example. Check Lo and MacKinlay (1988, 1989) for example. Add an item to bibliography without using it By default, the bibliography will only display items that are directly referenced in the document. If you want to include items in the bibliography without actually citing them in the body text, you can define a dummy nocite metadata field and put the citations there. --- nocite: | @item1, @item2 --- 3.6.1 Bibliographies Users may also choose to use either natbib (based on bibtex) or biblatex as a ‚Äúcitation package‚Äù. In this case, the bibliographic data files need to be in the bibtex or biblatex format, and the document output format is limited to PDF. output: pdf_document: citation_package: natbib bookdown::pdf_book: citation_package: biblatex If you use matching styles (e.g., biblio-style: apa for biblatex along with csl: apa.csl for pandoc-citeproc), output to PDF and to non-PDF formats will be very similar, though not necessarily identical. Once you have one or multiple .bib files, you may use the field bibliography in the YAML metadata of your first R Markdown document (which is typically index.Rmd in bookdown projects), and you can also specify the bibliography style via biblio-style (this only applies to PDF output), e.g., --- bibliography: [&quot;one.bib&quot;, &quot;another.bib&quot;, &quot;yet-another.bib&quot;] biblio-style: &quot;apalike&quot; link-citations: true --- The field link-citations can be used to add hyperlinks from the citations to the bibliography entries. Defaults to false. For any non-PDF output format, pandoc-citeproc is the only available option. If consistency across PDF and non-PDF output formats is important, use pandoc-citeproc throughout. To change the bibliography style, you will need to specify a CSL (Citation Style Language) file in the csl metadata field, e.g., --- output: html_document bibliography: references.bib csl: biomed-central.csl --- 3.6.2 Bibliography placement By default, the bibliography will be placed at the end of the document. So, you will want a final header titled # References or # Bibliography at the end your document. If you want to place the bibliography somewhere else, for instance before the appendices, you can insert a &lt;div id=\"refs\"&gt;&lt;/div&gt; html tag in source mode: # References &lt;div id=&quot;refs&quot;&gt;&lt;/div&gt; # Appendix Generation of the bibliography can be suppressed by setting suppress-bibliography: true in the YAML metadata in index.Rmd (rarely used, as you do want a reference section). If you use the bookdown::gitbook output format, by default, the bibliography is split (split_bib: true) and all citation items that are cited on a given html page are put at the end of that page, so that readers do not have to navigate to a different bibliography page to see the details of citations. This feature can be disabled by setting the split_bib YAML field to false, in which case all citations cited in the entire report or book are put on a separate bibliography page. To do this, you can add specific keys in the YAML header in _output.yml: --- author: Research Institute for Nature and Forest date: &#39;2025-08-26&#39; site: bookdown::bookdown_site output: bookdown::gitbook: split_by: chapter split_bib: false --- When the output format is LaTeX, the list of references will be automatically put in a chapter or section at the end of the document. For non-LaTeX output, you can add an empty reference chapter as the last chapter of your book. For example, if your last chapter is the Rmd file 06-references.Rmd, its content can be an inline R expression: # References {-} This will create a level 1 chapter for References used in the whole book. If you don‚Äôt have this command, the full list of references will be printed twice at the last section of your book. For more detailed instructions and further examples on how to use citations, please see the ‚ÄúCitations‚Äù section of the Pandoc manual: https://pandoc.org/MANUAL.html#citations https://bookdown.dongzhuoer.com/rstudio/bookdown/citations "],["3.7-cross-references.html", "3.7 Cross References", " 3.7 Cross References 3.7.1 Using bookdown You can number and refer to an equation by adding \\begin{equation} along with a label, provided with (\\#eq:label). The position of the label matters. For single-lined equations: First write your equation, then append your label (\\#eq:label). Otherwise, your equation won‚Äôt be rendered. For multi-lined equations: append (\\#eq:label) after \\end{split}, \\end{aligned} ‚Ä¶ Note that \\begin{equation} must NOT be quoted in $$...$$ for the equation to be rendered. Otherwise, will cause ‚ÄúBad math delimiter‚Äù error at the time of tex compilation for pdf output. Might be alright for html output though. Unexpected consequence: Without the $$...$$, RStudio won‚Äôt provide previews for equations. For temporary preview in RStudio at the composing stage, you can enclose the whole math environment in $$...$$. But remember to delete them when you are done editing the equation. See this post by Kenji Sato for a more efficient workaround. You can then refer to the equation in text using \\@ref(eq:CJ). Remember to put the label in parentheses (). General syntax for other environments: \\@ref(type:label) where type is the environment being referenced, and label is the chunk label. This is an equation redered using bookdown \\begin{equation} (\\#eq:CJ) y=\\beta_0 + \\beta_1x + e_t \\end{equation} will render as \\[\\begin{equation} y=\\beta_0 + \\beta_1x + e_t \\tag{3.1} \\end{equation}\\] You may refer to it using eqn \\@ref(eq:CJ), e.g., see eqn (3.1). Multilined equations. \\begin{equation} \\begin{aligned} y_i &amp;= f(x_{1i}, x_{2i}, \\ldots, x_{Ki}) + \\varepsilon_i \\\\ &amp;= x_{1i} \\beta_1 + x_{2i} \\beta_2 + \\cdots + x_{Ki} \\beta_K + \\varepsilon_i \\end{aligned}(\\#eq:scalar-form) \\end{equation} will render as \\[\\begin{equation} \\begin{aligned} y_i &amp;= f(x_{1i}, x_{2i}, \\ldots, x_{Ki}) + \\varepsilon_i \\\\ &amp;= x_{1i} \\beta_1 + x_{2i} \\beta_2 + \\cdots + x_{Ki} \\beta_K + \\varepsilon_i \\end{aligned}\\tag{3.2} \\end{equation}\\] You may refer to it using eqn \\@ref(eq:scalar-form), e.g., see eqn (3.2) . Note that For HTML output, bookdown can only number the equations with labels. Please make sure equations without labels are not numbered by either using the equation* environment or adding \\nonumber or \\notag to your equations. Troubleshooting Issue: Bad math environment delimiter on conversion to pdf when using equation or align. Cause: The error happens because I enclosed \\begin{equation} environment in $$. I did this as the dollar sings enable equation rendering and preview in file. Fix: remove the double signs. The following equation causes error. Need to remove the dollar signs. $$ \\begin{equation} y=x+2 \\end{equation} $$ Headers # Introduction {#intro} This is Chapter \\@ref(intro) The above is the bookdown syntax for cross-referencing headers. \\@ref(intro) creates a link to the header numbering with the ID intro. Sometimes you want to create a text-based link. You can use the markdown syntax [link text][#ID]. It will create a link to link text that points to the header with ID ID. Refer to Markdown: cross references for more examples. Note that cross references between documents are supported by bookdown. Don‚Äôt need to specify the file name, just use the ID of the header in the other document. Bookdown will automatically link to the correct document. More about Section IDs: By default, Pandoc will generate an ID for all section headers, e.g., a section # Hello World will have an ID hello-world. However, we recommend you to manually assign an ID to a section header to make sure you do not forget to update the reference label after you change the section header. To assign an ID to a section header, simply add {#id} to the end of the section header. Further attributes of section headers can be set using standard Pandoc syntax. Figures See Figure \\@ref(fig:cars-plot) ```{r cars-plot, fig.cap=&quot;A plot caption&quot;} plot(cars) # a scatterplot ``` Tables See Table \\@ref(tab:mtcars) ```{r mtcars} knitr::kable(mtcars[1:5, 1:5], caption = &quot;A caption&quot;) ``` Theorems See Theorem \\@ref(thm:boring) ```{theorem, boring} Here is my theorem. ``` Equations See equation \\@ref(eq:linear) \\begin{equation} a + bx = c (\\#eq:linear) \\end{equation} 3.7.2 Using the LaTeX Way The LaTeX way allows you to assign your own labels by \\tag. One drawback is that this does not allow preview of equations. Add the following script at the beginning of your document body: &lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } }); &lt;/script&gt; It configures MathJax to automatically number equations. Source. In the text, use label{eq:label}. If you want to provide a specific number to the equation, you can use \\tag{XX.XX}. Note that \\begin{equation} is NOT inside $$ ...$$! Cite using $\\ref{eq:label}$ (no parenthesis) or $\\eqref{eq:label}$ (with parenthesis). The dollar sign $ here around \\ref and \\eqref is not essential. Commands work with or without $. Without using the bookdown package. \\begin{equation} \\label{eq:test} \\tag{my custom label} Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\end{equation} Cite Equation $\\eqref{eq:test}$ like this. \\[\\begin{equation} \\label{eq:test} \\tag{my label} Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\end{equation}\\] Refer to the eq \\(\\eqref{eq:test}\\) Reference: https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html#equations "],["3.8-equations-1.html", "3.8 Equations", " 3.8 Equations Can use $...$ ($$...$$ for blocks) or \\(...\\) (\\[...\\] for blocks) to enclose equations. Difference: $...$ provides rendered equation previews in RStudio. \\(...\\) does not have previews. Rstudio equation previews do NOT work well with indented equations. \\(\\rightarrow\\) reduce indentation Â¶ÇÊûúÂÖ¨ÂºèÁº©ËøõÔºåRstudio ÂÖ¨ÂºèÈ¢ÑËßàÂäüËÉΩÂèØËÉΩ‰∏çËØÜÂà´„ÄÇÂú®‰∏çÂΩ±ÂìçÁêÜËß£ÁöÑÂâçÊèê‰∏ãÔºåÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑÁº©Ëøõ‰ª•‰æøÈ¢ÑËßàÂÖ¨Âºè„ÄÇ Multi-case functions using \\begin{cases} \\begin{align*} I_t = \\begin{cases} 1 &amp; \\text{if } r_t&gt;0 \\\\ 0 &amp; \\text{if } r_t\\leq0 \\end{cases} \\end{align*} will render as \\[\\begin{align*} I_t = \\begin{cases} 1 &amp; \\text{if } r_t&gt;0 \\\\ 0 &amp; \\text{if } r_t\\leq0 \\end{cases} \\end{align*}\\] For equation numbering support in bookdown you need to assign labels. For equation numbering support in bookdown::pdf_document2 you need to assign labels. Default behavior is not adding numbering. Use \\begin{equation}...\\end{equations} or \\begin{align}...\\end{align} environments. Use (\\#eq:eq1) or \\label{eq:eq1} to add labels. Automatically add numbering. Drawback is that rmd does not have preview of equations. Do NOT enclose the environments in double dollar signs $$. Otherwise, no label is added, but cross-references still show up. $$ do not add numbering automatically. But in bookdown::html_document2, it is ok to use $$ \\begin{equation} \\hat{\\beta}_{\\text{OLS}} = \\left(\\sum_{i=1}^n x_i x_i&#39; \\right)^{-1} \\left(\\sum_{i=1}^n x_i y_i \\right) . (\\#eq:simple-lm) \\end{equation} $$ Assign labels using the syntax (\\#eq:label). The above equation will be rendered as \\[ \\begin{equation} \\hat{\\beta}_{\\text{OLS}} = \\left(\\sum_{i=1}^n x_i x_i&#39; \\right)^{-1} \\left(\\sum_{i=1}^n x_i y_i \\right) . \\tag{3.3} \\end{equation} \\] Then reference with Eq. \\@ref(eq:simple-lm), e.g., see Eq. (3.3). Use \\@ref(eq:eq1) (note this uses parentheses) or the Latex command \\eqref{eq:eq1} (this uses curly braces) to cite the equation label. Load the dataset and calculate the monthly return in month $r$ ($r_t$) as \\begin{equation} r_t = \\frac{P_t-P_{t-1}}{P_{t-1}} = \\frac{P_t}{P_{t-1}}-1 , (\\#eq:eq1) \\end{equation} where $P_t$ is the adjusted price in month $t$. Test equation1 \\@ref(eq:eq1). Test equation2 \\eqref{eq:eq1}. Multilined equations. Use the¬†split¬†environment inside¬†equation¬†so that all lines share the same number. Note that the label (\\#eq:label) must be placed after \\end{split}. \\begin{equation} \\begin{split} \\mathrm{Var}(\\hat{\\beta}) &amp; =\\mathrm{Var}((X&#39;X)^{-1}X&#39;y)\\\\ &amp; =(X&#39;X)^{-1}\\sigma^{2} \\end{split} (\\#eq:var-beta) \\end{equation} will be rendenered as \\[ \\begin{equation} \\tag{3.4} \\begin{split} \\mathrm{Var}(\\hat{\\beta}) &amp; =\\mathrm{Var}((X&#39;X)^{-1}X&#39;y)\\\\ &amp; =(X&#39;X)^{-1}\\sigma^{2} \\end{split} \\end{equation} \\] Note that the label (\\#eq:var-beta) must be placed before \\begin{split} or after \\end{split}, i.e., outside the split environment. Then reference with Eq. \\@ref(eq:var-beta), e.g., see Eq. (3.4). Each line in the¬†align¬†environment will be assigned an equation number. We suppressed the number of the first line in the previous example using¬†\\notag. Naming conventions for equation labels in bookdown: Equation labels must start with the prefix eq:. In qmd, eq labels start with eq-. All labels must only contain alphanumeric characters, :, -, and/or /. Do NOT use underscore _. ‚ùå Note that qmd supports underscores in eq labels. If you want to provide a specific number to the equation, you can use \\tag{XX.XX}. With LaTeX LaTeX allows custom labels. \\begin{align} \\label{eq:my-label-latex} \\tag{my label latex} \\frac{p(x)}{1-p(x)} = \\exp (\\beta_0+\\beta_1 x) \\,. \\end{align} will be rendered as \\[\\begin{align} \\label{eq:my-label-latex} \\tag{my label latex} \\frac{p(x)}{1-p(x)} = \\exp (\\beta_0+\\beta_1 x) \\,. \\end{align}\\] My specific label here, see eq \\(\\eqref{eq:my-label-latex}\\) (\\eqref{eq:my-label-latex}). Cross reference using bookdown does NOT work: @ref{eq:my-label-latex} (eq. \\@ref(eq:my-label-latex)). With bookdown bookdown does NOT support custom tag though. Don‚Äôt use this. \\begin{align} \\frac{p(x)}{1-p(x)} = \\exp (\\beta_0+\\beta_1 x) \\,. (\\#eq:my-label-bookdown) \\tag{my label bookdown} \\end{align} will be rendered as \\[ \\begin{align} \\tag{3.5} \\tag{my label bookdown} \\frac{p(x)}{1-p(x)} = \\exp (\\beta_0+\\beta_1 x) \\,. \\end{align} \\] The equation does not show properly; there are two \\tag{}. My specific label here, see eq (3.5) (eq. \\@ref(eq:my-label-bookdown)). Color eqns using \\color{#00CC66}{...}. But sometime everything follows gets colored. You may want to use {\\color{#00CC66} ... } instead. $$ \\color{#008B45}{Y_t} = I_tI_{t-1} + (1-I_t)(1-I_{t-1}) $$ \\[\\begin{align*} {\\color{red}Y_t} = I_tI_{t-1} + (1-I_t)(1-I_{t-1}) \\end{align*}\\] This only works for color names, not hex codes starting with #, because html requires the # followed by 6 characters to define a color, but LaTeX package xcolor specifically excludes # in color specifications. Here is an (only works for LaTeX). A workaround: We can write a custom R function to insert the correct syntax depending on the output format using the is_latex_output() and is_html_output() functions in knitr as follows: colorize &lt;- function(x, color) { if (knitr::is_latex_output()) { sprintf(&quot;\\\\textcolor{%s}{%s}&quot;, color, x) } else if (knitr::is_html_output()) { sprintf(&quot;&lt;span style=&#39;color: %s;&#39;&gt;%s&lt;/span&gt;&quot;, color, x) } else x } We can then use the code in an inline R expression `r colorize(\"some words in red\", \"red\")`, which will create some words in red, which works for both html and . Mathjax https://bookdown.org/yihui/rmarkdown/html-document.html#mathjax-equations RStudio IDE comes with Mathjax 2.7 bundled. This is used by rmarkdown when local mode is used for example. Path is stored in Sys.getenv(\"RMARKDOWN_MATHJAX_PATH\"). &gt; Sys.getenv(&quot;RMARKDOWN_MATHJAX_PATH&quot;) [1] &quot;/Applications/RStudio.app/Contents/Resources/app/resources/mathjax-27&quot; &gt; rmarkdown:::mathjax_config() [1] &quot;MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; For online mode, rmarkdown uses https://mathjax.rstudio.com/latest/ with config MathJax.js?config=TeX-AMS-MML_HTMLorMML for now. Default configuration used by the rmarkdown package is given by rmarkdown:::mathjax_config(). As of rmarkdown v2.1, the function returns ‚ÄúMathJax.js?config=TeX-AMS-MML_HTMLorMML‚Äù. This configures Mathjax to HTML-CSS. Change Mathjax configuration to CommonHTML using the following codes. --- title: &quot;Trouble with MathJax&quot; output: html_document: mathjax: &quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js&quot; self_contained: false --- By default, MathJax scripts are included in HTML documents for rendering LaTeX and MathML equations. You can use the mathjax option to control how MathJax is included: Specify \"default\" to use an HTTPS URL from a CDN host (currently provided by RStudio). Specify \"local\" to use a local version of MathJax (which is copied into the output directory). Note that when using \"local\" you also need to set the self_contained option to false. Specify an alternate URL to load MathJax from another location. To use a self-hosted copy of MathJax. Specify null to exclude MathJax entirely. Q: Why my eqns are not rendered? A: MathJax is unlikely to work offline. Check internet connection. You load MathJax into a web page by including its main JavaScript file into the page. That is done via a &lt;script&gt;tag that links to the MathJax.js file. To do that, place the following line in the &lt;head&gt; section of your document. For example, if you are using the MathJax distributed network service, the tag might be &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js&quot;&gt; &lt;/script&gt; MathJax is available as a web service from cdn.mathjax.org, so you can obtain MathJax from there without needing to install it on your own server. The CDN is part of a distributed ‚Äúcloud‚Äù network, so it is handled by servers around the world. That means that you should get access to a server geographically near you, for a fast, reliable connection. The CDN hosts the most current version of MathJax, as well as older versions, so you can either link to a version that stays up-to-date as MathJax is improved, or you can stay with one of the release versions so that your pages always use the same version of MathJax. 3.8.1 RStudio Equation Live Preview Q: How to define custom macros that can be used in live equation preview in RStudio? A: Go to /Applications/RStudio.app/Contents/Resources/app/resources/mathjax-27/config/TeX-MML-AM_CHTML.js, add your custom macros as follows. MathJax.Hub.Config({ extensions: [&#39;[a11y]/accessibility-menu.js&#39;], TeX: { Macros: { // define TeX macros RR: &quot;{\\\\bf R}&quot;, bold: [&quot;{\\\\bf #1}&quot;,1], btheta: &quot;\\\\boldsymbol{\\\\theta}&quot;, bAlpha: &quot;\\\\boldsymbol{\\\\Alpha}&quot;, } }, }); RStudio DevTools RStudio functions like a web browser, you can use a web inspector to inspect every element of the appearance. Simply right-click pretty much anywhere on RStudio, and hit ‚ÄúInspect Element‚Äù and RStudio DevTools will appear. The MathJax supporting files can be found in Sources ‚Üí Page ‚Üí Electron Isolated Context ‚Üí mathjax. You will see MathJax.js?config=TeX-MML-AM_CHTML is loaded to parse and enable live previews of equations. The user configuration file is located under config/. You will see your custom defined macros here. "],["3.9-theorems-1.html", "3.9 Theorems", " 3.9 Theorems https://stackoverflow.com/questions/50379923/bookdown-remark-environment Language internationalization: https://bookdown.org/yihui/bookdown/internationalization.html Theorem environments in the bookdown package. Table: Theorem environments in bookdown. Environment Printed Name Label Prefix theorem Theorem thm lemma Lemma lem corollary Corollary cor proposition Proposition prp conjecture Conjecture cnj definition Definition def example Example exm exercise Exercise exr hypothesis Hypothesis hyp Definition ÂÆö‰πâ: an explanation of the mathematical meaning of a word. Theorem ÂÆöÁêÜ: A statement that has been proven to be true. ÊòØÊñáÁ´†‰∏≠ÈáçË¶ÅÁöÑÊï∞Â≠¶ÂåñÁöÑËÆ∫Ëø∞Ôºå‰∏ÄËà¨Êúâ‰∏•Ê†ºÁöÑÊï∞Â≠¶ËØÅÊòé„ÄÇ Proposition ÂëΩÈ¢ò: A less important but nonetheless interesting true statement. ÁªèËøáËØÅÊòé‰∏î interstingÔºå‰ΩÜÊ≤°Êúâ Theorem ÈáçË¶ÅÔºåÊØîËæÉÂ∏∏Áî®„ÄÇ Lemma ÂºïÁêÜ: A true statement used in proving other true statements (that is, a less important theorem that is helpful in the proof of other results). Â∏ÆÂä©ËØÅÊòé Theorem ÁöÑÂ∞èÁªìÊûú„ÄÇÊúâÊó∂ÂÄôÂèØ‰ª•Â∞Ü Theorem ÊãÜÂàÜÊàêÂ§ö‰∏™Â∞èÁöÑ Lemma Êù•ÈÄêÊ≠•ËØÅÊòéÔºå‰ª•‰ΩøÂæóËØÅÊòéÁöÑÊÄùË∑ØÊõ¥Âä†Ê∏ÖÊô∞„ÄÇÂæàÂ∞ëÊÉÖÂÜµ‰∏ã Lemma ‰ºö‰ª•ÂÖ∂Ëá™Ë∫´ÁöÑÂΩ¢ÂºèÂ≠òÂú®„ÄÇ Lemmas are considered to be less important than propositions. But the distinction between categories is rather blurred. There is no formal distinction among a lemma, a proposition, and a theorem. Corollary Êé®ËÆ∫: A true statment that is a simple deduction from a Theorem or Proposition. Proof: The explanation of why a statement is true. Conjecture ÁåúÊÉ≥ÔºåÁåúÊµã: A statement believed to be true, but for which we have no proof. (a statement that is being proposed to be a true statement). Axiom ÂÖ¨ÁêÜ: A basic assumption about a mathematical situation. (a statement we assume to be true). ‰∏çÈúÄË¶ÅËØÅÊòéÁöÑËÆ∫Ëø∞ÔºåÊòØÂÖ∂‰ªñÊâÄÊúâ Theorem ÁöÑÂü∫Á°Ä„ÄÇ Usage Theorems and proofs provide environments that are commonly used within articles and books in mathematics. To write a theorem, you can use the syntax below: ```{theorem, label, name=&quot;Theorem name&quot;} Here is my first theorem. ``` will be rendered as: Theorem 3.1 (Theorem name) Here is my first theorem. Refer to the theorem using \\@ref(prefix:label). See the column Label Prefix in Table for the value of prefix for each environment. E.g., see theorem 3.1 (\\@ref(thm:label)). If you want to refer to a theorem, you should label it. The label can be provided as an ID to the block of the form #label. Another example ```{theorem, thm-py, name=&quot;Pythagorean theorem&quot;} For a right triangle, if $c$ denotes the length of the hypotenuse and $a$ and $b$ denote the lengths of the other two sides, we have \\begin{align*} c^2 = a^2+b^2 \\end{align*} ``` will be rendered as: Theorem 3.2 (Pythagorean theorem) For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[\\begin{align*} c^2 = a^2+b^2 \\end{align*}\\] Alternatively, you can use the syntax based on Pandoc‚Äôs fenced Div blocks. It can already be used in any R Markdown document to write custom blocks. ::: {.theorem #pyth name=&quot;Pythagorean theorem&quot;} For a right triangle, if $c$ denotes the length of the hypotenuse and $a$ and $b$ denote the lengths of the other two sides, we have $$a^2 + b^2 = c^2$$ ::: Theorem 3.3 (Pythagorean theorem) For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Apply Theorem 3.3, ‚Ä¶ Variants of the theorem environments include: lemma, corollary, proposition, conjecture, definition, example, and exercise. The syntax for these environments is similar to the theorem environment, e.g., ```{lemma}. The proof environment behaves similarly to theorem environments but is unnumbered. Variants of the proof environments include remark and solution. The proofenvironment behaves similarly to theorem environments but is unnumbered. Customize math environment labels You need to create a file _bookdown.yml in the same directory as your .Rmd. In the configuration file _bookdown.yml For example, if you want FIGURE x.x instead of Figure x.x, you can change fig to \"FIGURE \": language: label: fig: &quot;FIGURE &quot; If you want to number proof, choose one of the predefined theorem like environments that you are not using otherwise, e.g.¬†example or exercise. Redefine the printed name for that environment in _bookdown.yml (c.f. https://bookdown.org/yihui/bookdown/internationalization.html) via: language: label: exr: &#39;Proof &#39; Here I changed the exercise environment leading word to ‚ÄúProof‚Äù. In your Rmd files use {exercise, mylabel} environment. ```{exercise, mylabel} my comment ``` In Remark \\@ref(exr:mylabel) we discussed... Note that you have to use exercise and the corresponding label prefix exr. Can specify environment style in style.css .exercise { margin: 10px 5px 20px 5px; } /* define a boxed environment */ .boxed { border: 1px solid #535353; padding-bottom: 20px; } &lt;div class = &quot;boxed&quot;&gt; ```{exercise, proof2} Show $\\pi=\\Phi \\left(\\frac{\\mu}{\\sigma}\\right)$. $$ \\begin{aligned}[b] P(r_t&gt;0) &amp;= P(\\mu+e_t&gt;0) \\\\ &amp;= P(e_t&gt;-\\mu) \\quad\\quad\\quad (\\sigma&gt;0, \\text{dividing by a pos. number, inequality unchanged}) \\\\ &amp;= P\\left( \\frac{e_t}{\\sigma} &gt; -\\frac{\\mu}{\\sigma}\\right) \\quad\\;\\; e_t\\sim N(0, \\sigma^2), \\text{ then } \\frac{e_t}{\\sigma}\\sim N(0,1) \\\\ &amp;= P \\left( \\frac{e_t}{\\sigma} &lt; \\frac{\\mu}{\\sigma} \\right) \\\\ &amp;= \\Phi \\left(\\frac{\\mu}{\\sigma} \\right) \\end{aligned} \\square $$ ``` &lt;/div&gt; "],["3.10-figures-1.html", "3.10 Figures", " 3.10 Figures The idea is to generate the figure, output to local, then reload using the following code. ```{r car-plot, eval=TRUE, fig.asp = 0.62, echo=FALSE, out.width=&quot;80%&quot;, fig.cap=&quot;Caption here.&quot; } knitr::include_graphics(img1_path) ``` Use code chunk label to cross reference, e.g., Fig. \\@ref(fig:car-plot). Note that you must specify fig.cap to enable labeling and cross references. Otherwise, the cross reference will show Fig. ??. knitr::include_graphics supports web url for html output, but NOT for latex output. You can let the code output to document directly, i.e., not generating a file and reload. But in this case, scale the figure will change the plot text too. The text might be scaled unexpectedly too small/large. Just be careful with it. Output directly to document library(quantmod) aapl &lt;- getSymbols(&quot;AAPL&quot;, src = &#39;yahoo&#39;, from = &quot;2014-08-01&quot;, to = &quot;2024-09-17&quot;, auto.assign = FALSE ) ```{r out.width=&quot;50%&quot;, fig.asp = 0.62, fig.cap=&quot;`out.width=\\&quot;50%\\&quot;`, fig.asp set to 0.62.&quot;} # plot text is scaled too plot(aapl$AAPL.Close) ``` &lt;img src=‚Äú0206-Rmd-Figure_files/figure-html/unnamed-chunk-2-1.png‚Äù alt=‚Äúout.width=\"50%\", fig.asp set to 0.62. Note that text font scales too, hard to read.‚Äù width=‚Äú50%‚Äù /&gt; Figure 3.1: out.width=\"50%\", fig.asp set to 0.62. Note that text font scales too, hard to read. ```{r fig.width=6, fig.asp=0.6} # Text font does NOT scale, but figure title got cropped plot(aapl$AAPL.Close) ``` Figure 3.2: Set fig.width. Note that text font does NOT scale with figure, BUT the figure title got cropped. ```{r out.width=&quot;100%&quot;, fig.asp = 0.6, fig.cap=&quot;`out.width=\\&quot;100%\\&quot;`.&quot;} plot(aapl$AAPL.Close) ``` &lt;img src=‚Äú0206-Rmd-Figure_files/figure-html/unnamed-chunk-4-1.png‚Äù alt=‚Äúout.width=\"100%\", fig.asp set to 0.6. Note that the plot text got zoomed too, can be too large.‚Äù width=‚Äú100%‚Äù /&gt; Figure 3.3: out.width=\"100%\", fig.asp set to 0.6. Note that the plot text got zoomed too, can be too large. Save and reload This approach preserves your preference better, maintains the relative size of your figure and the text. No cropping, no fuss. f_name &lt;- &quot;images/aapl.png&quot; png(f_name, width=2594, height=1600, res=300) plot(aapl$AAPL.Close) invisible(dev.off()) ```{r out.width=&quot;50%&quot;, fig.cap=&quot;include_graphics with `out.width=\\&quot;50%\\&quot;`.&quot; } knitr::include_graphics(f_name) ``` &lt;img src=‚Äúimages/aapl.png‚Äù alt=‚Äúinclude_graphics with out.width=\"50%\".‚Äù width=‚Äú50%‚Äù /&gt; Figure 3.4: include_graphics with out.width=\"50%\". ```{r out.width=&quot;100%&quot;, fig.cap=&quot;include_graphics with `out.width=\\&quot;100%\\&quot;`.&quot; } knitr::include_graphics(f_name) ``` &lt;img src=‚Äúimages/aapl.png‚Äù alt=‚Äúinclude_graphics with out.width=\"100%\".‚Äù width=‚Äú100%‚Äù /&gt; Figure 3.5: include_graphics with out.width=\"100%\". Q: How to suppress the following dev.off() messages generated by code chunks in Rmd? ## quartz_off_screen ## 2 A: Enclose dev.off() within invisible(), or dump the result of dev.off() to a garbage variable. invisible(dev.off()) # opt1 whatever &lt;- dev.off() # opt2 Specify code chunk options fig.width and fig.height for R-generated figures only. Default is fig.width = 7 and fig.height = 5 (in inches, though actual width will depend on screen resolution). Remember that these settings will default to rmarkdown values, not knitr values. If don‚Äôt know what size is suitable, can right-click the Plots Viewer and choose ‚ÄúCopy Image Address‚Äù. Scale by /100 (in inches) and fill the values to chunk options. out.width and out.height apply to both existing images and R-generated figures. note that the percentage need to be put in quotes. fig.width do not scale font, it shows the original font size. out.width scales the whole figure. Better to use this one. If you want to fix aspect ratio, use fig.asp=0.6 to set height:width = 6:10. out.width keeps the original aspect ratio of the figure and scale the text in the figure too. But what most people want is to scale the figure but not the text. For instance, you want to scale your figure to 70% width of page, but you want to keep the original size of text so it is readable. A caveat with out.widthis that the axis labels and ticks will be so small and hard to read. Other chunk options related to figures: fig.cap=NULL specify figure captions. Must provide fig.cap if you need to cross reference the figure. See Fig. \\@ref(fig:car-plot) use code chunk label to cross reference. The chunk label (car-plot) provides the identifier for referencing the figure generated by the chunk. Fig.&amp;nbsp;\\@ref(fig:logit-regression) use &amp;nbsp; to insert a non-breaking space. fig.align=\"center\" to set figure alignment. fig.pos=\"H\" fix placement. fig.asp=0.6 aspect ratio height:width=6:10. Suggested practice so that you have correct aspect ratio and automatically scaled text and labels in figures. ‚úÖ Generate the figure and save to local The benefit is that you have full control to adjust the figure as needed, such as font size, and could reuse it later. ```{r echo=FALSE, include=FALSE} p &lt;- ggplot(contingency_table %&gt;% as_tibble() %&gt;% mutate(chd69=factor(chd69, levels=c(&quot;non-developed&quot;, &quot;developed&quot;))), aes(x=smoke, y=n, fill=chd69)) + geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;, color=&quot;black&quot;, linewidth=0.1) + scale_fill_grey(start=0.88, end=0.7) + labs(y=&quot;Frequency&quot;) + theme(axis.title.x = element_blank(), legend.position = &quot;bottom&quot;) f_name &lt;- &quot;images/stacked_bar.png&quot; plot_png(p, f_name, 5.17, 5) ``` Specify chunk options include=FALSE (Do not include code output) to suppress the graphic window information like the following. ## quartz_off_screen ## 2 Add the figure using ```{r scatter-plot, echo=FALSE, fig.cap=&quot;Scatter plot of avearge wage against experience.&quot;, out.width = &quot;80%&quot;} include_graphics(f_name) ``` Cross reference pdf_document: using \\autoref{fig:scatter-plot} from hyperref package or Fig. \\ref{fig:scatter-plot} from base latex. hyperref uses Figure, could be changed to Fig. by putting the following cmd at the begin of the Rmd. \\renewcommand\\figureautorefname{Fig.} bookdown::html_document2: using \\@ref(fig:scatter-plot). Latex symbols in Fig. caption The R code block approach. \\\\Phi works. You need to escape the \\ in \\Phi . If there are quotation marks (\") in the figure caption, need to escape them using \\\"...\\\" to distinguish from the outer quotes of the caption parameter. You can use regular Markdown syntax in Fig captions, such as using **Bold** to make text bold. Better to use R code blocks to include figures. Note that include_graphics(\"https://link-to-Google-drive\") does NOT work for pdf output. Works for html output though. If using html tag &lt;figure&gt;, the numbering will be messed up. There is only automatic numbering with R code figures. Use example: ```{r fig.cap=&quot;The $\\\\Phi$ and $\\\\phi$ ($f_Z(.)$) functions (CDF and pdf of standard normal).&quot;, out.width=&quot;70%&quot;, echo=FALSE} include_graphics(&quot;images/Phi_b.png&quot;) ``` Will generate the following Fig 3.6. Figure 3.6: The \\(\\Phi\\) and \\(\\phi\\) (\\(f_Z(.)\\)) functions (CDF and pdf of standard normal). Alternatively, use the HTML approach, and enclose the caption inside &lt;figcaption&gt;. Benefit: You can type equations as you normally do. Don‚Äôt need to escape backslashes as using the R code blocks in the example above. Drawback: You need to manually add figure numbering. ‚ùóÔ∏èThat means, when you change the order of sections or figures in your webpage, the numbering will be a mess. You need to change all capitals manually. &lt;figure&gt; &lt;img src=&quot;https://drive.google.com/thumbnail?id=1nxfdIKXgZvOqXVSeA3h_hf0yxmsM361l&amp;sz=w1000&quot; alt=&quot;Phi_b&quot; style=&quot;display: block; margin-right: auto; margin-left: auto; zoom:80%;&quot; /&gt; &lt;figcaption&gt;Fig.1 The $\\Phi$ and $\\phi$ ($f_Z(.)$) functions (CDF and pdf of standard normal).&lt;/figcaption&gt; &lt;/figure&gt; Fig.1 The \\(\\Phi\\) and \\(\\phi\\) (\\(f_Z(.)\\)) functions (CDF and pdf of standard normal). Refer to another figure in figure caption Just need to use double backslash \\\\@ref(fig:xxx) in the figure caption. Use example: We first generate the figure to be referenced. ```{r firstplot, out.width=&quot;60%&quot;, fig.cap=&quot;Source Figure to be referred to.&quot;} library(ggplot2) p &lt;- ggplot(mtcars, aes(wt, mpg)) plot_A &lt;- p + geom_point() plot_A ``` &lt;img src=‚Äú0206-Rmd-Figure_files/figure-html/firstplot-1.png‚Äù alt=‚ÄúSource Figure to be referenced. Note that when specifying out.width=\"60%\", the text in the figure is scaled too small.‚Äù width=‚Äú60%‚Äù /&gt; Figure 3.7: Source Figure to be referenced. Note that when specifying out.width=\"60%\", the text in the figure is scaled too small. Now a second plot with a reference to Fig.: 3.7. ```{r secondplot, fig.cap = &quot;This is the same as Fig.: \\\\@ref(fig:firstplot) but now with a red line.&quot; } plot_A + geom_line(alpha = .75,col = &quot;red&quot;) ``` &lt;img src=‚Äú0206-Rmd-Figure_files/figure-html/secondplot-1.png‚Äù alt=‚ÄúThis is the same as Fig.: 3.7 but now with a red line and out.width=\"100%\".‚Äù width=‚Äú672‚Äù /&gt; Figure 3.8: This is the same as Fig.: 3.7 but now with a red line and out.width=\"100%\". "],["3.11-tables-1.html", "3.11 Tables", " 3.11 Tables Cross reference tables Using bookdown cmd: \\@ref(tab:chunk-label). Note that you must provide caption option in knitr::kable(). Otherwise the table won‚Äôt be numbered. And see Table \\@ref(tab:mtcars). ```{r mtcars, echo=FALSE} knitr::kable(mtcars[1:5, 1:5], caption = &quot;The mtcars data.&quot;) ``` Refer to the Table 3.1. Table 3.1: The mtcars data. mpg cyl disp hp drat Mazda RX4 21.0 6 160 110 3.90 Mazda RX4 Wag 21.0 6 160 110 3.90 Datsun 710 22.8 4 108 93 3.85 Hornet 4 Drive 21.4 6 258 110 3.08 Hornet Sportabout 18.7 8 360 175 3.15 knitr::kable(x, format=\"pipe\") is useful when you want to copy-and-paste R output from console to other document, e.g., markdown. knitr::kable(mtcars[1:5, 1:5], format = &quot;pipe&quot;) | | mpg| cyl| disp| hp| drat| |:-----------------|----:|---:|----:|---:|----:| |Mazda RX4 | 21.0| 6| 160| 110| 3.90| |Mazda RX4 Wag | 21.0| 6| 160| 110| 3.90| |Datsun 710 | 22.8| 4| 108| 93| 3.85| |Hornet 4 Drive | 21.4| 6| 258| 110| 3.08| |Hornet Sportabout | 18.7| 8| 360| 175| 3.15| 3.11.1 knitr::kable knitr::kable(x, digits, caption=NULL, escape=TRUE) Create tables in LaTeX, HTML, Markdown and reStructuredText. caption The table caption. In order to number the table, mut specify the caption argument. format Possible values are latex, html, pipe (Pandoc‚Äôs pipe tables), simple (Pandoc‚Äôs simple tables), rst, and jira. The value of this argument will be automatically determined if the function is called within a knitr document. digits Maximum number of digits for numeric columns, passed to round(). col.names Rename columns. escape=TRUE Whether to escape special characters when producing HTML or LaTeX tables. Default is TRUE, special characters will either be escaped or substituted. For example, $ is escaped as \\$, _ is escaped as \\_, and \\ is substituted with \\textbackslash{} When set to FALSE, you have to make sure yourself that special characters will not trigger syntax errors in LaTeX or HTML. Common special LaTeX characters include #, %, &amp;, {, and }. Common special HTML characters include &amp;, &lt;, &gt;, and \". You need to be cautious when generating tables with escape = FALSE, and make sure you are using the special characters in the right way. It is a very common mistake to use escape = FALSE and include % or _ in column names or the caption of a LaTeX table without realizing that they are special. align Column alignment: a character vector consisting of 'l' (left), 'c' (center) and/or 'r' (right). By default or if align = NULL, numeric columns are right-aligned, and other columns are left-aligned. If only one character is provided, that will apply to all columns. If a vector is provided, will map to each individual column specifically. Missing values (NA) in the table are displayed as NA by default. If you want to display them with other characters, you can set the option knitr.kable.NA, e.g.¬†options(knitr.kable.NA = '') in the YAML to hide NA values. booktabs = TRUE use the booktabs package linesep = \"\" remove the extra space after every five rows in kable output (with booktabs option) # For Markdown tables, use `pipe` format &gt; knitr::kable(head(mtcars[, 1:4]), format = &quot;pipe&quot;) | | mpg| cyl| disp| hp| |:-----------------|----:|---:|----:|---:| |Mazda RX4 | 21.0| 6| 160| 110| |Mazda RX4 Wag | 21.0| 6| 160| 110| |Datsun 710 | 22.8| 4| 108| 93| |Hornet 4 Drive | 21.4| 6| 258| 110| |Hornet Sportabout | 18.7| 8| 360| 175| |Valiant | 18.1| 6| 225| 105| # For Plain tables in txt, `simple` is useful &gt; knitr::kable(head(mtcars[, 1:4]), format = &quot;simple&quot;) mpg cyl disp hp ------------------ ----- ---- ----- ---- Mazda RX4 21.0 6 160 110 Mazda RX4 Wag 21.0 6 160 110 Datsun 710 22.8 4 108 93 Hornet 4 Drive 21.4 6 258 110 Hornet Sportabout 18.7 8 360 175 Valiant 18.1 6 225 105 3.11.2 Data frame printing To show the tibble information (number of row/columns, and group information) along with paged output, we can write a custom function by modifying the print.paged_df function (which is used internally by rmarkdown for the df_print feature) and use CSS to nicely format the output. https://stackoverflow.com/a/76014674/10108921 Paged df https://bookdown.org/yihui/rmarkdown/html-document.html#tab:paged https://github.com/rstudio/rmarkdown/issues/1403 --- title: &quot;Use caption with df_print set to page&quot; date: &quot;2025-08-26&quot; output: bookdown::html_document2: df_print: paged --- When the df_print option is set to paged, tables are printed as HTML tables with support for pagination over rows and columns. The possible values of the df_print option for the html_document format. Option Description default Call the print.data.frame generic method; console output prefixed by ##; kable Use the knitr::kable function; looks nice but with no navigation for rows and columns, neither column types. tibble Use the tibble::print.tbl_df function, this provides groups and counts of rows and columns info as if printing a tibble. paged Use rmarkdown::paged_table to create a pageable table; paged looks best but slows down compilation significantly; A custom function Use the function to create the table The possible values of the df_print option for the pdf_document format: default, kable, tibble, paged, or a custom function. paged print ```{r echo=TRUE, paged.print=TRUE} ggplot2::diamonds ``` default output ```{r echo=TRUE, paged.print=FALSE} ggplot2::diamonds ``` kable output ```{r echo=TRUE} knitr::kable(ggplot2::diamonds[1:10, ]) ``` Note that kable output doesn‚Äôt provide tibble information. Available options for paged tables: Option Description max.print The number of rows to print. rows.print The number of rows to display. cols.print The number of columns to display. cols.min.print The minimum number of columns to display. pages.print The number of pages to display under page navigation. paged.print When set to FALSE turns off paged tables. rownames.print When set to FALSE turns off row names. These options are specified in each chunk like below: ```{r cols.print=3, rows.print=3} mtcars ``` For pdf_document, it is possible to write LaTex code directly. ```{=latex} \\begin{tabular}{ll} A &amp; B \\\\ A &amp; B \\\\ \\end{tabular} ``` Do not forget the equal sign before latex, i.e., it is =latex instead of latex. 3.11.3 Stargazer stargazer print nice tables in Rmd documents and R scripts: Passing a data frame to stargazer package creates a summary statistic table. Passing a regression object creates a nice regression table. Support tables output in multiple formats: text, latex, and html. In R scripts, use type = \"text\" for a quick view of results. stargaer does NOT work with anova table, use pander::pander instead. Text table Specify stargazer(type = \"text\") ```{r descrptive-analysis-text, comment = &#39;&#39;} apply(data[,-1], 2, get_stat) %&gt;% stargazer(type = &quot;text&quot;, digits=2) ``` The text output looks like the following. =============================================== Dependent variable: --------------------------- delta_infl ----------------------------------------------- unemp -0.091 (0.126) Constant 0.518 (0.743) ----------------------------------------------- Observations 203 R2 0.003 Adjusted R2 -0.002 Residual Std. Error 2.833 (df = 201) F Statistic 0.517 (df = 1; 201) =============================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 By default, stargazer uses ***, **, and * to denote statistical significance at the one, five, and ten percent levels (* p&lt;0.1; ** p&lt;0.05; *** p&lt;0.01). In contrast, summary.lm uses * p&lt;0.05, ** p&lt;0.01, *** p&lt; 0.001. You can change the cutoffs for significance using star.cutoffs = c(0.05, 0.01, 0.001). There is one empty line after each coefficient, to remove the empty lines, specify no.space = TRUE. The regression table with all empty lines removed: =============================================== Dependent variable: --------------------------- delta_infl ----------------------------------------------- unemp -0.091 (0.126) Constant 0.518 (0.743) ----------------------------------------------- Observations 203 R2 0.003 Adjusted R2 -0.002 Residual Std. Error 2.833 (df = 201) F Statistic 0.517 (df = 1; 201) =============================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 HTML table Note that you need to specify results=\"asis\" in the chunk options. This option tells knitr to treat verbatim code blocks ‚Äúas is.‚Äù Otherwise, instead of your table, you will see the raw html or latex code. Note that *‚Äôs do not show properly in html output, see Fig. 3.9, need to specify in the footnote (notes) manually. Figure 3.9: Failed to show significance codes. Use the following code to display the correct significance symbols. See Fig. 3.10 for the expected output. ```{r descrptive-analysis-html, results=&quot;asis&quot;} apply(data[,-1], 2, get_stat) %&gt;% stargazer(type = &quot;html&quot;, digits=2, notes = &quot;&lt;span&gt;&amp;#42;&lt;/span&gt;: p&lt;0.1; &lt;span&gt;&amp;#42;&amp;#42;&lt;/span&gt;: &lt;strong&gt;p&lt;0.05&lt;/strong&gt;; &lt;span&gt;&amp;#42;&amp;#42;&amp;#42;&lt;/span&gt;: p&lt;0.01 &lt;br&gt; Standard errors in parentheses.&quot;, notes.append = F) ``` Figure 3.10: Correct significance codes. Common arguments: type specify output table format. Possible values: latex (default for latex code), html, and text. Need to specify to html in html outputs. digits an integer that indicates how many decimal places should be used. A value of NULL indicates that no rounding should be done at all, and that all available decimal places should be reported. Defaults to 3 digits. notes a character vector containing notes to be included below the table. notes.append = FALSE a logical value that indicates whether notes should be appended to the existing standard note(s) associated with the table‚Äôs style (typically an explanation of significance cutoffs). Defaults to TRUE. If the argument‚Äôs value is set to FALSE, the character strings provided in notes will replace any existing/default notes. notes.align \"l\" for left alignment, \"r\" for right alignment, and \"c\" for centering. This argument is not case-sensitive. single.row = TRUE to put coefficients and standard errors on same line no.space = TRUE to remove the spaces after each line of coefficients font.size = \"small\" to make font size smaller column.labels a character vector of labels for columns in regression tables. This is useful to denote different regressions, informing the name/nature of the model, instead of using numers to identify them. column.separate a numeric vector that specifies how column.labels should be laid out across regression table columns. A value of c(2, 1, 3), for instance, will apply the first label to the two first columns, the second label to the third column, and the third label will apply to the following three columns (i.e., columns number four, five and six). dep.var.labels labels for dependent variables covariate.labels labels for covariates in the regression tables. Can provide latex symbols in the labels, need to escape special symbols though. stargazer(mod_sel_lm_mtcars, covariate.labels = c(&quot;(Intercept)&quot;, &quot;drat&quot;, &quot;hp&quot;, &quot;$w_{i}$&quot;, &quot;\\\\textit{k}&quot;, &quot;logLik&quot;, &quot;AICc&quot;, &quot;\\\\Delta AICc&quot;)) add.lines add a row(s), such as reporting fixed effects. stargazer(output, output2, type = &quot;html&quot;, add.lines = list( c(&quot;Fixed effects?&quot;, &quot;No&quot;, &quot;No&quot;), c(&quot;Results believable?&quot;, &quot;Maybe&quot;, &quot;Try again later&quot;) ) ) Add a blank line under the stargazer table: &amp;nbsp; with a blank line above and below. Cross reference stargazer tables. In pdf output, use Table \\@ref(tab:reg-table) or Table \\ref{tab:reg-table}. Table \\@ref(tab:reg-table) summarize the regression results in a table. ```{r, include=TRUE, results=&#39;asis&#39;} stargazer(capm_ml, FF_ml, type=&#39;latex&#39;, header=FALSE, digits=4, no.space = TRUE, title=&quot;Regression Results for META&quot;, label = &quot;tab:reg-table&quot;) ``` header=FALSE is to suppress the % Table created by stargazer header. This applies to only latex tables. label=\"tab:reg-table\" is to specify the cross reference label for the table. table.placement = \"H\" set float to H to fix positions. Places the float at precisely the location in the code. This requires the float LaTeX package. Remember to load it in the YAML. Defaults to \"!htbp\". The htbp controls where the table or figure is placed. Tables and figures do not need to go where you put them in the text. LATEX moves them around to prevent large areas of white space from appearing in your paper. h (Here): Place the float here, i.e., approximately at the same point it occurs in the source text (however, not exactly at the spot) t (Top): Place the table at the top of the current page b (Bottom): Place the table at the bottom of the current page. p (Page): Place the table at the top of the next page. !: Override internal parameters LaTeX uses for determining ‚Äúgood‚Äù float positions. align = FALSE a logical value indicating whether numeric values in the same column should be aligned at the decimal mark in LaTeX output. In html output, cross references to stargazer tables are not so straightforward. label option in stargazer does not work. Cannot use chunk labels either. ```{r fit-age, echo=FALSE, results=&#39;asis&#39;, fig.cap=&quot;Logistic regression of CHD on age.&quot;} # Use title caption from fig.cap tit &lt;- knitr::opts_current$get(&quot;fig.cap&quot;) # Adding caption for html output tit_html &lt;- paste0(&#39;&lt;span id=&quot;tab:&#39;, knitr::opts_current$get(&quot;label&quot;), &#39;&quot;&gt;(#tab:&#39;, knitr::opts_current$get(&quot;label&quot;), &#39;)&lt;/span&gt;&#39;, tit) stargazer::stargazer(fit.age, label = paste0(&quot;tab:&quot;, knitr::opts_current$get(&quot;label&quot;)), title = ifelse(knitr::is_latex_output(), tit, tit_html), type = ifelse(knitr::is_latex_output(),&quot;latex&quot;,&quot;html&quot;), notes = &quot;&lt;span&gt;&amp;#42;&lt;/span&gt;: p&lt;0.1; &lt;span&gt;&amp;#42;&amp;#42;&lt;/span&gt;: &lt;strong&gt;p&lt;0.05&lt;/strong&gt;; &lt;span&gt;&amp;#42;&amp;#42;&amp;#42;&lt;/span&gt;: p&lt;0.01 &lt;br&gt; Standard errors in parentheses.&quot;, notes.append = F, header = F ) ``` Here is another reference to stargazer Table \\@ref(tab:fit-age). Don‚Äôt change things unless it is absolutely necessary. Run the code chunk before compiling the whole website. It gets slowly as the website gets larger. stargazer::stargazer() the :: is necessary, and header=F is necessary and should be place at the end, otherwise will have errors as follows. Error in `.stargazer.wrap()`: ! argument is missing, with no default Backtrace: 1. stargazer::stargazer(...) 2. stargazer:::.stargazer.wrap(...) Execution halted Exited with status 1. Another example if you don‚Äôt need to add footnotes. ```{r mytable, results=&#39;asis&#39;, fig.cap=&quot;This is my table.&quot;} # Use title caption from fig.cap tit &lt;- knitr::opts_current$get(&quot;fig.cap&quot;) # Adding caption for html output tit_html &lt;- paste0(&#39;&lt;span id=&quot;tab:&#39;, knitr::opts_current$get(&quot;label&quot;), &#39;&quot;&gt;(#tab:&#39;, knitr::opts_current$get(&quot;label&quot;), &#39;)&lt;/span&gt;&#39;, tit) stargazer::stargazer(fit.age, label = paste0(&quot;tab:&quot;, knitr::opts_current$get(&quot;label&quot;)), title = ifelse(knitr::is_latex_output(), tit, tit_html), type = ifelse(knitr::is_latex_output(),&quot;latex&quot;,&quot;html&quot;), header = F ) ``` Here is a reference to stargazer Table \\@ref(tab:mytable). Alignment of Stargazer Tables In PDF, the tables will be in the center by default. However, when working with HTML output, you need to add CSS styling to adjust the table. References: https://libguides.princeton.edu/c.php?g=1326286&amp;p=9763596#s-lg-box-wrapper-36305037 3.11.4 xtable print(xtable(tableResults, caption = NULL, digits = NULL), include.rownames=FALSE) Convert an R object to an xtable object, which can then be printed as a LaTeX or HTML table. align Character vector of length equal to the number of columns of the resulting table, indicating the alignment of the corresponding columns. Also, \"|\" may be used to produce vertical lines between columns in LaTeX tables, but these are effectively ignored when considering the required length of the supplied vector. If a character vector of length one is supplied, it is split as strsplit(align, \"\")[[1]] before processing. Since the row names are printed in the first column, the length of align is one greater than ncol(x) if x is a data.frame. Use \"l\", \"r\", and \"c\" to denote left, right, and center alignment, respectively. Use \"p{3cm}\" etc. for a LaTeX column of the specified width. For HTML output the \"p\" alignment is interpreted as \"l\", ignoring the width request. Default depends on the class of x. caption Character vector of length 1 or 2 containing the table‚Äôs caption or title. If length is 2, the second item is the ‚Äúshort caption‚Äù used when LaTeX generates a ‚ÄúList of Tables‚Äù. digits Numeric vector of length equal to one (in which case it will be replicated as necessary) or to the number of columns of the resulting table or matrix of the same size as the resulting table, indicating the number of digits to display in the corresponding columns. 3.11.5 kableExtra The kableExtra package is designed to extend the basic functionality of tables produced using knitr::kable(). kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) bootstrap_options A character vector for bootstrap table options. Please see package vignette or visit the w3schools‚Äô Bootstrap Page for more information. Possible options include basic, striped, bordered, hover, condensed, responsive and none. striped alternating row colors hover Use the :hover selector on tr (table row) to highlight table rows on mouse over. full_width A TRUE or FALSE variable controlling whether the HTML table should have 100% the preferable format for full_width. If not specified, TRUE for a HTML table , will have full width by default but this option will be set to FALSE for a LaTeX table. latex_options A character vector for LaTeX table options, i.e., won‚Äôt have effecs on html tables. Possible options: Arguments Meanings striped Add alternative row colors to the table. It will imports LaTeX package xcolor if enabled. scale_down useful for super wide table. It will automatically adjust the table to fit the page width. repeat_header only meaningful in a long table environment. It will let the header row repeat on every page in that long table. hold_position ‚Äúhold‚Äù the floating table to the exact position. It is useful when the LaTeX table is contained in a table environment after you specified captions in kable(). It will force the table to stay in the position where it was created in the document. HOLD_position A stronger version of hold_position. Requires the float package and specifies ‚Å†[H]‚Å†. Rows and columns can be grouped via the functions pack_rows() and add_header_above(), respectively. scroll_box(width = \"100%\", height = \"500px\") let you create a fixed height table while making it scrollable. This function only works for html long tables. # commonly used settings table %&gt;% knitr::kable(digits = 5) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;), full_width = FALSE, latex_options=&quot;scale_down&quot;) %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;500px&quot;) # escape=TRUE, this makes your life easier, will output the table exactly as it is result &lt;- read_csv(&quot;~/Documents/GDP/data/reg_result/IFE_result.csv&quot;) result %&gt;% knitr::kable(digits = 5, escape=T) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;), full_width = FALSE, latex_options=&quot;scale_down&quot;) # escape=FALSE, have to specify escape by replace `*` to `\\\\\\\\*` result &lt;- read_csv(&quot;~/Documents/GDP/data/reg_result/IFE_result.csv&quot;) result &lt;- result %&gt;% mutate(pval.symbol = gsub(&quot;[*]&quot;, &quot;\\\\\\\\*&quot;, pval.symbol) ) result %&gt;% knitr::kable(digits = 5, escape=FALSE) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;), full_width = FALSE, latex_options=&quot;scale_down&quot;) tables in pdf output reg_data %&gt;% select(Date, adjusted, eRi, rmrf) %&gt;% head(10) %&gt;% knitr::kable(digits = c(0,2,4,4), escape=T, format = &quot;latex&quot;, booktabs = TRUE, linesep = &quot;&quot; ) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;), full_width = FALSE, stripe_color = &quot;gray!15&quot;) knitr::kable() arguments format = \"latex\" specifies the output format. align = \"l\" specifies column alignment. booktabs = TRUE is generally recommended for formatting LaTeX tables. linesep = \"\" prevents default behavior of extra space every five rows. kableExtra::kable_styling() arguments position = \"left\" places table on left hand side of page. latex_options = c(\"striped\", \"repeat_header\") implements table striping with repeated headers for tables that span multiple pages. stripe_color = \"gray!15\" species the stripe color using LaTeX color specification from the xcolor package - this specifies a mix of 15% gray and 85% white. linebreak(x, align = \"l\", double_escape = F, linebreaker = \"\\n\") Make linebreak in LaTeX Table cells. align=\"l\" Choose from ‚Äúl‚Äù, ‚Äúc‚Äù or ‚Äúr‚Äù. Defaults to ‚Äúl‚Äù. Customize the looks for columns/rows kableExtra::column_spec(kable_input) this function allows users to select a column and then specify its look. row_spec() works similar with column_spec() but defines specifications for rows. For the position of the target row, you don‚Äôt need to count in header rows or the group labeling rows. row_spec(row = 0, align='c') specify format of the header row. Here I want to center align headers. Add header rows to group columns add_header_above(). The header variable is supposed to be a named character with the names as new column names and values as column span. For your convenience, if column span equals to 1, you can ignore the =1 part so the function below can be written as add_header_above(c(\"\", \"Group 1\" = 2, \"Group 2\" = 2, \"Group 3\" = 2)). kbl(dt) %&gt;% kable_classic() %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Group 1&quot; = 2, &quot;Group 2&quot; = 2, &quot;Group 3&quot; = 2)) You can add another row of header on top. Group rows collapse_rows will put repeating cells in columns into multi-row cells. The vertical alignment of the cell is controlled by valign with default as ‚Äútop‚Äù. Not working for html output. collapse_rows_dt &lt;- data.frame(C1 = c(rep(&quot;a&quot;, 10), rep(&quot;b&quot;, 5)), C2 = c(rep(&quot;c&quot;, 7), rep(&quot;d&quot;, 3), rep(&quot;c&quot;, 2), rep(&quot;d&quot;, 3)), C3 = 1:15, C4 = sample(c(0,1), 15, replace = TRUE)) kbl(collapse_rows_dt, align = &quot;c&quot;) %&gt;% kable_paper(full_width = F) %&gt;% column_spec(1, bold = T) %&gt;% collapse_rows(columns = 1:2, valign = &quot;top&quot;) Empty string as column name in tibble: use setNames or attr df &lt;- tibble(&quot; &quot;=1) setNames(df, &quot;&quot;) # # A tibble: 1 x 1 # `` # &lt;dbl&gt; # 1 1 attr(df, &quot;names&quot;) &lt;- c(&quot;&quot;) footnote() add footnotes to tables. There are four notation systems in footnote, namely general (no prefix for footnotes), number, alphabet and symbol. Math in rmd tables knitr::kable(x, escape=TRUE) escape=TRUE whether to escape special characters when producing HTML or LaTeX tables. Defaults to TRUE. When escape = FALSE, you have to make sure that special characters will not trigger syntax errors in LaTeX or HTML. You need to escape \\ passed into R code. ```{r, echo=FALSE} library(knitr) mathy.df &lt;- data.frame(site = c(&quot;A&quot;, &quot;B&quot;), b0 = c(3, 4), BA = c(1, 2)) colnames(mathy.df) &lt;- c(&quot;Site&quot;, &quot;$\\\\beta_0$&quot;, &quot;$\\\\beta_A$&quot;) kable(mathy.df, escape=FALSE) ``` It is possible to edit Latex table directly in Rmd. Don‚Äôt enclose in $$. Use \\begin{table} and start your table data. "],["3.12-rmd-github-pages.html", "3.12 Rmd GitHub Pages", " 3.12 Rmd GitHub Pages The rmarkdown package had provided a simple site generator that did not rely on a third-party site generator like Hugo. If you feel Hugo is too complex for you, and you only want to build a few Rmd documents into a website, this built-in site generator may be a good choice. Stage-commit-push many files Use the Terminal git add . to ‚Äústage‚Äù all the files that I want to commit as that‚Äôs quicker than clicking on all the files often that I want to commit. Go to RStudio Commit Pending changes icon (the white docs icon with a tick in a Git pane) to write the commit as I find git commit -m \"Write your message here\" a bit too long! Use the Push and Pull buttons in RStudio as that‚Äôs easier than typing git push or git pull in the terminal. Project structure Note that the minimum requirement for any R Markdown website is that it has an index.Rmd file and a _site.yml file. _site.yml configures the behavior of site generation. It provides the global YAML header for the site. index.Rmd provides the content for the home page of your website. Figure 3.11: Minimal example of a R Markdown website. Render your website If you run rmarkdown::render_site() (which is the function triggered by the ‚ÄúKnit‚Äù button) from within the directory containing the website, the following will occur: All of the *.Rmd and *.md files in the root website directory will be rendered into HTML. Note that Markdown files beginning with _ are not rendered (this is a convention to designate files that are to be included by top level Rmd documents as child documents). If you want to exclude certain files/folders from being rendered, put an underscore (_) before the file or folder name (e.g., _private.Rmd or _private_folder/). index.Rmd controls the content on your main page. The generated HTML files and any supporting files (e.g., CSS and JavaScript) are copied into an output directory (_site by default, on Github Pages the output folder is docs). Use output_dir (in _site.yml) to specify which directory to copy site content into. The HTML files within the output directory are now ready to deploy as a standalone static website. # render the entire site rmarkdown::render_site() # render a single file only rmarkdown::render_site(&quot;about.Rmd&quot;) _site.yml config _site.yml is a site configuration file. It provides the global YAML header for the site. It contains various common elements you want to apply to all pages (e.g., output options, CSS styles, header and footer elements, etc.). _site.yml is equivalent to the combination of _output.yml and _bookdown.yml in bookdown. A _site.yml example: name: &quot;my-website&quot; output_dir: &quot;docs&quot; include: [&quot;import.R&quot;] exclude: [&quot;docs.txt&quot;, &quot;*.csv&quot;] output: html_document: theme: cosmo highlight: textmate include: after_body: footer.html css: styles.css name provides a suggested URL path for your website when it is published (by default this is just the name of the directory containing the site). output_dir field indicates which directory to copy site content into. \"_site\" is the default if none is specified. It can be \".\" to keep all content within the root website directory alongside the source code. The include and exclude fields enable you to override the default behavior vis-√†-vis what files are copied into the output directory. By default, all files within the website directory are copied into the output directory (e.g.¬†_site) except for the following: Files beginning with \".\" (hidden files). Files beginning with \"_\" Files known to contain R source code (e.g.¬†\".R\", \".s\", \".Rmd\"), R data (e.g.¬†\".RData\", \".rds\"), or configuration data (e.g.¬†\"rsconnect\" ,\"packrat\", \"renv\")). The include and exclude fields of _site.yml can be used to override this default behavior (wildcards can be used to specify groups of files to be included or excluded). Note that the include and exclude fields target only top-level files and directories (i.e.¬†a directory is either included or not, you can‚Äôt exclude a subset of files within a directory). Note also that include and exclude are not used to determine which Rmd files are rendered (all of them in the root directory save for those named with the _ prefix will be rendered). output defines shared output options for all R Markdown documents within a site. Note that individual documents can also include their own output options, which will be merged with the common options at render time. include: ¬†¬†¬†after-body: footer.html An example of footer.thml: &lt;p&gt;Copyright &amp;copy; 2016 Skynet, Inc. All rights reserved.&lt;/p&gt; style.css is a CSS stylesheet. blockquote { font-style: italic } index.Rmd index.Rmd provides the content for the home page of your website. A parsimonious example of index.Rmd which only includes the website title. This is useful if your home page only includes simple text. --- title: &quot;Lab Scripts for Course Example Demonstration&quot; --- A full example of index.Rmd. You may use this when you want to write complex content in your home page. --- title: &quot;R Notes&quot; author: &quot;Menghan Yuan&quot; date: &quot;2025-08-26&quot; site: rmarkdown::default_site documentclass: book bibliography: [book.bib, packages.bib] biblio-style: apalike link-citations: yes description: &quot;This is a minimal example of using the rmarkdown to write a book.&quot; --- rmarkdown::default_site() is the default site generation function. It is also possible to define a custom site generator that has alternate behaviors, e.g., bookdown::bookdown_site. Note that index.Rmd in the rmarkdown build-in site generator is different than that of bookdown. In bookdown sites, you can specify global YAML that will apply to all pages in the website, but in the rmarkdown built-in site generator, each page has its own YAML. rmakrdown bookdown Site generator function rmarkdown::default_site() bookdown::bookdown_site index.Rmd home page First Chapter and YAML that will apply to all pages Rmd files More independent; contains YAML of their own; Start with first-level heading (e.g., # Introduction); do NOT have any YAML; References: https://bookdown.org/yihui/rmarkdown/rmarkdown-site.html#site-generator-function R scripts If you have R code that you would like to share across multiple R Markdown documents within your site, you can create an R script (e.g., utils.R) and source it within your Rmd files. For example: ```{r} source(&quot;utils.R&quot;) ``` Shared Rmd snippets You may have common fragments of R Markdown that you want to share across pages within your site. To share Rmd fragments, you should name them with a leading underscore (_), and then include them within their parent Rmd document using the child chunk option. For example: about.Rmd: --- title: &quot;About This Website&quot; --- More about this website. ```{r, child=&quot;_chunk-opt.Rmd&quot;, include=FALSE} # Load settings in &quot;_chunk-opt.Rmd&quot; # &quot;include=F&quot; disables printing the code in &quot;_chunk-opt.Rmd&quot; # into the current file ``` _chunk-opt.Rmd: &lt;!-- Chunk option settings --&gt; ```{r chunk-opt, include=FALSE} # set default chunk options opts_chunk$set(echo = TRUE, message=FALSE, fig.align=&quot;center&quot;, fig.pos = &quot;H&quot;) opts &lt;- options(knitr.kable.NA = &quot;&quot;) ``` &lt;!-- Other shared Rmd snippets --&gt; ... The leading underscore (_) is an indicator to the site generation engine that the Rmd is a partial document to be included in other documents, so it is not compiled as a standalone document during site rendering. Workflow Workflow: Edit your site, build it, then push and commit to GitHub to publish your changes online. To render all of the pages in the website, you use the Build pane, which calls rmarkdown::render_site() to build and then preview the entire site. Figure 3.1: Build an entire website in RStudio. As you work on the individual pages of your website, you can render them just as you do with conventional standalone R Markdown documents. This is useful when you want to preview a specific page without rendering the whole site, which can be time-consuming. Options to render individual pages: Using the Knit button just as you do with conventional standalone R Markdown documents. Figure 3.2: Knit a single page of a website. Note that the Knit button will call rmarkdown::render_site(input = \"0100-RStudio.Rmd\", output_format = \"all\"). It will generate the html output in the docs/ directory as specified in _site.yml. Knitting an individual page will only render and preview that page, not the other pages in the website. Or using the command line rmarkdown::render(\"0100-RStudio.Rmd\"). It will generate the html output RStudio.html in the current working directory where RStudio.html is located. You can see it in the Output pane &gt; Files tab. Click the file and choose View in Web Browser. Note that if using rmarkdown::render_site(\"0100-RStudio.Rmd\"), it will render \"0100-RStudio.Rmd\" and generate the html output in the docs/ directory as specified in _site.yml. # render the entire site rmarkdown::render_site() # render a single file only rmarkdown::render_site(&quot;about.Rmd&quot;) To clean up all of the files generated via render_site(), you can call the clean_site() function, which will remove all files generated by rendering your site‚Äôs Markdown documents, including knitr‚Äôs *_cache directories. You can specify the preview = TRUE option to just list the files to be removed rather than actually removing them: # list which files will be removed rmarkdown::clean_site(preview = TRUE) # actually remove the files rmarkdown::clean_site() References: https://bookdown.org/yihui/rmarkdown/rmarkdown-site.html#rstudio Customize the Knit button It is possible to control the behavior of the Knit button by providing the knit field within the YAML frontmatter of your document. The field takes a function with the main argument input (the path to the input Rmd document) and other arguments that are currently ignored. You can either write the source code of the function directly in the knit field, or put the function elsewhere (e.g., in an R package) and call the function in the knit field. --- knit: | (function(input, ...) { rmarkdown::render( input, output_file = paste0( xfun::sans_ext(input), &#39;-&#39;, Sys.Date(), &#39;.html&#39; ), envir = globalenv() ) }) --- Further readings: https://forum.posit.co/t/changing-default-behavior-of-knit-button/133874 If you want to have the pdf output, you add pdf_document to your document‚Äôs YAML after html_document. This way, your Rmd will supports multiple output format. When you click the Knit button of run rmarkdown::render(\"0100-RStudio.Rmd\"), it will use the first output format. You need to specify the output format you want in the second argument, call rmarkdown::render(\"0100-RStudio.Rmd\", 'pdf_document') More options can be passed by: rmarkdown::render(&quot;0207-Rmd-Table.Rmd&quot;, bookdown::pdf_document2( latex_engine = &quot;xelatex&quot;, keep_tex = TRUE, includes = includes( in_header = &quot;latex/preamble.tex&quot;, before_body= &quot;latex/before_body.tex&quot; ) ) ) # `output_options` is not compatible with output format function object Note: Each time you run rmarkdown::render_site(), the docs/ folder will be overwritten with updated HTML versions of your .Rmds. This means DON‚ÄôT EVER EDIT FILES IN THE docs/ FOLDER! Nothing catastrophic will happen if you do, but you will overwrite and lose all your changes the next time you knit or render_site(). Don‚Äôt forget to update index.Rmd (home page) and _site.yml (cross references files include: [\"w1.rmd\", \"w2.rmd\"]) This will copy files into docs so that you can put a downloadable link to them. CSS Style --- output: html_document: theme: cosmo # css: style.css # link to external CSS --- &lt;style type = &quot;text/css&quot;&gt; h2 { color: red; /* internal CSS */ } &lt;/style&gt; ## R Markdown When you want to change the style of certain element but don‚Äôt know where to start, open the html in Chrome, and go to View &gt; Developer &gt; Inspect Element to identify the corresponding elements. Refer to your posts using relative links If you have a Markdown file in your repository at docs/project1.html, and you want to link from that file to docs/another-page.md, you can do so with the following markup: [a relative link](project1.html) When you view the source file on GitHub.com, the relative link will continue to work, but now, when you publish that file using GitHub Pages, the link will be silently translated to docs/another-page.html to match the target page‚Äôs published URL. Link to another file [download](w1.rmd) &lt;a href=&quot;w1.rmd&quot;&gt;Download File&lt;/a&gt; TOC on home page: source code: https://github.com/lmullen/rmd-notebook/blob/master/index.Rmd webpage: https://lmullen.github.io/rmd-notebook/ # replacing with the following options # {r TOC, echo=FALSE, results=&#39;asis&#39;} rmd &lt;- Sys.glob(&quot;*.[Rr]md&quot;) rmd &lt;- rmd[!rmd %in% c(&quot;index.Rmd&quot;, &quot;about.Rmd&quot;)] html &lt;- sub(&quot;.Rmd&quot;, &quot;.html&quot;, rmd) lines &lt;- lapply(rmd, readLines) yaml &lt;- lapply(lines, rmarkdown:::parse_yaml_front_matter) cat(&quot;&lt;ul&gt;&quot;) for (i in seq_along(rmd)) { cat(paste0(&quot;&lt;li&gt;&lt;a href=&#39;&quot;, html[i], &quot;&#39;&gt;&quot;, yaml[[i]]$title, &quot;&lt;/a&gt;&lt;br/&gt;&quot;, &quot;&lt;code&gt;&quot;, rmd[i], &quot;&lt;/code&gt;&quot;, &quot;&lt;/li&gt;&quot;)) } cat(&quot;&lt;/ul&gt;&quot;) Project website: rmarkdown‚Äôs site generator, R Markdown: The Definitive Guide, https://bookdown.org/yihui/rmarkdown/rmarkdown-site.html Structure: https://www.storybench.org/convert-google-doc-rmarkdown-publish-github-pages/ Multi-page website: https://phuston.github.io/patrickandfrantonarethebestninjas/howto Blogdown: https://github.com/liuyanguu/Blogdown?tab=readme-ov-file Distill: https://rstudio.github.io/distill/website.html Bookdown Notes for One Course: https://github.com/bcallaway11/econ_4750_notes https://bcallaway11.github.io/econ_4750_notes/law-of-iterated-expectations.html# "],["4-bookdown.html", "Chapter 4 bookdown", " Chapter 4 bookdown Bookdown is an extra package for R Markdown that is particularly useful for long documents. In HTML format, produces a full website of interlinked pages, one page per chapter Other HTML features: contents bar, search, colour schemes, font size adjustment, etc Adds LaTeX-like theorem/definition/proof environments ‚ÄúPlain‚Äù R Markdown R Markdown with Bookdown Good for short documents, single HTML page Good for long documents, multi-page website PDF or accessible HTML PDF or accessible HTML LaTeX equations LaTeX equations No theorem environments Theorem environments We can reference chunks (tables and figures), sections, and equations in bookdown output formats bookdown extends Pandoc Examples of bookdown formats are bookdown::pdf_document2 or bookdown::html_document2. Refer to Figure \\@ref(fig:chunk-name) Table \\@ref(tab:chunk-name) Section \\@ref(my-section) Examples of chunks: ```{r chunk-name} plot(cars) ``` See Figure \\@ref{fig:chunk-name}. # Section {#my-section} Refer to Section \\@ref{my-section} Create a bookdown project: File ‚Üí New Project ‚Üí New Directory ‚Üí Book project using bookdown ‚Üí Create Project Bookdown cookbook: https://rstudio4edu.github.io/rstudio4edu-book/book-dress.html Deployment and Hosting bookdown on GitHub Pages Ref: Authoring Books with R Markdown, https://bookdown.org/yihui/bookdown/github.html Initialize your local git repository and link to the remote GitHub repo. See instructions HERE. Go to your _bookdown.yml file and add output_dir: \"docs\" on a line by itself Serve/preview your book locally Now the website output files should be in /docs. Create a .nojekyll in /docs that tells GitHub that your website is not to be built via Jekyll. touch .nojekyll Push your changes to GitHub remote Configure publishing source for GH pages as main branch /docs folder Go to your GH remote repo, click Settings ‚Üí click Pages in the left column ‚Üí under GitHub Pages, change the ‚ÄúSource‚Äù to be ‚Äúmain branch /docs folder‚Äù. "],["4.1-bookdown-project-structure.html", "4.1 bookdown project structure", " 4.1 bookdown project structure Below shows the basic structure of a default bookdown project: directory/ ‚îú‚îÄ‚îÄ index.Rmd ‚îú‚îÄ‚îÄ 01-intro.Rmd ‚îú‚îÄ‚îÄ 02-literature.Rmd ‚îú‚îÄ‚îÄ 03-method.Rmd ‚îú‚îÄ‚îÄ 04-application.Rmd ‚îú‚îÄ‚îÄ 05-summary.Rmd ‚îú‚îÄ‚îÄ 06-references.Rmd ‚îú‚îÄ‚îÄ _bookdown.yml ‚îú‚îÄ‚îÄ _output.yml ‚îú‚îÄ‚îÄ book.bib ‚îú‚îÄ‚îÄ preamble.tex ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ style.css As a summary of these files: index.Rmd: This is the only Rmd document to contain a YAML frontmatter, and is the first book chapter. Rmd files: A typical bookdown book contains multiple chapters, and each chapter lives in one separate Rmd file. _bookdown.yml: A configuration file for bookdown; includes book-specific metadata such as output directory, whether to create a new R session for each chapter. _output.yml: It specifies the output formatting of the HTML, LaTeX/PDF, e-books, etc. preamble.tex and style.css: They can be used to adjust the appearance and styles of the book output document(s). Knowledge of LaTeX and/or CSS is required. These files are explained in greater detail in the following subsections. _output.yml _output.yml Output formats can be specified either in the YAML metadata of the first Rmd file of the book (usually index.Rmd), or in a separate YAML file named _output.yml under the root directory of the book. See Section 12.4 in R Markdown: The Definitive Guide for a complete list of bookdown output formats. A quick takeaway is that bookdown supports both book types and single documents. Common uses of _output.yml: Specify supported output formats: bookdown::gitbook, bookdown::pdf_book, bookdown::epub_book, etc. Add an edit link, e.g., https://github.com/my1396/R-Notes/edit/main/%s This will configure which remote repo to link to and hence allow the page to be downloadable as an .Rmd. Also need to specify download: [\"rmd\"]. Link to your GitHub in the toolbar (also need index.Rmd) Add other sharing links Header and footer of your TOC Collapse the TOC by (sub)section Code highlighting Here we use highlight: tango to apply the Tango theme. The issue is that Tango highlighting does not work well with dark mode. Here is a brief example of _output.yml: bookdown::gitbook: css: style.css highlight: tango split_by: section+number includes: in_header: head.html config: fontsettings: theme: sky toc: collapse: section+number before: | &lt;li&gt;&lt;a href=&quot;./&quot;&gt;R Notes&lt;/a&gt;&lt;/li&gt; after: | &lt;li&gt;&lt;a href=&quot;https://github.com/rstudio/bookdown&quot; target=&quot;blank&quot;&gt;Published with bookdown&lt;/a&gt;&lt;/li&gt; toc_depth: 3 edit: link: https://github.com/my1396/R-Notes/edit/main/%s sharing: github: yes download: [&quot;pdf&quot;, &quot;epub&quot;, &quot;rmd&quot;] enableEmoji: true bookdown::pdf_book: includes: in_header: preamble.tex latex_engine: xelatex citation_package: natbib keep_tex: yes bookdown::epub_book: default You do NOT need the three dashes --- in _output.yml. In this case, all formats should be at the top level, instead of under an output field in individual Rmds. The output format bookdown::gitbook is built upon rmarkdown::html_document, which was explained in Section 3.1 in R Markdown: The Definitive Guide. The main difference between rendering in R Markdown and bookdown is that a book will generate multiple HTML pages by default. bookdown::gitbook settings: css: style.css specifies one or more custom CSS files to tweak the default CSS style. Note that bookdown::gitbook has a set of default CSS files. Use docs/libs/gitbook-2.6.7/css to see the default settings. If some of your custom CSS settings are not applied, it is likely due to conflicts with gitbook CSS. You may need to override the default CSS files by appending !important to your custom CSS settings. There are several sub-options in the config option for you to tweak some details in the user interface. See here for the default settings of config. Font/theme settings fontsettings: # changing the default theme: night family: serif size: 3 You can set the initial value of these settings via the fontsettings option. Font size is measured on a scale of 0-4; the initial value can be set to 1, 2 (default), 3, or 4. theme: white | sepia | night. Default to white. download: [\"pdf\", \"epub\", \"rmd\"] specifies which formats to allow users to download the book in. When download: null (by default), gitbook() will look for PDF, EPUB, and MOBI files in the book output directory, and automatically add them to the download option. If you just want to suppress the download button, use download: false. split_by= c(\"chapter\", \"chapter+number\", \"section\", \"section+number\", \"rmd\", \"none\") defaults to chapter, which splits the file by the first-level headers. chapter splits the file by the first-level headers; A chapter page may be too long if there are many sections within chapters. section splits the file by the second-level headers. chapter+number and section+number: the chapter/section numbers will be prepended to the HTML filenames. For example: if using chapter or section, the HTML file names will be introduction.html, literature.html, etc.; but with the numbering setting, the HTML file names will be 1-introduction.html, 2-literature.html, etc. I prefer section+number as it orders all html in the book‚Äôs section order. ‚úÖ The includes option allows you to insert arbitrary custom content before and/or after the body of the output. It has three sub-options: in_header, before_body, and after_body. You need to know the basic structure of an HTML or LaTeX document to understand these options. The source of an HTML document looks like this: &lt;html&gt; &lt;head&gt; &lt;!-- head content here, e.g. CSS and JS --&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- body content here --&gt; &lt;/body&gt; &lt;/html&gt; The in_header option takes a file path and inserts it into the &lt;head&gt; tag. The before_body file will be inserted right below the opening &lt;body&gt; tag, and after_body is inserted before the closing tag &lt;/body&gt;. Use example of includes option in HTML output. For example, when you have LaTeX math expressions rendered via the MathJax library in the HTML output, and want the equation numbers to be displayed on the left (default is on the right), you can create a text file (named mathjax-number.html) that contains the following code: &lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ TeX: { TagSide: &quot;left&quot; } }); &lt;/script&gt; Let‚Äôs assume the file mathjax-number.html is in the root directory of your book (the directory that contains all your Rmd files). You can insert this file into the HTML head via the in_header option, e.g., --- output: bookdown::gitbook: includes: in_header: mathjax-number.html --- Example of MathJax config files: 0007bookdown/mathjax_header.html by matthew-towers @matthew-towers uses a very clever approach, probably the easiest setup ‚Äì define the command directly in LaTeX and enclode in &lt;span class=\"math inline\"&gt;$...$&lt;/span&gt; to indicate this is inline math. Using inline config options Stack Overflow MathJax Bookdown uses MathJax 2.7 by default. Define TeX macros You can include macro definitions in the Macros section of the TeX blocks of your configuration: MathJax.Hub.Config({ TeX: { Macros: { RR: &quot;{\\\\bf R}&quot;, bold: [&quot;{\\\\bf #1}&quot;,1] } } }); Note that MathJax.Hub.Config is used in inline configuration. It is case-sensitive. Macros has capital M, and TeX, rather than tex, is used. A LaTeX source document has a similar structure: \\documentclass{book} % LaTeX preamble % insert in_header here \\begin{document} % insert before_body here % body content here % insert after_body here \\end{document} You can add a table of contents using the toc option and specify the depth of headers that it applies to using the toc_depth option. If the TOC depth defaults to 3 in html_document. For pdf_document, if the TOC depth is not explicitly specified, it defaults to 2 (meaning that all level 1 and 2 headers will be included in the TOC). --- bookdown::gitbook: toc: collapse: subsection toc_depth: 3 --- collapse specifies a level to expand to by default, aka at #, ##, or ###. I suggest collapsing at level 2. This way, you get a good overview of what each major topic (level 1 heading) includes, without showing the most detailed items. collapse: subsection: At startup, the toc will collapse at the level 2 headings. As you go to one specific subsection, the content inside will expand. You can see level 3 headings. ‚úÖ collapse: section: At startup, the toc will collapse at the level 1 headings, which keeps the appearance concise. However, a side effect is that level 3 headings will never be displaied when navigating to a specific level 2 heading. bookdown ‰∏≠Êñá‰π¶Á±ç _output.yml ËåÉ‰æã: https://github.com/yihui/bookdown-chinese/blob/96d526572f0c6648d06c2d4bebf57c5fb4eafce3/_output.yml You can set up a tex template. Yihui sets up the Chinese support in the template file (latex/template.tex). bookdown::pdf_book: includes: in_header: latex/preamble.tex before_body: latex/before_body.tex after_body: latex/after_body.tex keep_tex: yes dev: &quot;cairo_pdf&quot; latex_engine: xelatex citation_package: natbib template: latex/template.tex The base format for bookdown::pdf_book is rmarkdown::pdf_document. dev: Graphics device to use for figure output, defaults to pdf. _bookdown.yml _bookdown.yml allows you to specify optional settings to build the book. For example: Set output_dir: docs Set new_session: yes When set to yes, it uses ‚ÄúKnit and Merge‚Äù (K‚ÄìM), and creates a new R session for each chapter, which is useful for avoiding conflicts between packages or variables across chapters. The default is no, which uses ‚ÄúMerge and Knit‚Äù (M-K), where all chapters are knitted in one R session. Change themes Change the chapter name Change chapter order delete_merged_file: true output_dir: &quot;docs&quot; new_session: yes language: ui: chapter_name: &quot;Chapter &quot; Note that you don‚Äôt need to manually create the docs folder, bookdown will create one if it doesn‚Äôt exists. delete_merged_file: whether to delete the main Rmd file after the book is successfully rendered. An Rmd file that is merged from all chapters; by default, it is named _main.Rmd. before_chapter_script: one or multiple R scripts to be executed before each chapter. After you serve your site locally, all supporting files will be output to docs. Be sure to add one .nojekyll file in docs to tell GitHub that your website is not to be built via Jekyll. Because bookdown only overwrites existing files and does not delete unused ones, you can simply delete the docs folder so that bookdown will recreate everything necessary without any redundancy. Remember to recreate .nojekyll too after bookdown has created the new docs. index.Rmd index.Rmd homepage of your website. Contains the first chapter and the YAML metadata which will be applied to all other Rmd pages. That is, index.Rmd sets the global YAML for the entire website. Moreover, index.Rmd is the only Rmd document that contains a YAML frontmatter. See Chapter 2.2 in R Markdown: The Definitive Guide for YAML details. Common uses of index.Rmd‚Äôs YAML frontmatter: Book cover, title, author, date, and description Add bibliography Once you have one or multiple .bib files, you may use the field bibliography in the YAML metadata of your first R Markdown document (which is typically index.Rmd), and you can also specify the bibliography style via biblio-style (this only applies to PDF output). bibliography: [book.bib, packages.bib] biblio-style: &quot;apalike&quot; Add your user citation to book.bib in stead. Don‚Äôt remove packages.bib; it is automatically generated and overwritten every time you rebuild the book. Link to your GitHub in the toolbar (also need _output.yml) Add a favicon Add the following line to index.Rmd YAML: favicon: &quot;images/r-project-favicon.ico&quot; Issue: Favicon shows ok on Chrome but couldn‚Äôt display on Safari. Same issue reported in Stack Overflow. Fix: There is a delay for Safari to show Favicon. Wait for two hours and the issue resolves itself‚Ä¶ An example of index.Rmd --- title: &quot;A Minimal bookdown Project&quot; site: bookdown::bookdown_site documentclass: book bibliography: [book.bib, packages.bib] csl: chicago-fullnote-bibliography.csl github-repo: my1396/R-Notes favicon: &quot;images/r-project-favicon.ico&quot; # typesetting for LaTeX output papersize: a4 # The printed size of the thesis geometry: - top=25.4mm - bottom=25.4mm - left=25.4mm - right=38.1mm # - bindingoffset=6.4mm # removes a specified space from the inner-side for twoside. # - asymmetric # disable alternating margins on odd/even pages classoption: - twoside - openright --- # Preface Some content site: bookdown::bookdown_site tells rmarkdown to use bookdown to build all Rmd files, instead of rendering a single Rmd file. .Rmd files Chapters (also sections) are based on separate Rmd files. Besides index.Rmd, other R Markdown files will make up the chapters of your book. By default, bookdown merges all Rmd files by the order of filenames, e.g., 01-intro.Rmd will appear before 02-literature.Rmd. The Rmd files must start immediately with the chapter title using the first-level heading, e.g., # Chapter Title. Note that YAML metadata should NOT be included in these Rmd files, as it is inherited from the index.Rmd file. 01-intro.Rmd # Introduction This chapter is an overview of the methods that we propose to solve an **important problem**. 02-literature.Rmd # Literature Here is a review of existing methods. "],["4.2-rendering-bookdown.html", "4.2 Rendering bookdown", " 4.2 Rendering bookdown Two approaches: ‚ÄúMerge and Knit‚Äù (M-K): default; runs all code chunks in all chapters; the state of the R session from previous chapters is carried over to later chapters (e.g., objects created in previous chapters are available to later chapters, unless you deliberately deleted them) ‚ÄúKnit and Merge‚Äù (K-M): separate R sessions for individual chapters; all chapters are isolated from each other. Other differences: Because knitr does not allow duplicate chunk labels in a source document, you need to make sure there are no duplicate labels in your book chapters when you use the M-K approach, otherwise knitr will signal an error when knitting the merged Rmd file. Note that this means there must not be duplicate labels throughout the whole book. The K-M approach only requires no duplicate labels within any single Rmd file. K-M does not allow Rmd files to be in subdirectories, but M-K does. To switch to K-M, you either use the argument new_session = TRUE when calling render_book(), or set new_session: yes in the configuration file _bookdown.yml. 4.2.1 Rendering bookdown website Assuming you have a website, use the following command to render your site: # render gitbook format only rmarkdown::render_site(output_format = &quot;bookdown::gitbook&quot;) Every time you make changes to individual Rmd files or to CSS style files, you can knit the single page using rmarkdown::render(\"0100-RStudio.Rmd\") or the Knit button in the source editor. The change will be reflected to your website. This is faster than Build the website. There are two major ways to build multiple Rmd documents: blogdown (Xie, Hill, and Thomas 2017; Xie, Dervieux, and Presmanes Hill 2023) for building websites, and bookdown (Xie 2016, 2023a)for authoring books. Creating Websites with R Markdown: https://bookdown.org/yihui/blogdown/global-options.html Q: What‚Äôs the difference between Bookdown and Blogdown? A: Bookdown is for books, grouped in chapters; Blogdown is for blogs, ordered by dates. 4.2.2 Rendering bookdown book Assume you have a local bookdown project, you can use the following to edit, build, preview, and serve the book locally. Build the book To build all Rmd files into a book, you can call the function bookdown::render_book(). It uses the settings specified in the _output.yml (if it exists). If multiple output formats are specified in it, all formats will be built. If you are using RStudio, this can be done through the Build tab. Open the drop down menu Build Book if you only want to build one format. Preview a chapter Building the whole book can be slow when the size of the book is big or your book contains large amounts of computation. We can use the preview_chapter() function in bookdown to only build a single chapter at a time. Equivalently, you can click the Knit button in RStudio. Serve the book Instead of running render_book() or preview_chapter() each time you want to view the changes, you can use the function bookdown::serve_book() to start a live preview of the book. Any time a Rmd file is saved, the book will be recompiled automatically, and the preview will be updated to reflect the changes. Control long outputs by using hooks Put the following functions to the set up code chunk. Then you could use the option max.lines = 10 whenever you want to set a limit to the maximum output length to print. ## control long outputs by using eg `max.lines = 10` hook_output_default &lt;- knitr::knit_hooks$get(&#39;output&#39;) truncate_to_lines &lt;- function(x, n) { if (!is.null(n)) { x = unlist(stringr::str_split(x, &#39;\\n&#39;)) if (length(x) &gt; n) { # truncate the output x = c(head(x, n), &#39;...\\n&#39;) } x = paste(x, collapse = &#39;\\n&#39;) # paste first n lines together } x } knitr::knit_hooks$set(output = function(x, options) { max.lines &lt;- options$max.lines x &lt;- truncate_to_lines(x, max.lines) hook_output_default(x, options) }) Issue: In RStudio dark mode, kableExtra tables are invisible in the code block output preview because both the font and background are white, making the content unreadable. Fix: Force the font color to be black. First run this edited version of kableExtra:::print.kableExtra(): print.kableExtra &lt;- function (x, ...) { view_html &lt;- getOption(&quot;kableExtra_view_html&quot;, TRUE) if (view_html &amp; interactive()) { dep &lt;- list( rmarkdown::html_dependency_jquery(), rmarkdown::html_dependency_bootstrap(theme = &quot;cosmo&quot;), kableExtra::html_dependency_kePrint(), kableExtra::html_dependency_lightable() ) x &lt;- sub(&#39;style=&quot;&#39;, &#39;style=&quot;color: black; &#39;, as.character(x), fixed = TRUE) html_kable &lt;- htmltools::browsable( htmltools::HTML( as.character(x), &quot;&lt;script type=\\&quot;text/x-mathjax-config\\&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath: [[\\&quot;$\\&quot;,\\&quot;$\\&quot;]]}})&lt;/script&gt;&lt;script async src=\\&quot;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\\&quot;&gt;&lt;/script&gt;&quot; ) ) htmltools::htmlDependencies(html_kable) &lt;- dep class(html_kable) &lt;- &quot;shiny.tag.list&quot; print(html_kable) } else { cat(as.character(x)) } } The changes consisted of adding the x &lt;- sub('style=\"', 'style=\"color: black; ', as.character(x), fixed = TRUE) line and also adding full references to some of the functions. Then you can print the table as before, the table font color will be forced to be black, and hence visible. library(tidyverse) head(iris) %&gt;% knitr::kable(caption = &quot;**Table 1.** Iris data. &quot;, digits = 2) %&gt;% kableExtra::kable_styling() "],["4.3-toggle-visibility-of-solutions.html", "4.3 Toggle Visibility of Solutions", " 4.3 Toggle Visibility of Solutions When the bookdown file loads, you would like all the solutions to be hidden. You would like a button for each solution to toggle its visibility. Easy solution: this works but cannot show math equations properly. ‚ùå Advantage is that it does not need to define any java function. &lt;button class=&quot;button&quot; onclick=&quot;$(&#39;#target2&#39;).toggle();&quot;&gt; Show/Hide &lt;/button&gt; &lt;div id=&quot;target2&quot; style=&quot;display: none&quot;&gt; Solution: $P(\\textrm{A wins or B wins}) = P\\big(\\{\\textrm{A wins}\\} \\cup \\{\\textrm{B wins}\\}\\big)$ &lt;/div&gt; Example: Show/Hide Solution: $P(\\textrm{A wins or B wins}) = P\\big(\\{\\textrm{A wins}\\} \\cup \\{\\textrm{B wins}\\}\\big)$ Ultimate solution! Able to show math equations properly ‚úÖ Put the following codes in the Rmd header. This defines the button action myFunction. &lt;script&gt; function myFunction(id) { var x = document.getElementById(id); if (x.style.display === &quot;none&quot;) { x.style.display = &quot;block&quot;; } else { x.style.display = &quot;none&quot;; } } &lt;/script&gt; In case of bookdown, put the JavaScript in script.hhml, which will be loaded into the header of all your html files via _output.yml. bookdown::gitbook: css: assets/styling/style.css includes: in_header: assets/styling/head.html after_body: assets/styling/scripts.html # ... When you want to create a solution division, use the following codes. Change the function argument myDIV, which is the id of the element. id must be unique in one file. Change the text shown on the button (Solution1) if you need. Put your solution inside the &lt;div id=myDIV&gt; tag, where id is what you specified in the function argument. ```{example, ex1} Let $Y=g(X)=\\mu+\\sigma X$ where $\\sigma&gt;0$. Representing the CDF of $Y$ using $F_X(x)$. ``` &lt;button onclick=&quot;myFunction(&#39;myDIV&#39;)&quot;&gt;Solution1&lt;/button&gt; &lt;div id=&quot;myDIV&quot; style=&quot;display: none; color: blue;&quot;&gt; $P(\\textrm{A wins or B wins}) = P\\big(\\{\\textrm{A wins}\\} \\cup \\{\\textrm{B wins}\\}\\big)$ solution1 &lt;/div&gt; ```{example, ex2} Let $X\\sim N(0,1)$ and $Y=\\mu+\\sigma X$. Calculate $\\mathbb{E}(Y)$. ``` &lt;button onclick=&quot;myFunction(&#39;myDIV2&#39;)&quot;&gt;Solution2&lt;/button&gt; &lt;div id=&quot;myDIV2&quot; style=&quot;display: none; color: blue;&quot;&gt; $P(\\textrm{A wins or B wins}) = P\\big(\\{\\textrm{A wins}\\} \\cup \\{\\textrm{B wins}\\}\\big)$ solution2 &lt;/div&gt; blabla ... blabla ... Example 4.1 Let \\(Y=g(X)=\\mu+\\sigma X\\) where \\(\\sigma&gt;0\\). Representing the CDF of \\(Y\\) using \\(F_X(x)\\). Solution Note that \\(g(x)\\) is strictly increasing in \\(x\\). The inverse function is \\[ X = g^{-1}(Y) = \\frac{Y-\\mu}{\\sigma} \\] and so \\[ F_Y(y) = F_X\\left(g^{-1}(y)\\right) = F_X\\left(\\frac{y-\\mu}{\\sigma}\\right) \\] References: https://stackoverflow.com/questions/62549757/toggle-show-hide-element-where-default-on-refresh-is-hide https://naras.su.domains/post/toggle-visibility-of-solutions-in-bookdown/ "],["4.4-quarto.html", "4.4 Quarto", " 4.4 Quarto Quarto Guide: https://quarto.org/docs/guide/ Quarto Tutorial: https://jmjung.quarto.pub/m02-advanced-literate-programming/#learning-outcomes Host Quarto on GitHub Pages. To get started, change your project configuration _quarto.yml to use docs as the output-dir. project: type: book output-dir: docs Then, add a .nojekyll file to the root of your repository that tells GitHub Pages not to do additional processing of your published site using Jekyll (the GitHub default site generation tool): touch .nojekyll Note that .nojekyll‚Äôs location is different than that of bookdown, which is at /docs folder. Strengths of Quarto: hoverable citations and cross-references, easy to read easy subplots Weakness of Quarto: slow compared to Bookdown issues when you want to compile one single page within a package. Changes are not realized in time unless render the whole website. Workaround: Need to exclude from project index, and need file header yaml to import mathjax settings and themes. Bookdown is reliable. Don‚Äôt need yaml in single Rmd, website theme will apply automatically. 4.4.1 Book Structure book: chapters: - index.qmd - preface.qmd - part: dice.qmd chapters: - basics.qmd - packages.qmd - part: cards.qmd chapters: - objects.qmd - notation.qmd - modifying.qmd - environments.qmd - references.qmd appendices: - tools.qmd - resources.qmd The index.qmd file is required (because Quarto books also produce a website in HTML format). This page should include the preface, acknowledgements, etc. The remainder of chapters includes one or more book chapters. You can divide your book into parts using part within the book chapters. Note that the markdown files dice.qmd and cards.qmd contain the part title (as a level one heading) as well as some introductory content for the part. If you just need a part title then you can alternatively use this syntax: book: chapters: - index.qmd - preface.qmd - part: &quot;Dice&quot; chapters: - basics.qmd - packages.qmd The references.qmd file will include the generated bibliography (see References below for details). Syntax differences with R Markdown: Code chunks Both R markdown and Quarto can use the following ways to specify chunk options: Use tag=value in the chunk header ```{r}. ```{r my-label, fig.cap = caption} # R code ``` Alternatively, you can write chunk options in the body of a code chunk after #|, e.g., ```{r} #| label: fig-my-label #| fig-cap: caption # R code ``` tag: value is the YAML syntax. Logical values in YAML can be any of: true/false, yes/no, and on/off. They all equivalent to TRUE/FALSE (uppercase) in R. Options format: space after #| and colon : TRUE/FALSE need to be in uppercase Note that Quarto accepts Rmd‚Äôs way of specifying chunk options. The difference is that Quarto‚Äôs label for figures must start with fig-, while Rmd accepts any labels. ```{r label = &quot;fig-my-label&quot;, fig.cap = caption} # R code ``` 4.4.2 HTML Theming One simple theme title: &quot;My Document&quot; format: html: theme: cosmo fontsize: 1.1em linestretch: 1.7 Enable dark and light modes format: html: include-in-header: themes/mathjax.html respect-user-color-scheme: true theme: dark: [cosmo, themes/cosmo-dark.scss] light: [cosmo, themes/cosmo-light.scss] respect-user-color-scheme: true honors the user‚Äôs operating system or browser preference for light or dark mode. Otherwise, the order of light and dark elements in the theme or brand will determine the default appearance for your html output. For example, since the dark option appears first in the first example, a reader will see the light appearance by default, if respect-user-color-scheme is not enabled. As of Quarto 1.7, respect-user-color-scheme requires JavaScript support: users with JavaScript disabled will see the author-preferred (first) brand or theme. Custom Themes Your custom.scss file might look something like this: /*-- scss:defaults --*/ $h2-font-size: 1.6rem !default; $headings-font-weight: 500 !default; /*-- scss:rules --*/ h1, h2, h3, h4, h5, h6 { text-shadow: -1px -1px 0 rgba(0, 0, 0, .3); } Note that the variables section is denoted by /*-- scss:defaults --*/: the defaults section (where Sass variables go) Used to define global variables that can be used throughout the theme. /*-- scss:rules --*/: the rules section (where normal CSS rules go) Used to define more fine grained behavior of the theme, such as specific styles for headings, paragraphs, and other elements. Theme Options You can do extensive customization of themes using Sass variables. Bootstrap defines over 1,400 Sass variables that control fonts, colors, padding, borders, and much more. The Sass Variables can be specified within SCSS files. These variables should always be prefixed with a $ and are specified within theme files rather than within YAML options Commonly used Sass variables: Category Variable Description Colors $body-bg The page background color. $body-color The page text color. $link-color The link color. $input-bg The background color for HTML inputs. $popover-bg The background color for popovers (for example, when a citation preview is shown). You can see all of the variables here: https://github.com/twbs/bootstrap/blob/main/scss/_variables.scss Note that when you make changes to your local .scss, the changes will be implemented in-time. That is, you don‚Äôt need to re-build your website to see the effects. Ref: Quarto document: https://quarto.org/docs/output-formats/html-themes.html Check sass variables: https://bootswatch.com 4.4.3 Render Quarto Rendering the whole website is slow. When you are editing a new section/page, you may want to edit as a standalone webpage and when you are finished, you add the qmd file to the _quarto.yml file index. Difference btw a standalone webpage from a component of a qmd project Standalone webpage: include yaml at the header of the file. Fast compile and rendering. ‚úÖ A component of qmd project: added to the file index, no yaml needed, format will automatically apply. Slow, need to render the whole qmd project in order to see your change. In terminal This will provide live preview of the document in your web browser. Newest changes will be reflected while you edit the document. ‚úÖ Render a Quarto document to HTML using the command line: $quarto render 0304-Quarto.Rmd --to html Quarto Preview: display output in the external web browser. $ quarto preview 0304-Quarto.Rmd # all formats $ quarto preview 0304-Quarto.Rmd --to html # specific format Note that quarto render can be used to Rmd files too. You can also render a Quarto project using: $quarto render --to html In VS Code You have to refresh to see your updates when using VS Code command palette quarto preview. You can render a Quarto document in VS Code using the command palette: Quarto: Render Document to render the document. Quarto: Render Project to render the entire project. Quarto: Preview to preview the default document in a web browser. If you want to preview a different format, use the Quarto: Preview Format command: This will show a preview of the project in the internal browser. In R quarto::quarto_render(input = NULL, output_format = \"html\") can be used to render a Quarto document or project in R. If input is not specified, it will render the current Quarto project. If input is specified, it will render the specified Quarto document. If output_format is not specified, it will render the document to HTML. You can specify other formats such as PDF or Word. output_format = \"all\" will render all formats specified in the _quarto.yml file. # Render a Quarto document to HTML quarto::quarto_render(&quot;0304-Quarto.Rmd&quot;, output_format = &quot;html&quot;) # Render a Quarto project to HTML quarto::quarto_render(output_format = &quot;html&quot;) # Render a Quarto document to PDF quarto::quarto_render(&quot;0304-Quarto.Rmd&quot;, output_format = &quot;pdf&quot;) # Render a Quarto project to PDF quarto::quarto_render(output_format = &quot;pdf&quot;) Alternatively, you can use the Render button in RStudio. The Render button will render the first format listed in the document YAML. If no format is specified, then it will render to HTML. 4.4.4 Cross References Add labels: Code cell: add option label: prefix-LABEL Markdown: add attribute #prefix-LABEL Add references: @prefix-LABEL, e.g. You can see in @fig-scatterplot, that... Element ID How to cite Figure #fig-xxx @fig-xxx Table #tbl-xxx @tbl-xxx Equation #eq-xxx @eq-xxx Section #sec-xxx @sec-xxx Equations $$ y_i = \\beta_{i}&#39;x + u_i. $$ {#eq-cross_sectional_hetero} @eq-cross_sectional_hetero gives Equation (1) [-@eq-cross_sectional_hetero] gives only the tag (1) You can customize the appearance of inline references by either changing the syntax of the inline reference or by setting options. Here are the various ways to compose a cross-reference and their resulting output: Type Syntax Output Default @fig-elephant Figure 1 Capitalized @Fig-elephant Figure 1 Custom Prefix [Fig @fig-elephant] Fig 1 No Prefix [-@fig-elephant] 1 Note that the capitalized syntax makes no difference for the default output, but would indeed capitalize the first letter if the default prefix had been changed via an option to use lower case (e.g.¬†‚Äúfig.‚Äù). Change the prefix in inline reference using *-prefix options. You can also specify whether references should be hyper-linked using the ref-hyperlink option. --- title: &quot;My Document&quot; crossref: fig-prefix: figure # (default is &quot;Figure&quot;) tbl-prefix: table # (default is &quot;Table&quot;) ref-hyperlink: false # (default is true) --- 4.4.5 Equations Load MathJax Config Load mathjax.html in YAML --- title: &quot;Model specifications&quot; author: &quot;GDP and climate&quot; date: &quot;2025-05-13&quot; from: markdown+tex_math_single_backslash format: html: toc: true self-contained: true html-math-method: mathjax include-in-header: mathjax.html --- In mathjax.html &lt;script&gt; MathJax = { tex: { tags: &#39;ams&#39;, // should be &#39;ams&#39;, &#39;none&#39;, or &#39;all&#39; macros: { // define TeX macro RR: &quot;{\\\\bf R}&quot;, bold: [&quot;{\\\\bf #1}&quot;, 1] }, }, }; &lt;/script&gt; tags: 'ams' allows equation numbering Math delimiters Issue: Cannot use \\( and \\[ for math delimiters. Fix: Add from: markdown+tex_math_single_backslash to YAML frontmatter. Source --- title: &quot;Quarto Playground&quot; from: markdown+tex_math_single_backslash format: html: html-math-method: mathjax --- Inline math example: \\( E = mc^2 \\) Block math example: \\[ a^2 + b^2 = c^2 \\] form: Format to read from. Extensions can be individually enabled or disabled by appending +EXTENSION or -EXTENSION to the format name (e.g.¬†markdown+emoji). Extension: tex_math_single_backslash Causes anything between \\( and \\) to be interpreted as inline TeX math, and anything between \\[ and \\] to be interpreted as display TeX math. Note: a drawback of this extension is that it precludes escaping ( and [. Refer to Docs of Quarto and Pandoc: https://quarto.org/docs/reference/formats/html.html#rendering https://pandoc.org/MANUAL.html#extension-tex_math_single_backslash https://pandoc.org/MANUAL.html#extension-tex_math_double_backslash Q: How to get rid of the qmd dependence file? A: Use format: html: self-contained: true 4.4.6 Divs and Spans You can add classes, attributes, and other identifiers to regions of content using Divs and Spans. Div example ::: {.border} This content can be styled with a border ::: Once rendered to HTML, Quarto will translate the markdown into: &lt;div class=&quot;border&quot;&gt; &lt;p&gt;This content can be styled with a border&lt;/p&gt; &lt;/div&gt; A bracketed sequence of inlines, as one would use to begin a link, will be treated as a Span with attributes if it is followed immediately by attributes: [This is *some text*]{.class key=&quot;val&quot;} Once rendered to HTML, Quarto will translate the markdown into &lt;span class=&quot;class&quot; data-key=&quot;val&quot;&gt; This is &lt;em&gt;some text&lt;/em&gt; &lt;/span&gt; 4.4.7 Theorems ::: {#thm-line} The equation of any straight line, called a linear equation, can be written as: $$ y = mx + b $$ ::: See @thm-line. In Quarto, #thm-line is a combined command us .theorem #thm-line in bookdown. In bookdown, the label can be anything, does not have to begin with #thm-. But in Quarto, #thm-line is restrictive, it indicates the thm environment and followed by the label of the theorem line. Theorem 4.1 The equation of any straight line, called a linear equation, can be written as: \\[ y = mx + b \\] See Theorem 4.1. To add a name to Theorem, use name=\"...\". ::: {#thm-topo name=&quot;Topology Space&quot;} A topological space $(X, \\Tcal)$ is a set $X$ and a collection $\\Tcal \\subset \\Pcal(X)$ of subsets of $X,$ called open sets, such that ... ::: See Theorem @thm-topo. Theorem 4.2 (Topology Space) A topological space \\((X, \\Tcal)\\) is a set \\(X\\) and a collection \\(\\Tcal \\subset \\Pcal(X)\\) of subsets of \\(X,\\) called open sets, such that ‚Ä¶ See Theorem 4.2. Change the label prefix: --- crossref: cnj-title: &quot;Assumption&quot; cnj-prefix: &quot;Assumption&quot; --- cnj-title: The title prefix used for conjecture captions. cnj-prefix: The prefix used for an inline reference to a conjecture. 4.4.8 Callouts There are five different types of callouts available. note warning important tip caution The color and icon will be different depending upon the type that you select. ::: {.callout-note} Note that there are five types of callouts, including: `note`, `warning`, `important`, `tip`, and `caution`. ::: ::: {.callout-tip} ## Tip with Title This is an example of a callout with a title. ::: ::: {.callout-caution collapse=&quot;true&quot;} ## Expand To Learn About Collapse This is an example of a &#39;folded&#39; caution callout that can be expanded by the user. You can use `collapse=&quot;true&quot;` to collapse it by default or `collapse=&quot;false&quot;` to make a collapsible callout that is expanded by default. ::: Here are what the various types look like in HTML output: Callout heading can be defined using title = \"Heading\" in the callout header, or ## Heading in the callout body It can be any level of heading. icon = false to disable the icon in the callout. To cross-reference a callout, add an ID attribute that starts with the appropriate callout prefix, e.g., #nte-xxx. You can then reference the callout using the usual @nte-xxx syntax. appearance = \"default\" | \"simple\" | \"minimal\" default: to use the default appearance with a background color and border. simple: to remove the background color, but keep the border and icon. minimal: A minimal treatment that applies borders to the callout, but doesn‚Äôt include a header background color or icon. appearance=\"minimal\" is equivalent to appearance = \"simple\" icon = false in the callout header. References: https://www.njtierney.com/post/2022/04/11/rmd-to-qmd/ Cheatsheet: https://rstudio.github.io/cheatsheets/html/quarto.html Citations: https://quarto.org/docs/authoring/citations.html Theorems: https://quarto.org/docs/authoring/cross-references.html#theorems-and-proofs "],["5-basic-r.html", "Chapter 5 Basic R", " Chapter 5 Basic R Get help CRAN: https://cran.r-project.org Rdocumentation powered by datacamp, it has interactive interface with good examples, the typesetting also looks better, https://www.rdocumentation.org R is case-sensitive; comments start with #. Curly brackets or braces {} are used to keep code that needs to be run together as a single expression. This is commonly done when writing a function or when writing an if statement. Double curly braces {{}} are used programming with tidyverse. See the dplyrprogramming vignette for details. ?Syntax to check precedence of operators. Save &amp; Load R objects save(..., f_name) and saveRDS() save() When loaded the named object is restored to the current environment (in general use this is the global environment ‚Äî the workspace) with the same name it had when saved. save writes an external representation of R objects to the specified file. The objects can be read back from the file at a later date by using the function load or attach (or data in some cases). save(..., list = character(), file = stop(\"'file' must be specified\"), ascii = FALSE, version = NULL, envir = parent.frame(), compress = isTRUE(!ascii), compression_level, eval.promises = TRUE, precheck = TRUE) ... The names of the objects to be saved (as symbols or character strings). list A character vector containing the names of objects to be saved. The names of the objects specified either as symbols (or character strings) in ... or as a character vector in list are used to look up the objects from environment envir. file the name of the file where the data will be saved. saveRDS() doesn‚Äôt save the both the object and its name it just saves a representation of the object. As a result, the saved object can be loaded into a named object within R that is different from the name it had when originally serialized. Serialization is the process of converting a data structure or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and ‚Äúresurrected‚Äù later in the same or another computer environment. saveRDS works only for saving a single R object, save() can save multiple R objects in one file. A workaround for saveRDS is to save all target objects in a single R object (e.g., in a list), and then use saveRDS() to save it at once. datalist = list(mtcars = mtcars, pressure=pressure) saveRDS(datalist, &quot;twodatasets.RDS&quot;) rm(list = ls()) datalist = readRDS(&quot;twodatasets.RDS&quot;) datalist rm(list = ls()) removes all objects in the current environment. It will not unload the packages that you have loaded. If you want to both remove all objects and unload all packages, you can restart your R session. Load R objects load(f_name) to load .rda file. readRDS(f_name) to load .rds file. Naming conventions: rda and rds for selected objects .RData for all objectes in your workspace The file extensions are up to you; you can use whatever file extensions you want. An example &gt; require(mgcv) Loading required package: mgcv This is mgcv 1.7-13. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. &gt; mod &lt;- gam(Ozone ~ s(Wind), data = airquality, method = &quot;REML&quot;) &gt; mod Family: gaussian Link function: identity Formula: Ozone ~ s(Wind) Estimated degrees of freedom: 3.529 total = 4.529002 REML score: 529.4881 &gt; save(mod, file = &quot;mymodel.rda&quot;) &gt; ls() [1] &quot;mod&quot; &gt; load(file = &quot;mymodel.rda&quot;) &gt; ls() [1] &quot;mod&quot; &gt; ls() [1] &quot;mod&quot; &gt; saveRDS(mod, &quot;mymodel.rds&quot;) &gt; mod2 &lt;- readRDS(&quot;mymodel.rds&quot;) &gt; ls() [1] &quot;mod&quot; &quot;mod2&quot; &gt; identical(mod, mod2, ignore.environment = TRUE) [1] TRUE Save figures in a list p_list &lt;- list(p_ano=p_ano, p_tr=p_tr) # p_list[[name]] &lt;- p_obj p_list[[1]] f_name &lt;- paste0(fig_dir, sprintf(&quot;trend_analysis/image_list_%s.rds&quot;, con_name)) # saveRDS(p_list, f_name) # plot in a panel grid p_allCON &lt;- plot_grid(plotlist=p_list, align=&quot;vh&quot;, labels=sprintf(&quot;(%s)&quot;, letters[1:length(p_list)]), hjust=-1, nrow=3, label_size=12) p_allCON "],["5.1-data-input-output.html", "5.1 Data Input &amp; Output", " 5.1 Data Input &amp; Output 5.1.1 Read Data Read Fortran read.fortran(file, format, ..., as.is = TRUE, colClasses = NA) format Character vector or list of vectors. Read dta haven::read_dta() read Stata data file. data &lt;- read_dta(&quot;climate_health_2406yl.dta&quot;) # retrieve variable labels/definitions var_dict &lt;- tibble( &quot;name&quot; = colnames(data), &quot;label&quot; = sapply(data, function(x) attr(x, &quot;label&quot;)) %&gt;% as.character() ) var_dict var_label(data$gor) # get variable label val_labels(data$gor) # get value labels Read fixed width text files 5.1.1.1 Base R functions read.fwf(file, widths) widths integer vector, giving the widths of the fixed-width fields (of one line), or list of integer vectors giving widths for multiline records. read.table(f_name, header=FALSE, row.names, col.names, sep=\"\", na.strings = \"NA\") a very versatile function. Can be used to read .csv or .txt files. f_name path to data. header=FALSE defaults to FALSE, assumes there is no header row in the file unless specified otherwise. If there is a header in the first row, should specify header=TRUE. row.names a vector of row names. This can be a vector giving the actual row names, or a single number giving the column of the table which contains the row names, or character string giving the name of the table column containing the row names. col.names a vector of optional names for the variables. The default is to use \"V\" followed by the column number. sep use white space as delimiter. if it is a csv file, use sep=',' to specify comma as delimiter na.strings = \"NA\" a character vector of strings which are to be interpreted as NA values. A useful setting: na.strings = c(\"\", \"NA\", \"NULL\") read.csv(f_name, header = TRUE, sep = \",\", na.strings = \"..\", dec=\".\") header = TRUE whether the file contains the names of the variables as its first line. sep the field separator string. Values within each row of x are separated by this string. na the string to use for missing values in the data. dec the string to use for decimal points in numeric or complex columns: must be a single character. fileEncoding UTF-8 When reading data from github, you need to pass in the raw version of the data in read.csv(), R cannot read the display version. You can get the URL for the raw version by clicking on the Raw button displayed above the data. read.table(filename, header=FALSE, sep=\"\") is more versatile than read.csv. Useful when you have a data file saved as txt. Default separator is ‚Äúwhite space‚Äù for read.table, i.e., one or more spaces, tabs, newlines or carriage returns. # read.table can be used to read txt and csv. Need to specify sep=&#39;,&#39; when reading csv. data &lt;- read.table(&quot;https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/bonedensity.txt&quot;, header=TRUE) data data &lt;- read.table(&quot;https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/bonedensity.csv&quot;, header=TRUE, sep=&quot;,&quot;) # Alternatively, can use read_csv or read.csv directly data &lt;- read_csv(&quot;https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/bonedensity.csv&quot;) data 5.1.1.2 readr The major difference of readr is that it returns a tibble instead of a data frame. read_delim(f_name, delim = \";\", col_names = TRUE, skip = 0) allows you to specify the delimeter as ;. col_names = TRUE whether the first row contains column names. skip = 0 number of lines to skip before reading the data. Default is 0, meaning no lines are skipped. read_delim(f_name, delim = \"\\t\") read tab separated values. read_tsv() read tab separated values. Read comma separated values. readr::read_csv( f_name, na = c(&quot;..&quot;, NA, &quot;&quot;), locale = locale(encoding = &quot;UTF-8&quot;), col_types = cols(Date = col_date(format = &quot;%m/%d/%y&quot;)) ) col_types specify column types. Could be created by list() or cols(). read_csv will automatically guess, if you don‚Äôt explicitly specify column types. You can override column types by providing the argument col_types. You don‚Äôt need to provide all column types, just the ones you want to override. By default, reading a file without a column specification will print a message showing what readr guessed they were. To remove this message, set show_col_types = FALSE for one time setting, or set options(readr.show_col_types = FALSE) for the current sessions‚Äô global options setting. If want to change permanently everytime when R starts, put options(readr.show_col_types = FALSE) in .Rprofile as global options. read_csv2(f_name, na = c(\"..\", NA, \"\")) use semicolon ; to separate values; and use comma , for the decimal point. This is common in some European countries. locale The locale controls defaults that vary from place to place. The default locale is US-centric (like R), but you can use locale() to create your own locale that controls things like the default time zone, encoding, decimal mark, big mark, and day/month names. locale(date_names = \"en\", date_format = \"%AD\", time_format = \"%AT\", decimal_mark = \".\", grouping_mark = \",\", tz = \"UTC\", encoding = \"UTF-8\", asciify = FALSE) decimal_mark indicate the decimal place, can only be , or . encoding This only affects how the file is read - readr always converts the output to UTF-8. 5.1.2 Write Data Save data in uft8 encoding with special language characters write_excel_csv() include a UTF-8 Byte order mark which indicates to Excel the csv is UTF-8 encoded. write.csv(x, f_name, row.names=TRUE, fileEncoding =\"UTF-8\") x a matrix or data frame. If not one of the types, it is attempted to coerce x to a data frame. write_csv(x) x can only be data frame or tibble. Doesn‚Äôt support matrix. mat %&gt;% as_tibble(rownames = &quot;rowname&quot;) %&gt;% write_csv(&quot;mat.csv&quot;) mat %&gt;% write.csv(&quot;mat.csv&quot;) row.names whether to write row names of x. Defaults to TRUE. flextable flextable package create tables for reporting and publications. The main function is flextable which takes a data.frame as argument and returns a flextable. If you are using RStudio or another R GUI, the table will be displayed in the Viewer panel or in your default browser. The package provides a set of functions to easily create some tables from others objects. The as_flextable() function is used to transform specific objects into flextable objects. For example, you can transform a crosstab produced with the ‚Äòtables‚Äô package into a flextable which can then be formatted, annotated or augmented with footnotes. "],["5.2-functions.html", "5.2 Functions", " 5.2 Functions Function arguments fall into two sets: data argument: give input data to compute on detail argument: control details of the computation You can refer to an argument by its unique prefix. That is, partial matching is acceptable. But this is generally best avoided to reduce confusion. When calling a function you can specify arguments by position, by complete name, or by partial name. Arguments are matched first by exact name (perfect matching), then by prefix matching, and finally by position. If you specify arguments by names (full or partial), you can specify them in any order. If you specify arguments by position, you must specify them in the order they are defined in the function. Example: Here is a read.csv() function. read.csv(file, header = TRUE, sep = &quot;,&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) If you call read.csv(&quot;path/to/file.csv&quot;) it will read the file path/to/file.csv with default values for all other arguments. But if you call read.csv(FALSE, &quot;path/to/file.csv&quot;) this will return an error because¬†FALSE¬†is assigned to¬†file¬†and the filename is assigned to the argument¬†header. You can run. read.csv(header = FALSE, file = &quot;path/to/file.csv&quot;) To summarize: You can pass the arguments to¬†read.csv¬†without naming them if they are in the order that¬†R¬†expects. However, the order of the arguments matter if they are not named. When you call a function and specify arguments, it is recommended to put a space around =, also put a space after a comma, not before. x &lt;- 10; y &lt;- 5 x + y #&gt; [1] 15 `+`(x, y) #&gt; [1] 15 # ---------------------------- for (i in 1:2) print(i) #&gt; [1] 1 #&gt; [1] 2 `for`(i, 1:2, print(i)) #&gt; [1] 1 #&gt; [1] 2 # ---------------------------- x[3] #&gt; [1] NA # Note that only need to call the open braket `[`(x, 3) #&gt; [1] NA # ---------------------------- { print(1); print(2); print(3) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 `{`(print(1), print(2), print(3)) #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 # ---------------------------- sapply(1:5, `+`, 3) #&gt; [1] 4 5 6 7 8 sapply(1:5, &quot;+&quot;, 3) #&gt; [1] 4 5 6 7 8 Note the difference between + and \"+\". The first one is the value of the object called +, and the second is a string containing the character +. The second version works because sapply can be given the name of a function instead of the function itself: if you read the source of sapply(), you‚Äôll see the first line uses match.fun() to find functions given their names. Every operation is a function call Every operation in R is a function call, whether or not it looks like one. This includes infix operators like +, control flow operators like for, if, and while, subsetting operators like [] and $, and even the curly brace {. This means that each pair of statements in the following example is exactly equivalent. Note that `, the backtick, lets you refer to functions or variables that have otherwise reserved or illegal names: 5.2.1 Inspecting Object Types and Structure str(x), class(x), and typeof(x) str(x) focus on the structure not the contents. The output of the str() will vary depending on the type of R object you are passing it. For a data frame, the output will show the names of the columns, the class of each column, and the first few rows of data. For a list, the output will show the names of the elements in the list, the class of each element, and the value of each element. class(x) returns a class attribute, a character vector giving the names of the classes from which the object inherits. ÂèòÈáèÁöÑÁ±ªÂûã, eg., dataframe, tibble, vector. If the object does not have a class attribute, it has an implicit class, notably \"matrix\", \"array\", \"function\" or \"numeric\" or the result of typeof(x) A property (Â±ûÊÄß) assigned to an object that determines how generic functions operate with it. It is not a mutually exclusive classification. If an object has no specific class assigned to it, such as a simple numeric vector, it‚Äôs class is usually the same as its mode, by convention. typeof determines the (R internal) type or storage mode of any object. ÂèòÈáèÈáåÈù¢Â≠òÂÇ®Êï∞ÊçÆÁöÑÁ±ªÂûã, eg., string, numeric, integer. Current values are the vector types \"logical\", \"integer\", \"double\",\"complex\", \"character\", \"raw\" and \"list\", \"NULL\", \"closure\" (function), \"special\"and \"builtin\" (basic functions and operators), \"environment\", \"S4\" (some S4 objects) and others that are unlikely to be seen at user level (\"symbol\", \"pairlist\", \"promise\",\"language\", \"char\", \"...\", \"any\", \"expression\", \"externalptr\", \"bytecode\" and\"weakref\"). mode(x) is similar to typeof(x) mutually exclusive. One object has one typeof and mode. methods(class=\"zoo\") get a list of functions that have zoo-methods. attributes(x) returns the object‚Äôs attributes/metadata as a list. Some of the most common attributes are: row names and column names, dimensions, and class. Attributes are not stored internally as a list and should be thought of as a set and not a vector, i.e, the order of the elements of attributes()does not matter. To access a specific attribute, you can use the attr()function. attr(x, which) Get or set specific attributes of an object. x an object whose attributes are to be accessed. which a character string specifying which attribute is to be accessed. attr(x, which) &lt;- value specify value to the attribute value an object, the new value of the attribute, or NULL to remove the attribute. # create a 2 by 5 matrix &gt; x &lt;- 1:10 &gt; attr(x, &quot;dim&quot;) &lt;- c(2, 5) &gt; x [,1] [,2] [,3] [,4] [,5] [1,] 1 3 5 7 9 [2,] 2 4 6 8 10 &gt; my_factor &lt;- factor(c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;), ordered = T, levels = c(&quot;A&quot;, &quot;B&quot;)) &gt; my_factor [1] A A B Levels: A &lt; B &gt; attributes(my_factor) $levels [1] &quot;A&quot; &quot;B&quot; $class [1] &quot;ordered&quot; &quot;factor&quot; Q: What does 1L mean? A: 1L is a shorthand for as.integer(1). Adding suffix L ensures that the value is treated as an integer and it is useful for memory usage and specific computations involving integer operations. # create numerical value num_val &lt;- 1 # check the data type print(class(num_val)) [1] &quot;numeric&quot; print(typeof(num_val)) [1] &quot;double&quot; # create integer value int_val &lt;- 1L # check the data type print(class(int_val)) [1] &quot;integer&quot; print(typeof(int_val)) [1] &quot;integer&quot; 5.2.2 Type of Variables There are two types of vectors: Atomic vectors, of which there are six types: logical, integer, double, character, complex, and raw. Integer and double vectors are collectively known as numericvectors. Lists, which are sometimes called recursive vectors because lists can contain other lists. The chief difference between atomic vectors and lists is that atomic vectors are homogeneous, while lists can be heterogeneous. There‚Äôs one other related object: NULL. NULL is often used to represent the absence of a vector (as opposed to NAwhich is used to represent the absence of a value in a vector). NULL typically behaves like a vector of length 0. The Figure below summarises the interrelationships. Variable Coercion as.logical convert 0/1 to boolean values # remove NA values mask &lt;- is.na(reg_data) %&gt;% rowSums() %&gt;% as.logical() reg_data[mask,] reg_data &lt;- reg_data[!mask,] Dimension Reduction Convert a data frame, tibble, list to an atomic vector unlist(df) or as.matrix(df) %&gt;% as.vector() Convert a matrix to a vector as.vector(x) unlist() TL;DR: takes in a list, returns a vector. It ‚Äúun-lists‚Äù nested lists or vectors and converts them into a simple atomic vector. In other words, it takes a list that contains other lists, vectors, or atomic elements and flattens it into a single vector. Useful when flatten a nested or hierarchical list to a vector. Dimension reduction. Simplifying the structure of a data object Passing a list to a function that only accepts vectors Combining the elements of a list into a single vector &gt; list(1, 2, 3, 4, 5) [[1]] [1] 1 [[2]] [1] 2 [[3]] [1] 3 [[4]] [1] 4 [[5]] [1] 5 &gt; list(1, 2, 3, 4, 5) %&gt;% unlist() [1] 1 2 3 4 5 # flatten a nested list &gt; list(a = 1, b = list(c = 2, d = 3), e = 4) %&gt;% unlist() a b.c b.d e 1 2 3 4 &gt; data.frame(matrix(1:12,3,4)) %&gt;% unlist() # flatten by column X11 X12 X13 X21 X22 X23 X31 X32 X33 X41 X42 X43 1 2 3 4 5 6 7 8 9 10 11 12 5.2.3 Variable Scope with(data, expr, ‚Ä¶) Evaluate an R expression, expr, in an environment constructed from data, possibly modifying (a copy of) the original data. expr a single expression or a compounded one, i.e., of the form { a &lt;- somefun() # do some changes to cols b &lt;- otherfun() ..... rm(unused1, temp) # remove cols you don&#39;t want anymore } within(data, expr, ‚Ä¶) is similar to with, except that it examines the environment after the evaluation of expr and makes the corresponding modifications to a copy of data (this may fail in the data frame case if objects are created which cannot be stored in a data frame), and returns it. Returned value: For within, the modified object. For with, the value of the evaluated expr. with(mtcars, mpg[cyl == 8 &amp; disp &gt; 350]) # is the same as, but nicer than mtcars$mpg[mtcars$cyl == 8 &amp; mtcars$disp &gt; 350] 5.2.4 Control Structures Note all keywords are lowercase here. A ‚Äòdo ‚Ä¶ until‚Äô loop in R: repeat { # code if(stop_condition_is_true) break } while(TRUE){ # Do things if (stop_condition_is_true) break } break statement can break out of a loop. next statement causes the loop to skip the current iteration and start the next one. switch(exp, case1, case2, ...) the expression is matched with the list of values and the corresponding value is returned. a &lt;- 4 switch(a, &quot;1&quot;=&quot;this is the first case in switch&quot;, &quot;2&quot;=&quot;this is the second case in switch&quot;, &quot;3&quot;=&quot;this is the third case in switch&quot;, &quot;4&quot;=&quot;this is the fourth case in switch&quot;, &quot;5&quot;=&quot;this is the fifth case in switch&quot; ) ifelse(test_expression, x, y) The returned vector has element from x if the corresponding value of test_expression is TRUE; or from y if the corresponding value of test_expression is FALSE. That is the i-th element of result will be x[i] if test_expression[i] is TRUE else it will take the value of y[i]. ref: R programming for Data Science, chap 13, control structures "],["5.3-basic-r-functions.html", "5.3 Basic R functions", " 5.3 Basic R functions 5.3.1 Workspace Management Functions for managing objects and getting help in your R workspace. ls() lists all of the objects in your workspace. rm(list=ls()) remove all objects in the working environment Check if var exists exists(\"x\") determine whether x exists in global environment. Note that variable is in quotes. Check if a file/folder exists if (!dir.exists(&quot;output&quot;)){ dir.create(&quot;output&quot;) } else{ print(&quot;dir exists&quot;) } data &lt;- &#39;my_data.csv&#39; if(file.exists(data)){ df &lt;- read.csv(data) } else { print(&#39;Does not exist&#39;) } list.files(dir, pattern=NULL) returns a character vector of the file names in dir. pattern an optional regular expression. Only file names which matches the regular expression will be returned. dir.create(path) create a direcotry. file.copy(from, to) copy files from one directory to another. 5.3.2 Data Display and Output Functions for displaying and formatting data output. cat(x) Outputs the objects, concatenating the representations. cat performs much less conversion than print. cat is useful for producing output in user-defined functions. It converts its arguments to character vectors, concatenates them to a single character vector, appends the given sep = string(s) to each element and then outputs them. sprintf(fmt, ...) The string fmt contains normal characters, which are passed through to the output string, and also conversion specifications which operate on the arguments provided through ‚Ä¶. The allowed conversion specifications start with a % and end with one of the letters in the set aAdifeEgGosxX%. https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sprintf re-use one argument in fmt: add 1$ between % and s, numbers specify the position, $ as place holder; Immediately after % may come 1$ to 99$ to refer to a numbered argument. If this is done it is best if all formats are numbered: if not the unnumbered ones process the arguments in order. a &lt;- &quot;tree&quot; sprintf(&quot;The %1$s is large but %1$s is small. %1$s&quot;, a) [1] &quot;The tree is large but tree is small. tree&quot; Round to 2 digits after decimal. sprintf(&quot;%.2f&quot;, r_A) Use double percentage to escape % # print percentage sing: use double percent &gt; sprintf(&quot;%.2f%%&quot;, r_A*100) [1] &quot;9.62%&quot; The difference in print and sprintf is in the return value. sprintf returns a character vector containing a formatted combination of text and variable values. print prints its argument and returns it invisibly. If we want to print in a for loop wrap the sprintf in a print or cat or message. Automatic printing is turned off in loops. If you want to print some return values, must explicitly specify. summary(x), str(x) implicitly have print function, this info will print inside for loops. print vs.¬†cat: print is simply returning the character string as data object. The cat function, in contrast, interprets code-specific information (e.g.¬†\\n is converted into a newline), returns a readable version of our character string. if you want to display an R object in a for loop, use print. data &lt;- data.frame(x1 = 1:5, # Create example data x2 = 3) print(data) # Apply print to data frame # x1 x2 # 1 1 3 # 2 2 3 # 3 3 3 # 4 4 3 # 5 5 3 if you want to display charater strings as tracking messages, use cat. my_string &lt;- &quot;This is \\nan example string&quot; # Create example string print(my_string) # Apply print to character string # [1] &quot;This is \\nan example string&quot; cat(my_string) # Apply cat to character string # This is # an example string cat is valid only for atomic types (logical, integer, real, complex, character) and names. It means you cannot call cat on a non-empty list or any type of object. In practice it simply converts arguments to characters and concatenates so you can think of something like as.character() %&gt;% paste(). print is a generic function so you can define a specific implementation for a certain S3 class. &gt; foo &lt;- &quot;foo&quot; &gt; print(foo) [1] &quot;foo&quot; &gt; attributes(foo)$class &lt;- &quot;foo&quot; &gt; print(foo) [1] &quot;foo&quot; attr(,&quot;class&quot;) [1] &quot;foo&quot; &gt; print.foo &lt;- function(x) print(&quot;This is foo&quot;) &gt; print(foo) [1] &quot;This is foo&quot; Another difference between cat and print is returned value. cat invisibly returns NULL while print returns its argument. This property of print makes it particularly useful when combined with pipes: coefs &lt;- lm(Sepal.Width ~ Petal.Length, iris) %&gt;% print() %&gt;% coefficients() paste(‚Ä¶, sep = \" \", collapse = NULL) ... one or more R objects, to be converted to character vectors. sep a character string to separate the terms. collapse an optional character string to separate the results Creating a comma separated string vector single quotes &gt; paste(shQuote(seq(1:5)), collapse=&quot;, &quot;) [1] &quot;&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;&quot; double quotes &gt; cat(paste(shQuote(seq(1:5), type=&quot;cmd&quot;), collapse=&quot;, &quot;)) &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot; Useful function to enclose text strings in quotes and connect with comma This is useful when you want to create a long sql command including many identifiers. paste_with_quotes &lt;- function(string_vec, type=&quot;single&quot;) { # Paste collapse with quotes # @string_vec: vector of string # @type: single or double quotes, defaults to single if (type==&quot;single&quot;) { paste(shQuote(string_vec), collapse=&quot;, &quot;) %&gt;% cat() } else { # double quotes toString(shQuote(string_vec, type=&quot;cmd&quot;)) %&gt;% cat() } } &gt; paste_with_quotes(c(&quot;CLZD12-R&quot;, &quot;GW7KTS-R&quot;)) &#39;CLZD12-R&#39;, &#39;GW7KTS-R&#39; &gt; paste_with_quotes(c(&quot;CLZD12-R&quot;, &quot;GW7KTS-R&quot;), type=&quot;double&quot;) &quot;CLZD12-R&quot;, &quot;GW7KTS-R&quot; sep creates element-wise sandwich stuffed with the value in the sep argument. paste(slices_1, slices_2, sep='jam') collapse creates ONE big sandwich with the value of collapse argument added between the sandwiches produced by using the sep argument. paste(slices_1, slices_2, sep='jam', collapse='cheese') paste(charac_vec, collapse='+') auxilary_v # [1] &quot;CLD&quot; &quot;DTR&quot; &quot;FRS&quot; &quot;PRE&quot; &quot;TMN&quot; &quot;TMP&quot; &quot;TMX&quot; &quot;VAP&quot; &quot;WET&quot; &quot;URB&quot; &quot;LAT&quot; # [12] &quot;LON&quot; &quot;MON&quot; &quot;ALT&quot; paste(auxilary_v, sep = &quot; + &quot;) # doesn&#39;t change anything # [1] &quot;CLD&quot; &quot;DTR&quot; &quot;FRS&quot; &quot;PRE&quot; &quot;TMN&quot; &quot;TMP&quot; &quot;TMX&quot; &quot;VAP&quot; &quot;WET&quot; &quot;URB&quot; &quot;LAT&quot; # [12] &quot;LON&quot; &quot;MON&quot; &quot;ALT&quot; paste(auxilary_v, collapse = &quot; + &quot;) # [1] &quot;CLD + DTR + FRS + PRE + TMN + TMP + TMX + VAP + WET + URB + LAT + LON + MON + ALT&quot; Paste vector with \", \", using shQuote. paste(shQuote(as.list(Model_vec)), collapse = &quot;, &quot; ) paste0(‚Ä¶, collapse) is equivalent to paste(‚Ä¶, sep = \"\", collapse), slightly more efficiently. gor_shape@data %&gt;% as_tibble() %&gt;% print(n=10, width=Inf) display all data.frame. n specify # of rows. width specify # of columns. Controls the maximum number of columns on a line used in printing vectors, matrices and arrays, and when filling by cat. 5.3.3 Data Manipulation and Transformation Functions for manipulating, transforming, and organizing data. rev(x) provides a reversed version of its argument. Useful in reversing color scales. &gt; rev(c(1:5, 5:3)) # [1] 3 4 5 5 4 3 2 1 seq(from, to, by, length.out, along.with) create a sequence by increment of the sequence. length.out desired length of the sequence. along.with take the length from the length of this argument. rep() repeats vector rep(x, times) if times is one integer, then repeat x as a whole times if times is a vector of the same lenght as x, then repeat each element by the number of times in times. rep(x, each) repeat each element in x by times # repeat x as a whole &gt; rep(c(0, 0, 7), times = 3) # [1] 0 0 7 0 0 7 0 0 7 # repeat each element by respective times &gt; rep(c(0, 7), times = c(4,2)) # [1] 0 0 0 0 7 7 # repeat each element by the same number of times &gt; rep(c(2, 4, 2), each = 3) # [1] 2 2 2 4 4 4 2 2 2 # repeat as a whole, same length as specified in `length.out` &gt; rep(1:3, length.out=7) # [1] 1 2 3 1 2 3 1 Finding and Locating Elements match(x, y) returns a vector of the positions of (first) matches of its first argument in its second. x the values to be matched; y the values to be matched against. which(x) returns TRUE indices of a logical object x a logical vector or array. NAs are allowed and omitted (treated as if FALSE). identical(x, y) The safe and reliable way to test two objects for being exactly equal. near() This is a safe way of comparing if two vectors of floating point numbers are (pairwise) equal. This is safer than using ==, because it has a built in tolerance. Returns a logical vector (TRUE/FALSE for each element comparison) near(x, y, tol = .Machine$double.eps^0.5) all.equal(x, y, tolerance=1.5e-8) floating point comparison, in contrast to exact comparison identical(). The all.equal() function allows you to test for equality with a difference tolerance of 1.5e-8, ‚Äúnear equality‚Äù. It does an overall comparison of the two objects and returns a single TRUE/FALSE, while near does a pairwise comparison and returns a logical vector of for the element-wise comparison. If the difference is greater than the tolerance level the function will return the mean relative difference. return TRUE if x and y are approximately equal; it returns a summary, either equal or not equal; not a complete result of one-by-one comparison; x &lt;- c(0.1, 0.2, 0.3) y &lt;- c(0.1000001, 0.2000001, 0.3000001) # near() - element-wise logical vector near(x, y) # [1] TRUE TRUE TRUE # all.equal() - single summary result all.equal(x, y) # [1] TRUE (if within tolerance) subset(x, subset, select, drop = FALSE, ‚Ä¶) Subsetting Vectors, Matrices And Data Frames x object to be subsetted. subset logical expression indicating elements or rows to keep: missing values are taken as false. select columns to select from a data frame. subset(airquality, Temp &gt; 80, select = c(Ozone, Temp)) subset(airquality, Day == 1, select = -Temp) subset(airquality, select = Ozone:Wind) with(airquality, subset(Ozone, Temp &gt; 80)) Standardizing Data scale(x, center = TRUE, scale = TRUE) scale is generic function whose default method centers and/or scales the columns of a numeric matrix. x a numeric matrix(like object). center either a logical value or numeric-alike vector of length equal to the number of columns of x The value of center determines how column centering is performed. If center is a numeric-alike vector with length equal to the number of columns of x, then each column of x has the corresponding value from center subtracted from it. If center is TRUE then centering is done by subtracting the column means (omitting NAs) of x from their corresponding columns, and if center is FALSE, no centering is done. scale either a logical value or a numeric-alike vector of length equal to the number of columns of x. The value of scale determines how column scaling is performed (after centering). If scale is a numeric-alike vector with length equal to the number of columns of x, then each column of x is divided by the corresponding value from scale. If scale is TRUE then scaling is done by dividing the (centered) columns of x by their standard deviations if center is TRUE, and the root mean square otherwise. If scale is FALSE, no scaling is done. ## To scale by the standard deviations without centering, use scale(x, center = FALSE, scale = apply(x, 2, sd, na.rm = TRUE)) Squish values into range scales::squish(x, range = c(0, 1) ) out of bound values handling. It replaces out of bounds values with the nearest limit. # sometimes need to add to a small number to the lower bound c &lt;- 10 * .Machine$double.eps squish(x, range = c(0+c, 1) ) Data Subsetting subset(x, subset, select, drop = FALSE, ...) subsetting vectors, matrices, tibbles ‚Ä¶ This is a generic function, with methods supplied for many data types. x vector, list, matrix, dataframe, or tibble subset logical expression indicating elements or rows to keep: missing values are taken as false. select expression, indicating columns to select from a data frame. drop passed on to [ indexing operator. ## examples subset(airquality, Temp &gt; 80, select = c(Ozone, Temp)) subset(airquality, Day == 1, select = -Temp) subset(airquality, select = Ozone:Wind) with(airquality, subset(Ozone, Temp &gt; 80)) agg_window &lt;- function(df, win, FUN, ...){ # applies `FUN` non-overlapping window of length `win` # for data frame `df` n &lt;- ceiling(nrow(df)/win) if (n!=(nrow(df)/win)) print (&quot;short multiples&quot;) group_idx &lt;- rep(1:n, each=win)[1:nrow(df)] df$key &lt;- group_idx df_agg &lt;- aggregate(. ~ key, df, FUN, ...) df_agg$key &lt;- NULL return (df_agg) } agg_window(the_CC[,-(1:6)], 12, mean, na.rm=TRUE, na.action=NULL) # na.rm=TRUE, drop rows with na values to enable mean calcualion # na.action=NULL, to ensure that rows with NA values are not dropped when performing calculations. drop=FALSE prevents from droping the dimensions of the array. Note the comma before the cmd, it is necessary to indicate drop=FALSE is the third argument. &gt; df &lt;- data.frame(a = 1:100) &gt; df[1:10,] [1] 1 2 3 4 5 6 7 8 9 10 &gt; df[1:10, , drop=FALSE] a 1 1 2 2 3 3 4 4 5 5 ... cut() - Converting Continuous to Categorical cut(x, breaks, labels=NULL, include.lowest=FALSE, right = TRUE, dig.lab=3) converts numeric to factor, divides the range of x into intervals and codes the values in x according to which interval they fall. The leftmost interval corresponds to level one, the next leftmost to level two and so on. Key Features: cut() by default is left open right closed interval as \"(b1, b2]\", \"(b2, b3]\" etc. for right = TRUE and as \"[b1, b2)\", ‚Ä¶ if right = FALSE. dig.lab = 3 defines integer which is used when labels are not given. Example Usage: addPval.symbol &lt;- function(x){ ## add significance as symbols. The p-value of a trend slope ## is added as symbol as following: ## *** (p &lt;= 0.001), ** (p &lt;= 0.01), ## * (p &lt;= 0.05), . (p &lt;= 0.1) and no symbol if p &gt; 0.1. ## @param x: a numeric vector of p-values cutpoints &lt;- c(0, 0.001, 0.01, 0.05, 0.1, 1) symbols &lt;- c(&quot;***&quot;, &quot;**&quot;, &quot;*&quot;, &quot;.&quot;, &quot; &quot;) cut(x, breaks=cutpoints, labels=symbols) } 5.3.4 Generate Random Seeds Save Random Seed When you need to generate random numbers for your model, in order to ensure reproducibility, the best practice is to save the random seed. # y is a Bernoulli with probability pr eff_seed &lt;- sample(1:2^15, 1) eff_seed &lt;- 25662 print(sprintf(&quot;Seed for session: %s&quot;, eff_seed)) set.seed(eff_seed) 5.3.5 Draw random samples rbinom(n, size, prob) binomial distribution with parameter size and prob. size for the number of trials. when size=1, it generate the Bernoulli distribution \\[ X = \\begin{cases} 1 &amp; \\text{with prbability }p \\\\ 0 &amp; \\text{with prbability }1-p \\\\ \\end{cases} \\] Binomial is the sum of Bernoulli \\[ Y = \\sum_{i=1}^{\\text{size}} X_i \\] The probability is given by \\[ P(Y=y) = \\begin{pmatrix} \\text{size} \\\\ y \\end{pmatrix} p^y (1-y)^{\\text{size}-y} \\] for \\(y=0, \\ldots, \\text{size}.\\) Two ways to generate a Bernoulli distribution sample. use rbinom and specify size to be 1. rbinom(n = 20, size = 1, prob = 0.7) set n = 20 to indicate 20 draws from a binomial distribution, set size = 1to indicate the distribution is for 1 trial, and p = 0.7 to specify the distribution is for a ‚Äúsuccess‚Äù probability of 0.7: use sample and specify respective probabilities using prob. sample(x, size, replace=FALSE, prob=NULL) draw random samples from a sample space using either with or without replacement. prob a vector of probability weights for obtaining the elements of the vector being sampled. Of the same length as x. sample(c(0,1), size = 20, replace = TRUE, prob = c(0.3, 0.7)) Here‚Äôs a sample of 20 zeroes and ones, where 0 has a 30% chance of being sampled and 1 has a 70% chance of being sampled. 5.3.6 Fit a distribution # fit a lognormal distribution library(MASS) fit_params &lt;- fitdistr(prices_monthly$AdjustedPrice,&quot;lognormal&quot;) fit_params$estimate x &lt;- prices_monthly$AdjustedPrice %&gt;% {seq(min(.), max(.), length=30)} # data point at which to compute density x fit &lt;- dlnorm(x, fit_params$estimate[&#39;meanlog&#39;], fit_params$estimate[&#39;sdlog&#39;]) 5.3.7 Operation on list purrr::map(.x, .f, ...) return a list. The map function transforms its input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input. The main advantage of map() is the helpers which allow you to write compact code for common special cases. .x A list or atomic vector. .f A function, formula, or vector (not necessarily atomic). .f can be a named function, e.g., mean. Formula: ~ . + 1 is equivalent to function(x) x + 1. This syntax allows you to create very compact anonymous functions. . or .x refer to the first argument. For a two argument function, use .x and .y For more arguments, use ..1, ..2, ..3 etc subset lists For instance, we need the 2nd element of a nested list. We can use map(list, 2), while lapply(list, 2) doesn‚Äôt work ; ... Additional arguments passed on to .f. Type-specific map functions simply many lines of code map_dfr() and map_dfc() return data frames created by row-binding and column-binding respectively. map_lgl(), map_int(), map_dbl() and map_chr() return an atomic vector of the indicated type (or die trying). If character vector, numeric vector, or list, it is converted to an extractor function. Character vectors index by name and numeric vectors index by position; use a list to index by position and name at different levels. If a component is not present, the value of .default will be returned. map() can be used as a concise loop. l &lt;- map(1:4, ~ sample(1:10, 15, replace = T)) str(l) #&gt; List of 4 #&gt; $ : int [1:15] 7 1 8 8 3 8 2 4 7 10 ... #&gt; $ : int [1:15] 3 1 10 2 5 2 9 8 5 4 ... #&gt; $ : int [1:15] 6 10 9 5 6 7 8 6 10 8 ... #&gt; $ : int [1:15] 9 8 6 4 4 5 2 9 9 6 ... Select first element of nested list x &lt;- list(list(1,2), list(3,4), list(5,6)) # use lapply lapply(x, `[[`, 1) # use `purrr::map` purrr::map(x, 1) Use examples of purrr:map # Generate normal distributions from an atomic vector giving the means 1:10 %&gt;% map(rnorm, n = 10) # You can also use an anonymous function 1:10 %&gt;% map(function(x) rnorm(10, x)) # Or a formula 1:10 %&gt;% map(~ rnorm(10, .x)) # Simplify output to a vector instead of a list by computing the mean of the distributions 1:10 %&gt;% map(rnorm, n = 10) %&gt;% # output a list map_dbl(mean) # output an atomic vector #&gt; [1] 1.328465 2.489343 2.598304 4.208711 5.036009 5.853896 6.943884 7.779394 8.727930 9.793523 &gt; set_names(c(&quot;foo&quot;, &quot;bar&quot;)) %&gt;% map_chr(paste0, &quot;:suffix&quot;) foo bar &quot;foo:suffix&quot; &quot;bar:suffix&quot; You can apply regression to each group with map, see Split-and-Apply Operations for more details. Find the values that occur in every element. # use `intersect` three times out &lt;- l[[1]] out &lt;- intersect(out, l[[2]]) out &lt;- intersect(out, l[[3]]) out &lt;- intersect(out, l[[4]]) out #&gt; [1] 8 4 # alternatively use `reduce` once reduce(l, intersect) #&gt; [1] 8 4 purrr::reduce(.x, .f, ...) takes a vector of length n and produces a vector of length 1 by calling a function with a pair of values at a time: reduce(1:4, f) is equivalent to f(f(f(1, 2), 3), 4). .x A list or atomic vector. List all the elements that appear in at least one entry. reduce(l, union) #&gt; [1] 7 1 8 3 2 4 10 5 9 6 "],["5.4-data-splitting-and-grouping.html", "5.4 Data Splitting and Grouping", " 5.4 Data Splitting and Grouping Functions for splitting data into groups and applying operations. 5.4.1 The split() Function split(x, f, drop = FALSE, ‚Ä¶) divides the data in the vector x into the groups defined by f. Parameters: x - vector or data frame containing values to be divided into groups. f - a ‚Äòfactor‚Äô in the sense that as.factor(f) defines the grouping; f is recycled as necessary and if the length of x is not a multiple of f, a warning is printed. if x is a data frame, f can be a lambda formula of the form ~ g to split by the variable g, or by the interaction of more variables, e.g., ~ g1+g2. Return Value: split returns a list of vectors containing the values for the groups. The components of the list are named by the levels of f. Key Advantage: The advantage of split is to index groups by their key names directly, which is not true for dplyr::group_by, which can only be indexed by group number. Basic Vector Splitting &gt; a &lt;- c(x = 3, y = 5, x = 1, x = 4, y = 3) &gt; a x y x x y 3 5 1 4 3 &gt; split(a, f=names(a)) $x x x x 3 1 4 $y y y 5 3 # split into two groups with roughly equal size &gt; split(a, f=factor(1:2)) $`1` x x y 3 1 3 $`2` y x 5 4 Warning message: In split.default(a, f = factor(1:2)) : data length is not a multiple of split variable Data Frame Splitting You can use the split function to split data frames in groups. Single Variable Splitting: # split `df` based on `Treatment` &gt; split(df, f = df$Treatment) $`nonchilled` Plant Type Treatment conc uptake 15 Qn3 Quebec nonchilled 95 16.2 49 Mn1 Mississippi nonchilled 1000 35.5 48 Mn1 Mississippi nonchilled 675 32.4 10 Qn2 Quebec nonchilled 250 37.1 44 Mn1 Mississippi nonchilled 175 19.2 $chilled Plant Type Treatment conc uptake 68 Mc1 Mississippi chilled 500 19.5 32 Qc2 Quebec chilled 350 38.8 27 Qc1 Quebec chilled 675 35.4 23 Qc1 Quebec chilled 175 24.1 79 Mc3 Mississippi chilled 175 18.0 split can be based on a combination of columns. # split by `Treatment` and `Type` &gt; split(df, f = list(df$Type, df$Treatment)) $`Quebec.nonchilled` Plant Type Treatment conc uptake 15 Qn3 Quebec nonchilled 95 16.2 10 Qn2 Quebec nonchilled 250 37.1 $Mississippi.nonchilled Plant Type Treatment conc uptake 49 Mn1 Mississippi nonchilled 1000 35.5 48 Mn1 Mississippi nonchilled 675 32.4 44 Mn1 Mississippi nonchilled 175 19.2 $Quebec.chilled Plant Type Treatment conc uptake 32 Qc2 Quebec chilled 350 38.8 27 Qc1 Quebec chilled 675 35.4 23 Qc1 Quebec chilled 175 24.1 $Mississippi.chilled Plant Type Treatment conc uptake 68 Mc1 Mississippi chilled 500 19.5 79 Mc3 Mississippi chilled 175 18.0 You can recover the original data frame with the unsplit function: unsplit(dfs, f = list(df$Type, df$Treatment)) More options for spiting data frames: https://www.spsanderson.com/steveondata/posts/2024-10-01/ 5.4.2 Split-and-Apply Operations Combining splitting with function application for grouped operations. Manual Split-Apply Pattern The most common pattern is: split ‚Üí lapply Example Usage: # Sample order data orders &lt;- data.frame( order_id = 1:10, product = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;), amount = c(100, 150, 200, 120, 180, 90, 210, 160, 130, 140) ) # Split orders by product orders_by_product &lt;- split(orders, orders$product) # Analyze each product category lapply(orders_by_product, function(x) sum(x$amount)) Another commonly used combination is: split ‚Üí map Apply regression to each group with map # A more realistic example: split a data frame into pieces, fit a # model to each piece, summarise and extract R^2 mtcars %&gt;% split(.$cyl) %&gt;% map(~ lm(mpg ~ wt, data = .x)) %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) # If each element of the output is a data frame, use # map_dfr to row-bind them together: mtcars %&gt;% split(.$cyl) %&gt;% map(~ lm(mpg ~ wt, data = .x)) %&gt;% map_dfr(~ as.data.frame(t(as.matrix(coef(.))))) # (if you also want to preserve the variable names see # the broom package) All-in-One Approach plyr::ddply() - For each subset of a data frame, apply function then combine results into a data frame. Parameter Definition .data data frame to be processed .variables variables to split data frame by, as as.quoted variables, a formula or character vector .fun function to apply to each piece ... other arguments passed on to .fun Usage example: library(plyr) ddply(BData[,c(&quot;iso&quot;,&quot;gdp&quot;)], .(iso), function(x) sum(is.na(x[,-1]))) by(data, INDICES, FUN, ‚Ä¶, simplify = TRUE) an object-oriented wrapper for tapply applied to data frames. Apply a function to a data frame split by factors. data a data frame, matrix INDICES a factor or a list of factors, each of length nrow(data) FUN a function to be applied to (usually data-frame) subsets of data. ... further arguments to FUN. x &lt;- by(mtcars, mtcars$cyl, function(x) apply(x,2,mean)) # reshape to a data frame do.call(rbind, x) # or t(sapply(x, I)) # mpg cyl disp hp drat wt qsec vs # 4 26.66364 4 105.1364 82.63636 4.070909 2.285727 19.13727 0.9090909 # 6 19.74286 6 183.3143 122.28571 3.585714 3.117143 17.97714 0.5714286 # 8 15.10000 8 353.1000 209.21429 3.229286 3.999214 16.77214 0.0000000 # am gear carb # 4 0.7272727 4.090909 1.545455 # 6 0.4285714 3.857143 3.428571 # 8 0.1428571 3.285714 3.500000 Efficient spliting with data.table For large datasets, the data.table package offers high-performance data manipulation tools. Here‚Äôs how you can split a data frame using data.table: library(data.table) # a simple example &gt; set.seed(123) &gt; df &lt;- data.frame( id = 1:6, group = sample(LETTERS[1:3], 6, replace=TRUE), value = c(10, 15, 20, 25, 30, 35) ) # Convert the data frame to a data.table &gt; dt &lt;- as.data.table(df) &gt; dt id group value 1: 1 C 10 2: 2 C 15 3: 3 C 20 4: 4 B 25 5: 5 C 30 6: 6 B 35 # Split the data.table &gt; split_dt &lt;- dt[, .SD, by = group] # This creates a data.table with a list column &gt; split_dt group id value 1: C 1 10 2: C 2 15 3: C 3 20 4: C 5 30 5: B 4 25 6: B 6 35 # use print(.SD) to print by group &gt; dt[, print(.SD), by = group] id value 1: 1 10 2: 2 15 3: 3 20 4: 5 30 id value 1: 4 25 2: 6 35 Empty data.table (0 rows and 1 cols): group You will notice the data.table comes back as one but you will see that were id was, is now a factor column called group. .SD stands for something like ‚ÄúSubset of Data.table‚Äù. It refers to the current group. .BY gives a list of the current value of the groupping variable. There‚Äôs no significance to the initial \".\", except that it makes it even more unlikely that there will be a clash with a user-defined column name. library(data.table) # Simulate a large dataset set.seed(123) large_df &lt;- data.table( id = 1:1e6, group = sample(LETTERS[1:5], 1e6, replace = TRUE), value = rnorm(1e6) ) # Split and process the data efficiently result &lt;- large_df[, .(mean_value = mean(value), count = .N), by = group] print(result) "],["5.5-apply-family.html", "5.5 *apply() Family", " 5.5 *apply() Family tapply(x, INDEX, FUN, ...) break x into groups based on INDEX, apply FUN to each group (subset), and return the results in a convenient form. INDEX a list of one or more factors, each of same length as X. The elements are coerced to factors by as.factor. Return a vecotr when FUN returns a single atomic value, length determined by # of component in INDEX; if FUN returns more than one value, tapply returns a list. tapply(X, INDEX, FUN = NULL) Arguments: -X: An object, usually a vector -INDEX: A list containing factor, of length of X -FUN: Function applied to each element of x apply(x, MARGIN, FUN) -x: an array or matrix -MARGIN: take a value or range between 1 and 2 to define where to apply the function: -MARGIN=1: the manipulation is performed per row -MARGIN=2: the manipulation is performed per column -MARGIN=c(1,2) the manipulation is performed on rows and columns lapply() takes list, vector or data frame as input and gives output in list. lapply(X, FUN) Arguments: -X: A vector or an object -FUN: Function applied to each element of x lapply(df, FUN) is a shortcut to apply(df, MARGIN=2, FUN), conducting FUN on each column. sapply() function takes list, vector or data frame as input and gives output in vector or matrix. sapply() function does the same job as lapply() function but returns a vector. sapply() function is more efficient than lapply() in the output returned because sapply() store values direclty into a vector. sapply(X, FUN, ...) X: A vector or an object FUN: Function to be applied to each element of X. In the case of functions like +, %*%, the function name must be backquoted or quoted. ... : optional arguments to FUN. A summary for differences of *apply() functions. Function Arguments Objective Input Output apply apply(x, MARGIN, FUN) Apply a function to the rows or columns or both, MARGIN=1 on rows, MARGIN=2 on cols Data frame or matrix vector, list, array lapply lapply(X, FUN, ‚Ä¶) Apply a function to all the elements of the input List, vector or data frame list sapply sappy(X, FUN, ‚Ä¶) Apply a function to all the elements of the input List, vector or data frame vector or matrix mapply mapply(FUN, ‚Ä¶ , MoreArgs = NULL, SIMPLIFY = TRUE) mapply is a multivariate version of sapply. mapply applies FUN to the first elements of each ... argument, the second elements, the third elements, and so on. Arguments are recycled if necessary. Multiple List or multiple Vector Arguments. vector or matrix tapply tapply(X, INDEX, FUN = NULL) applies a function or operation on subset of the vector broken down by a given factor variable. Similar to group_by and summarize. List, vector or data frame vector or list A useful application is to combine lapply() or sapply() with subsetting: x &lt;- list(1:3, 4:9, 10:12) sapply(x, &quot;[&quot;, 2) #&gt; [1] 2 5 11 # equivalent to sapply(x, function(x) x[2]) #&gt; [1] 2 5 11 mapply(FUN, ... , MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) ... FUN arguments to vectorize over (vectors or lists of strictly positive length, or all of zero length). MoreArgs passing a list of other arguments, that don‚Äôt need to vectorize, to FUN. SIMPLIFY logical or character string; attempt to reduce the result to a vector, matrix or higher dimensional array; For sapply it must be named and not abbreviated. The default value, TRUE, returns a vector or matrix if appropriate, whereas if SIMPLIFY = \"array\" the result may be an array of ‚Äúrank‚Äù (=length(dim(.))) one higher than the result of FUN(X[[i]]). USE.NAME logical; use names if the first ‚Ä¶ argument has names, or if it is a character vector, use that character vector as the names. mapply calls FUN for the values of ... (re-cycled to the length of the longest, unless any have length zero), followed by the arguments given in MoreArgs. The arguments in the call will be named if ... or MoreArgs are named. mapply(rep, times = 1:4, x = 4:1) # [[1]] # [1] 4 # # [[2]] # [1] 3 3 # # [[3]] # [1] 2 2 2 # # [[4]] # [1] 1 1 1 1 mapply(rep, times = 1:4, MoreArgs = list(x = 42)) # [[1]] # [1] 42 # # [[2]] # [1] 42 42 # # [[3]] # [1] 42 42 42 # # [[4]] # [1] 42 42 42 42 clusterMap(cl = NULL, fun, ..., MoreArgs = NULL, RECYCLE = TRUE, SIMPLIFY = FALSE, USE.NAMES = TRUE, .scheduling = c(\"static\", \"dynamic\")) is the parallel version for mapply. To iterate over more than one variable, clusterMap is very useful. Since you‚Äôre only iterating over int1 and int2, you should use the ‚ÄúMoreArgs‚Äù option to specify the variables that you aren‚Äôt iterating over. clusterMap returns the results in a list by default, you should be able to combine the results using do.call('rbind', result). cluster &lt;- makeCluster(detectCores()) clusterEvalQ(cluster, library(xts)) result &lt;- clusterMap(cluster, function1, int1=1:8, int2=c(1, rep(0, 7)), MoreArgs=list(df1=df1, df2=df2, char1=&quot;someString&quot;)) df &lt;- do.call(&#39;rbind&#39;, result) parApply(cl = NULL, X, MARGIN, FUN, ..., chunk.size = NULL) parApply is the parallel version of apply while clusterApply apply a function to a list of arguments. No vectorization is involved. clusterApply(cl = NULL, x, fun, ...) clusterApply calls fun on the first node with arguments x[[1]] and ‚Ä¶, on the second node with x[[2]] and ‚Ä¶, and so on, recycling nodes as needed. clusterApply only vectorize x. Example: &gt; clusterApply(cl, c(2:4), sum, 10) [[1]] [1] 12 [[2]] [1] 13 [[3]] [1] 14 5.5.1 Parallel version of apply function parallel::parLapply Performs the calculations in parallel, possibly on several nodes parLapply(cl = NULL, X, fun, ...) cl a cluster object X A vector (atomic or list) for parLapply and parSapply, an array for parApply. fun function or character string naming a function. ... additional arguments to pass to fun: beware of partial matching to earlier arguments. e.g.: parSapply(cl,var1,FUN=myfunction,var2=var2,var3=var3,var4=var4) Note that: Can use several types of communications, including PSOCK and MPI For parLapply, the worker processes must be prepared with any loaded packages with clusterEvalQ or clusterCall. For parLapply, large data sets can be exported to workers with clusterExport. Best practice: Test interactively with lapply serially, and mclapply or parLapply (PSOCK) in parallel ## Sample codes for using cluster library(parallel) # initialize a cluster cl &lt;- makeCluster(4, type=&#39;SOCK&#39;) clusterEvalQ(cl, library(raster)) cldCON &lt;- parSapply(cl, as.list(cldNC), simulation_mask, tgt=con_shape[shape_dict[con_name],]) # stop a cluster stopCluster(cl) mclapply(X, FUN, ...) is a parallelized version of lapply, it returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X. parallel::mcmapply(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE, mc.preschedule = TRUE, mc.set.seed = TRUE, mc.silent = FALSE, mc.cores = getOption(\"mc.cores\", 2L), mc.cleanup = TRUE, affinity.list = NULL) is a parallelized version of mapply. mc*apply takes an argument, mc.cores. By default, mc*apply will use all cores available to it. If you don‚Äôt want to (either becaues you‚Äôre on a shared system or you just want to save processing power for other purposes) you can set this to a value lower than the number of cores you have. Setting it to 1 disables parallel processing, and setting it higher than the number of available cores has no effect. stackApply(r, indices=num_years, fun=mean, na.rm=T, ...) Apply a function on subsets of a RasterStack or RasterBrick. The layers to be combined are indicated with the vector indices. The function used should return a single value, and the number of layers in the output Raster* equals the number of unique values in indices. For example, if you have a RasterStack with 6 layers, you can use indices=c(1,1,1,2,2,2) and fun=sum. This will return a RasterBrick with two layers. x Raster* object indices integer. Vector of length nlayers(x) num_yearss &lt;- rep(1:55, each=12) # 55 years r_year &lt;- stackApply(r, indices=num_years, fun=mean, na.rm=T) # A parallel version beginCluster(4) r_year &lt;- clusterR(r, stackApply, args=list(indices=num_years, fun=mean, na.rm=T)) endCluster() snow package for parallel computing snow::clusterMap(cl, fun, ..., MoreArgs=NULL, RECYCLE=TRUE) clusterMap is a multi-argument version of clusterApply, analogous to mapply. If RECYCLE is true shorter arguments are recycled; otherwise, the result length is the length of the shortest argument. Cluster nodes are recycled if the length of the result is greater than the number of nodes. clusterApply(cl, x, fun, ...) calls fun on the first cluster node with arguments seq[[1]] and ‚Ä¶, on the second node with seq[[2]] and ‚Ä¶, and so on. If the length of seq is greater than the number of nodes in the cluster then cluster nodes are recycled. A list of the results is returned; the length of the result list will equal the length of seq. clusterCall(cl, fun, ...) calls a function fun with identical arguments ... on each node in the cluster cl and returns a list of the results. clusterEvalQ(cl, expr) evaluates a literal expression on each cluster node. It a cluster version of evalq, and is a convenience function defined in terms of clusterCall. clusterExport(cl, list, envir = .GlobalEnv) assigns the values on the master of the variables named in list to variables of the same names in the global environments of each node. The environment on the master from which variables are exported defaults to the global environment. cl &lt;- makeSOCKcluster(c(&quot;localhost&quot;,&quot;localhost&quot;)) clusterApply(cl, 1:2, get(&quot;+&quot;), 3) clusterEvalQ(cl, library(boot)) # load packages needed x&lt;-1 clusterExport(cl, &quot;x&quot;) clusterCall(cl, function(y) x + y, 2) endCluster() 5.5.2 Related Functions lapply returns a list, we can use unlist() to convert the list into a vector. do.call(what, args) useful in assembling lists. It applies a given function to the list as a whole, there is only one function call. what, gives a function to call. Common options could be 'c', rbind or cbind. either a function or a non-empty character string naming the function to be called. args, is a list of arguments to pass to fun, and so do.call(\"f\", list(x, y, z)) is equivalent to f(x, y, z). args are passed by through named lists. A good friend with apply and by, use do.call(rbind, apply(...)) to form a list result to a data frame. x1 &lt;- c(1:10, NA) # Example vector with NA x1 # Print example vector # 1 2 3 4 5 6 7 8 9 10 NA do.call(&quot;sum&quot;, list(x1)) # Basic application of do.call # NA do.call(&quot;sum&quot;, list(x1, na.rm = TRUE)) # Specify additional arguments # 55 You can use pipe operator list(x1) %&gt;% do.call(&quot;sum&quot;, .) But using the new base/native pipe (|&gt;) leads to errors performing the same operation: myList |&gt; do.call(&quot;rbind&quot;, .) #&gt; Error in do.call(myList, &quot;rbind&quot;, .) : #&gt; second argument must be a list The error happens because |&gt; always inserts into the first argument and does NOT support dot. A workaround is to use named arguements: myList |&gt; do.call(what = &quot;rbind&quot;) "],["5.6-statistical-summary-functions.html", "5.6 Statistical Summary Functions", " 5.6 Statistical Summary Functions Functions for calculating descriptive statistics and summaries. Summary statistics for box plots boxplot.stats returns the extreme of the lower whisker, the lower ‚Äòhinge‚Äô (Q1 or the 1st quartile), the median (or midhinge), the upper ‚Äòhinge‚Äô (Q3 or the 3rd quartile) and the extreme of the upper whisker. lower whisker is the smallest data point within 1.5 √ó IQR from Q1 The larger of min and Q1‚Äì1.5*IQR, IQR=Q3-Q1 is the interquartile range. upper whisker is the the largest data point within 1.5 √ó IQR from Q3 The smaller of max and Q3+1.5*IQR box.stat &lt;- function(col){ # Return a vector of statistics of 6 elements, containing # 1) the extreme of the lower whisker, # 2) the lower &#39;hinge&#39;, 3) the median, 4)the upper &#39;hinge&#39;, # 5) the extreme of the upper whisker, # and 6) the mean. setNames(c(boxplot.stats(col)$stats, mean(unlist(col), na.rm=TRUE) ), c(&quot;low.whisker&quot;, &quot;1st.Q&quot;, &quot;median&quot;, &quot;3rd.Q&quot;, &quot;upper.whisker&quot;, &quot;mean&quot;) ) } Get boxplot and its stats # boxplot of price groupped by make boxplot(price~make, sample_price2)$stats %&gt;% as_tibble() %&gt;% setNames(c(&quot;bmw&quot;,&quot;toyota&quot;)) %&gt;% add_column(&quot;stat&quot;=c(&quot;low.whisker&quot;, &quot;1st.Q&quot;, &quot;median&quot;, &quot;3rd.Q&quot;, &quot;upper.whisker&quot;), .before=1) Summary statistics for box plots boxplot.stats returns the extreme of the lower whisker, the lower ‚Äòhinge‚Äô (Q1 or the 1st quartile), the median (or midhinge), the upper ‚Äòhinge‚Äô (Q3 or the 3rd quartile) and the extreme of the upper whisker. lower whisker is the smallest data point within 1.5 √ó IQR from Q1 The larger of min and Q1‚Äì1.5*IQR, IQR=Q3-Q1 is the interquartile range. upper whisker is the the largest data point within 1.5 √ó IQR from Q3 The smaller of max and Q3+1.5*IQR box.stat &lt;- function(col){ # Return a vector of statistics of 6 elements, containing # 1) the extreme of the lower whisker, # 2) the lower ‚Äòhinge‚Äô, 3) the median, 4)the upper ‚Äòhinge‚Äô, # 5) the extreme of the upper whisker, # and 6) the mean. setNames(c(boxplot.stats(col)$stats, mean(unlist(col), na.rm=TRUE) ), c(&quot;low.whisker&quot;, &quot;1st.Q&quot;, &quot;median&quot;, &quot;3rd.Q&quot;, &quot;upper.whisker&quot;, &quot;mean&quot;) ) } Get boxplot and its stats # boxplot of price groupped by make boxplot(price~make, sample_price2)$stats %&gt;% as_tibble() %&gt;% setNames(c(&quot;bmw&quot;,&quot;toyota&quot;)) %&gt;% add_column(&quot;stat&quot;=c(&quot;low.whisker&quot;, &quot;1st.Q&quot;, &quot;median&quot;, &quot;3rd.Q&quot;, &quot;upper.whisker&quot;), .before=1) setNames() updates the column names without having to write another replacement function. "],["5.7-contingency-table.html", "5.7 Contingency Table", " 5.7 Contingency Table Confusion tables / contingency tables / crosstabs for analyzing categorical data relationships. 5.7.1 Basic Frequency Counting The dplyr::count() Function dplyr::count(x, vars=NULL, wt_var=NULL) lets you quickly count the freqency of unique values of one or more variables. Returns a data.frame. x data frame to be processed vars variable(s) to count unique values of. If it is one variable, then it returns a frequency table. It there are two variables, then it returns the count for each possible combination of categories of the two variables. Return a data.frame with 3 columns. First 2 columns (named after the 2 variables) specify combinations, the third column (Freq) shows the frequency. wt_var optional variable to weight by - if this is non-NULL, count will sum up the value of this variable for each combination of id variables. df %&gt;% count(a, b) is roughly equivalent to df %&gt;% group_by(a, b) %&gt;% summarise(n = n()). For each combination of (a,b), count the frequency. # Count of each value of &quot;id&quot; in the first 100 cases count(baseball[1:100,], vars = &quot;id&quot;) # Count of ids, weighted by their &quot;g&quot; loading count(baseball[1:100,], vars = &quot;id&quot;, wt_var = &quot;g&quot;) # exercises is a dummay variable in `her.no` count(hers.no, exercise) # exercise n # 1 0 1191 # 2 1 841 dplyr::tally() works similarly to count, but you need to do group_by first manually. One step more than count. df %&gt;% group_by(a,b) %&gt;% tally() is equivalent to df %&gt;% group_by(a,b) %&gt;% summarise(n = n()). The dplyr::tally() Function dplyr::tally() works similarly to count, but you need to do group_by first manually. One step more than count. 5.7.2 Creating Contingency Tables The table() Function Frequency table if providing one variable Cross tabulation table with proportion if providing multiple variables # contingency table between chd69 and smoke with(contingency_data, table(chd69, smoke)) A relative frequency table can be produced using the function prop.table(x, margin=NULL), which takes a table object as argument: margin: 1 indicates rows, 2 indicates columns. with(contingency_data, table(chd69, smoke)) %&gt;% prop.table() Table tutorial: https://cran.r-project.org/web/packages/DescTools/vignettes/TablesInR.pdf table(..., exclude = if (useNA == \"no\") c(NA, NaN), useNA = c(\"no\", \"ifany\", \"always\"), dnn = list.names(...), deparse.level = 1) table() is more flexible than count(). table() accepts vectors, lists, data.frames, but count() accepts data.frames only. ... one or more objects which can be interpreted as factors (including character strings), or a list (or data frame) whose components can be so interpreted. E.x codes: set.seed(1) tt &lt;- sample(letters, 100, rep=TRUE) ## using table table(tt) tt a b c d e f g h i j k l m n o p q r s t u v w x y z 2 3 3 3 2 4 6 1 6 5 6 4 7 2 2 2 5 4 5 3 8 4 5 4 3 1 ## using tapply tapply(tt, tt, length) a b c d e f g h i j k l m n o p q r s t u v w x y z 2 3 3 3 2 4 6 1 6 5 6 4 7 2 2 2 5 4 5 3 8 4 5 4 3 1 Another example: breaks &lt;- seq(1, 3.5, by=0.5) labels &lt;- seq(1.25, by=0.5, length.out=length(breaks)-1) cut(x$TCS_reported, breaks=breaks, labels=labels) %&gt;% table() # . # 1.25 1.75 2.25 2.75 3.25 # 3 10 4 5 0 apply(x, 2, function(col) cut(col, breaks=breaks, labels=labels) %&gt;% table()) # TCS_reported TCS_global_cvt # 1.25 3 2 # 1.75 10 5 # 2.25 4 11 # 2.75 5 2 # 3.25 0 2 The xtabs() Function xtabs(formula = ~., data) Create a contingency table from cross-classfifying factors. We now create a cross-tabulated table to see how occurrences break down across age and gender. Notice we use the cut() function to quickly create 4 arbitrary age groups containing equal numbers of people. &gt; xtabs(~ cut(age,4) + gender + y) , , y = 0 gender cut(age, 4) 0 1 (18.9,34] 15 10 (34,49] 11 1 (49,64] 1 0 (64,79.1] 0 0 , , y = 1 gender cut(age, 4) 0 1 (18.9,34] 2 4 (34,49] 4 8 (49,64] 5 11 (64,79.1] 11 17 5.7.3 Working with Arrays and Multi-dimensional Tables The returned table works like an array. Array Creation and Manipulation # Define an empty array a &lt;- array(numeric(), c(2,3,0)) &gt; a &lt;2 x 3 x 0 array of double&gt; [,1] [,2] [,3] [1,] [2,] Note that you need to set at least one dimension to zero, otherwise the array will contain something by definition. dim can be an integer (will coerce to a vector), or a vector giving the maximal indices in each dimension. array(1:24, dim=c(2,3,4)) # equivalently, array(1:24, dim=2:4) aperm(a, perm) Transpose an array by permuting its dimensions. perm the subscript permutation vector, usually a permutation of the integers 1:n, where n is the number of dimensions of a. # initialize a 3D arry 2x3x2 &gt; x &lt;- array(1:12, dim = c(2,3,2)) &gt; x , , 1 [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6 , , 2 [,1] [,2] [,3] [1,] 7 9 11 [2,] 8 10 12 # array transpose, from 2x3 to 3x2 &gt; aperm(x, c(2,1,3)) , , 1 [,1] [,2] [1,] 1 2 [2,] 3 4 [3,] 5 6 , , 2 [,1] [,2] [1,] 7 8 [2,] 9 10 [3,] 11 12 Add data to the array using abind. abind works like rbind/cbind but in a generalized way. So, as rbind/cbind add a 1-dimensional structure to a 2-dimensional one, using abind with a 3-dimensional array. abind(..., along=N) Combine multi-dimensional arrays. ... Any number of vectors, matrices, arrays, or data frames. along=N (optional) The dimension along which to bind the arrays. The default is the last dimension. # Append a matrix a &lt;- abind(a, matrix(5,nrow=2,ncol=3), along=3) &gt; a , , 1 [,1] [,2] [,3] [1,] 5 5 5 [2,] 5 5 5 # Append a matrix again a &lt;- abind(a, matrix(7,nrow=2,ncol=3), along=3) &gt; a , , 1 [,1] [,2] [,3] [1,] 5 5 5 [2,] 5 5 5 , , 2 [,1] [,2] [,3] [1,] 7 7 7 [2,] 7 7 7 5.7.4 Flattening and Exporting Tables Creating Flat Tables with ftable() Save ftable() output to csv Save to local and you‚Äôll be able to read the data afterwards. Higher dimensional tables can be ‚Äúfalttened‚Äù into one table using ftable. The resulting three-way table shows the frequencies of all three variables in a ‚Äúflat‚Äù format. #view three-way table three_way , , starter = No position team F G A 1 2 B 1 1 , , starter = Yes position team F G A 1 1 B 2 1 #convert table to ftable three_way_ftable &lt;- ftable(three_way) #view ftable three_way_ftable starter No Yes team position A F 1 1 G 2 1 B F 1 2 G 1 1 ftable(x) Create ‚Äòflat‚Äô contingency tables. Condense into 2-dimension. Hard to read, but easy to save. 3-D array is difficult to save on the other hand. x R objects which can be interpreted as factors (including character strings), or a list (or data frame) whose components can be so interpreted, or a contingency table object of class \"table\" or \"ftable\". Use stats to first format ftable and then use write.table. # `confusion_matrix_all` is an 3D array: 2x2x7 df &lt;- ftable(confusion_matrix_all) # quote=FALSE makes the table more readable cont &lt;- stats:::format.ftable(df, method = &quot;col.compact&quot;, quote = FALSE) write.table(cont, sep = &quot;,&quot;, file = &quot;table.csv&quot;) # disable row and column names write.table(cont, sep = &quot;,&quot;, file = &quot;table.csv&quot;, row.names = FALSE, col.names = FALSE) Load confusion table # read as a regular table confusion_ftable &lt;- read.table(f_name, sep = &quot;,&quot;, skip = 2) # change to array, check if need to transpose confusion_ftable &lt;- array(confusion_ftable[,3:12] %&gt;% unlist(), dim = c(2,2,10)) %&gt;% aperm(c(2,1,3)) # specify dimension names dimnames(confusion_ftable) &lt;- list(rf.class.test = c(0, 1), obs.test = c(0,1), Group = paste0(&quot;G&quot;,1:10)) ftable examples &gt; Pet &lt;- c(&quot;Cat&quot;,&quot;Dog&quot;,&quot;Cat&quot;,&quot;Dog&quot;,&quot;Cat&quot;,&quot;Fish&quot;) &gt; Food &lt;- c(&quot;F1&quot;,&quot;F3&quot;,&quot;F2&quot;,&quot;F4&quot;,&quot;F2&quot;,&quot;F4&quot;) &gt; Sex &lt;- c(&quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;) &gt; Color &lt;- c(&quot;Black&quot;, &quot;White&quot;, &quot;Yellow&quot;, &quot;NA&quot;, &quot;White&quot;, &quot;Black&quot; ) &gt; ft &lt;- ftable(Pet, Food, Sex, Color) &gt; ft Color Black NA White Yellow Pet Food Sex Cat F1 F 0 0 0 0 M 1 0 0 0 F2 F 0 0 1 1 M 0 0 0 0 F3 F 0 0 0 0 M 0 0 0 0 F4 F 0 0 0 0 M 0 0 0 0 Dog F1 F 0 0 0 0 M 0 0 0 0 F2 F 0 0 0 0 M 0 0 0 0 F3 F 0 0 0 0 M 0 0 1 0 F4 F 0 0 0 0 M 0 1 0 0 Fish F1 F 0 0 0 0 M 0 0 0 0 F2 F 0 0 0 0 M 0 0 0 0 F3 F 0 0 0 0 M 0 0 0 0 F4 F 1 0 0 0 M 0 0 0 0 &gt; ft3 &lt;- ftable(ft, row.vars = &quot;Food&quot;, col.vars = c(&quot;Sex&quot;, &quot;Pet&quot;)) &gt; ft3 Sex F M Pet Cat Dog Fish Cat Dog Fish Food F1 0 0 0 1 0 0 F2 2 0 0 0 0 0 F3 0 0 0 0 1 0 F4 0 0 1 0 1 0 &gt; as.table(ft3) , , Pet = Cat Sex Food F M F1 0 1 F2 2 0 F3 0 0 F4 0 0 , , Pet = Dog Sex Food F M F1 0 0 F2 0 0 F3 0 1 F4 0 1 , , Pet = Fish Sex Food F M F1 0 0 F2 0 0 F3 0 0 F4 1 0 write.ftable has three formats: row.compact, col.compact, and compact. &gt; ft22 Survived No Yes Age Child Adult Child Adult Sex Class Male 1st 0 118 5 57 2nd 0 154 11 14 3rd 35 387 13 75 Crew 0 670 0 192 Female 1st 0 4 1 140 2nd 0 13 13 80 3rd 17 89 14 76 Crew 0 3 0 20 ------------------------------------------------------------- &gt; write.ftable(ft22, quote = FALSE, method=&quot;row.compact&quot;) Survived No Yes Sex Class Age Child Adult Child Adult Male 1st 0 118 5 57 2nd 0 154 11 14 3rd 35 387 13 75 Crew 0 670 0 192 Female 1st 0 4 1 140 2nd 0 13 13 80 3rd 17 89 14 76 Crew 0 3 0 20 ------------------------------------------------------------- &gt; write.ftable(ft22, quote = FALSE, method=&quot;col.compact&quot;) Survived No Yes Age Child Adult Child Adult Sex Class Male 1st 0 118 5 57 2nd 0 154 11 14 3rd 35 387 13 75 Crew 0 670 0 192 Female 1st 0 4 1 140 2nd 0 13 13 80 3rd 17 89 14 76 Crew 0 3 0 20 ------------------------------------------------------------- &gt; write.ftable(ft22, quote = FALSE, method=&quot;compact&quot;) Survived No Yes Sex Class | Age Child Adult Child Adult Male 1st 0 118 5 57 2nd 0 154 11 14 3rd 35 387 13 75 Crew 0 670 0 192 5.7.5 Table Visualization and Formatting Professional Table Display with flextable Print crosstabs using flextable Good for visualization because they have good typesettings. But you won‚Äôt be able to read the data easily as they are buried in tablel aesthetics. Observation Prediction 0 1 0 3108 531 1 35 49 flextable::as_flextable will print the table in the Viewer pane. # might need to load `officer` pkg # library(officer) library(flextable) confusion_matrix %&gt;% as_flextable() %&gt;% set_caption( as_paragraph( as_chunk(&quot;Year: 2013&quot;, # specify caption text props = fp_text(bold = TRUE, # bold face font.family = &quot;Helvetica&quot; # font family ) ) ) ) More about flextable: https://blog.djnavarro.net/posts/2024-07-04_flextable/ Excel Export with xltabr Write crosstabs into Excel devtools::install_github(&quot;moj-analytical-services/xltabr&quot;) library(xltabr) titles = c(&quot;Breakdown of car statistics&quot;, &quot;Cross tabulation of drive and age against type*&quot;) footers = &quot;*age as of January 2015&quot; wb &lt;- xltabr::auto_crosstab_to_wb(ct, titles = titles, footers = footers) openxlsx::openXL(wb) Given a crosstabulation ct produced by reshape2:dcast, the following table is generated. "],["5.8-data-frame-operations.html", "5.8 Data Frame Operations", " 5.8 Data Frame Operations Basic operations: colSums(x) returns sum for each column rowSums(x) returns sum for each row setNames() updates the column names without having to write another replacement function. magrittr package provides a series of aliases which can be more pleasant to use when composing chains using the %&gt;%operator. Alias Cmd set_colnames colnames&lt;- set_rownames rownames&lt;- set_names names&lt;- subset(x, subset, select, drop = FALSE, ‚Ä¶) Subsetting Vectors, Matrices And Data Frames x object to be subsetted. subset logical expression indicating elements or rows to keep: missing values are taken as false. select columns to select from a data frame. subset(airquality, Temp &gt; 80, select = c(Ozone, Temp)) subset(airquality, Day == 1, select = -Temp) subset(airquality, select = Ozone:Wind) with(airquality, subset(Ozone, Temp &gt; 80)) Column/Row-wise Operations sweep(data, MARGIN, STATS, FUN='-') Return an array obtained from an input array by sweeping out a summary statistic; useful in standardizing data, eg., center or scale columns. data: matrix/dataframe MARGIN: 1 row-wise, 2 column-wise.. STATS: a vector with the same length as row \\[MARGIN=1\\]/ column \\[MARGIN=2\\]. FUN: the function to be used to carry out the sweep. Example: sweep(z, 2, colMeans(z),'-') substract column mean from each column; Alternatively, can use mutate_at. You can perform the operation on selected cols without changing the data structure. By contrast, with sweep, you need to subset the relevant cols, apply the operation, and concatenate them back to the remainder of the columns afterwards. Takeaway: Use sweep if the operation is to all cols; use mutate_at if to selected cols. center_col &lt;- function(data, cols){ ## Mean center columns in a table # @data: table or data frame # @col: a vector of selected columns to center # @return A data frame with the selected columns mean-centered # (i.e., each value minus its column mean). data %&gt;% mutate_at(cols, ~.-mean(., na.rm=TRUE)) } Remove duplicate columns, regardless of column names: # remove duplicate columns, regardless column names df[!duplicated(lapply(df, summary))] 5.8.1 Process NA values NaN not a number. 0/0 is an example of a calculation that will produce a NaN. NaNs print as NaN, but generally act like NAs. Use is.nan to check if NaN. Inf is infinite numbers. is.infinite find infinite numbers (Inf, -Inf). na.omit(x) remove NA values in x; x could be vectors, matrices, and data frames; if x is a data frame, remove rows with NA values; Find NA values complete.cases(x) return a logical vector indicating which cases/rows are complete. # count number of rows with NA values data %&gt;% negate(complete.cases)() %&gt;% sum() purrr::negate() works similar to base::Negate() R is case-sensitive. (!is.na(x)) %&gt;% colSums() %&gt;% sort() returns a vector of the number of non-NA values per column, column names as vector name. When you have a long list of columns, vector is hard to read, use as_tibble_row() %&gt;% t() to convert to a tibble column. # when you have a long list of columns, you can convert to a tibble column for best visualization. miss_per_col &lt;- data %&gt;% filter(date &gt; ymd(&quot;2013-01-01&quot;)) %&gt;% is.na() %&gt;% colSums() miss_per_col &lt;- miss_per_col %&gt;% as_tibble_row() %&gt;% t() miss_per_col %&gt;% dim() miss_per_col[1:40, ] %&gt;% t() %&gt;% t() miss_per_col[41:77, ] %&gt;% t() %&gt;% t() (!is.na(x)) %&gt;% rowSums() %&gt;% sort() calculate the number of non-NA values per row tidyr::drop_na(x, any_of(vars)) allow you to specify which columns you want to eliminate NA values from; it doesn‚Äôt have to be the whole columns; x must be a data frame. which(is.na(data)) returns positions of omitted missing values A custom function for handling common missing/invalid values: cast_na &lt;- function(x){ # Remove records with nonvalid values, such as NA, Inf, NaN # remain x when mask==TRUE if (is.null(dim(x))){ # `x` is a vector mask &lt;- !(x %in% c(NA, NaN, Inf, -Inf) ) x &lt;- x[mask] } else { # `x` is a data.frame mask &lt;- apply(x, 1, function(x) sum(x %in% c(NA, NaN, Inf, -Inf))) mask &lt;- (mask == 0) x &lt;- x[mask,]} return (x) } Keep rows with least NA‚Äôs for duplicated rows User_Table %&gt;% arrange(rowSums(is.na(.))) %&gt;% # sort rows by number of NAs distinct(User_ID, .keep_all = TRUE) # keep first row per User_ID only Fill missing values Forward/Backward filling tidyr::fill(data, ..., .direction = c(\"down\", \"up\", \"downup\", \"updown\")) Fill missing values in selected columns using the next or previous entry. This is useful in the common output format where values are not repeated, and are only recorded when they change. ... Columns to fill. .direction Direction in which to fill missing values. Default: ‚Äúdown‚Äù. Alternatively, you can use zoo::na.locf # Last obs. carried forward na.locf(x) # Next obs. carried backward na.locf(x, fromLast = TRUE) fromLast defaults to FALSE, carry forward. If set to TRUE, carry backward. Replace with specific values dplyr::na_if(x, y) that replaces any values in x that are equal to y with NA. It is useful if you want to convert an annoying value to NA. &gt; na_if(1:5, 5:1) # [1] 1 2 NA 4 5 &gt; y &lt;- c(&quot;abc&quot;, &quot;def&quot;, &quot;&quot;, &quot;ghi&quot;) &gt; na_if(y, &quot;&quot;) # [1] &quot;abc&quot; &quot;def&quot; NA &quot;ghi&quot; tidyr::replace_na(data, replace) Replace NAs with specified values replace If data is a data frame, replace takes a list of values, with one value for each column that has NA values to be replaced. # Replace NAs in a data frame df &lt;- tibble(x = c(1, 2, NA), y = c(&quot;a&quot;, NA, &quot;b&quot;)) df %&gt;% replace_na(list(x = 0, y = &quot;unknown&quot;)) If data is a vector, replace takes a single value. This single value replaces all of the NA values in the vector. # Replace NAs in a vector df %&gt;% dplyr::mutate(x = replace_na(x, 0)) # OR df$x %&gt;% replace_na(0) df$y %&gt;% replace_na(&quot;unknown&quot;) "],["5.9-matrix-multiplication.html", "5.9 Matrix multiplication", " 5.9 Matrix multiplication Dot product (element to element multiplication, then sum the result) sum(x*y) Matrix product or inner product of vectors X %*% Y is \\(X^TY\\). crossprod(x, y) is equivalent to t(x) %*% y, or tcrossprod(x, y) same as x %*% t(y). Vector outer product %o% or outer(X, Y, FUN = \"*\") outer product is \\(XY^T\\), can be calculated as as.vector(X) %*% t(as.vector(Y)) outer(x, y, FUN = \"*\") FUN can be other operations, such as +. Kronecker product %x% New Empty Matrix # initialize an empty matrix prediction_df &lt;- matrix(ncol=7, nrow=0) for (i in 1:10){ the_prediction &lt;- ... prediction_df &lt;- rbind(prediction_df, the_prediction) } varImp_df &lt;- matrix(ncol=0, nrow=4) for (i in 1:10){ varImp &lt;- ... varImp_df &lt;- bind_cols(varImp_df, varImp) # or cbind(varImp_df, varImp), bind_cols and cbind bind by positions # if need to match by namesm use left_join(varImp_df, varImp, by=&quot;name&quot;) } "],["5.10-string-operations.html", "5.10 String Operations", " 5.10 String Operations stringr::str_starts(string, pattern) str_detect(string, pattern) returns a logical vector with TRUE for each element of string that matches pattern and FALSE otherwise. str_locate(string, pattern) returns the start and end position of the first match; # match a pattern and select data %&gt;% mutate(end = str_locate(key,&quot;_sd&quot;)[1], name = str_sub(key, 1, end-1)) str_locate_all(string, pattern) returns the start and end position of each match. str_sub(string, start = 1L, end = -1L) Extract substrings from a character vector. Negative indices index from end of string. # replace substring with specific value str_sub(string, start = 1L, end = -1L, omit_na = FALSE) &lt;- value # A pair of integer vectors defining the range of characters to extract hw &lt;- &quot;Hadley Wickham&quot; str_sub(hw, start=c(1, 8), end=c(6, 14)) # select 1-6 and 8-14 #&gt; [1] &quot;Hadley&quot; &quot;Wickham&quot; base::substr(name, start, end) base::substring(name, start, end=1000000L) if not providing end, will subset from start until the end of the string. base:: is to specify using base R function. "],["6-tidyverse.html", "Chapter 6 Tidyverse", " Chapter 6 Tidyverse tidyverse is a collection of packages for data analyses. This package is designed to make it easy to install and load multiple tidyverse packages in a single step. The following packages are included in the core tidyverse: ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats, lubridate. The tidyverse also includes many other packages with more specialized usage. They are not loaded automatically with library(tidyverse), so you‚Äôll need to load each one with its own call to library(). tibble Package Create a tibble, just the same way as data.fram, only that without row names. tibble(x = 1:5, y = 1, z = x ^ 2 + y) tibble() does much less than data.frame(): it never changes the type of the inputs (e.g.¬†it never converts strings to factors!), it never changes the names of variables, it only recycles inputs of length 1, and it never creates row.names(). as_tibble() vs tibble(): as_tibble() turns an existing object, such as a data frame or matrix, into a so-called tibble, a data frame with class tbl_df. This is in contrast with tibble(), which builds a tibble from individual columns. If using tibble() on a whole data frame, it would generate a one column tibble in which the column contains the data frame. tibble columns are versatile, can be lists, matrices, tibbles, etc. tibble( a = list( c = &quot;three&quot;, d = list(4:5) ) ) #&gt; # A tibble: 2 √ó 1 #&gt; a #&gt; &lt;named list&gt; #&gt; 1 &lt;chr [1]&gt; #&gt; 2 &lt;list [1]&gt; Print tibbles tbl_df %&gt;% print(n = Inf) print all rows. print.tbl_df is useful in terms of explicitly and setting arguments like n and width. n print the first n rows. When n=Inf, it means to print all rows. width Width of text output to generate. This defaults to NULL, which means use the width in options(). When width=Inf, will print all columns. Use ?print.tbl_df to show help page. Alternatively, use, tbl_df %&gt;% data.frame() to print the whole table. data.frame won‚Äôt round numbers. Usually tbl round at the 6-th digit after the decimal point. print(as_tibble(mtcars), n = 3) first convert to tibble, then specify the rows to print. data.table package has nice table print settings. You can preview the head and tail at the same time. It doesn‚Äôt give you column details, such as data type, but it gives you a feeling of the data structure without using head and tail functions twice. The data.table R package is being used in different fields such as finance and genomics and is especially useful for those of you that are working with large data sets (for example, 1GB to 100GB in RAM). data.table Cheatsheet: https://www.datacamp.com/cheat-sheet/the-datatable-r-package-cheat-sheet Data Frame and Vector Conversion reframe can return an arbitrary number of rows per group, while summarise()reduces each group down to a single row and mutate returns the same number of rows as the input. reframe() always returns an ungrouped data frame. reframe() is theoretically connected to two functions in tibble, tibble::enframe()and tibble::deframe(): enframe(): vector ‚Üí data frame deframe(): data frame ‚Üí vector reframe(): data frame ‚Üí data frame, with arbitrary number of rows per group. enframe and deframe convert vectors to tibbles and vice verse. Example Usage: enframe(1:3) #&gt; # A tibble: 3 √ó 2 #&gt; name value #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 2 2 #&gt; 3 3 3 enframe(c(a = 5, b = 7)) #&gt; # A tibble: 2 √ó 2 #&gt; name value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 5 #&gt; 2 b 7 enframe(list(one = 1, two = 2:3, three = 4:6)) #&gt; # A tibble: 3 √ó 2 #&gt; name value #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 one &lt;dbl [1]&gt; #&gt; 2 two &lt;int [2]&gt; #&gt; 3 three &lt;int [3]&gt; deframe(enframe(3:1)) #&gt; 1 2 3 #&gt; 3 2 1 deframe(tibble(a = 1:3)) #&gt; [1] 1 2 3 deframe(tibble(a = as.list(1:3))) #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 2 #&gt; #&gt; [[3]] #&gt; [1] 3 Concatenate list elements into a table Use magrittr‚Äôs pipe operator myList %&gt;% do.call(&quot;rbind&quot;, .) But using the new base/native pipe (|&gt;) leads to errors performing the same operation: myList |&gt; do.call(&quot;rbind&quot;, .) #&gt; Error in do.call(myList, &quot;rbind&quot;, .) : #&gt; second argument must be a list The error happens because |&gt; always inserts into the first argument and does NOT support dot. A workaround is to use named arguements: myList |&gt; do.call(what = &quot;rbind&quot;) Use bind_rows from dplyr or rbindlist from data.table: library(dplyr) myList |&gt; bind_rows() library(data.table) myList |&gt; rbindlist() Use reduce from purrr library(purrr) myList |&gt; reduce(rbind) one row/column tibble as_tibble_row(x) and as_tibble_col(x, column_name=\"value\") convert a vector to one row or one column tibble; from vetor to tibble. as_tibble(data, rownames=\"new_col_name\") convert (df) to tibble. Flexible with the format of the input data, can be a range of classes. data A data frame, list, matrix, or other object that could reasonably be coerced to a tibble. rownames the name of a new column. Existing rownames are transferred into this column. If NULL then remove the rowname column. rownames_to_column(.data, var = \"new colname\") and column_to_rownames(.data, var = \"col to use as rownames\") using one column as row names, or converting row names to one column. .data needs to be a data frame; strict with input data type; var in rownames_to_column: new column name for original rownames in the data.frame, or in column_to_rownames: convert tibble to data frame, and specify which column to use as rownames. "],["6.1-basic-operations-on-tibbles.html", "6.1 Basic operations on tibbles", " 6.1 Basic operations on tibbles 6.1.1 Check Unique Values n_distinct(x) This is a faster and more concise equivalent of length(unique(x)), # of unique values in x. distinct(df, ..., .keep_all=FALSE) select distinct/unique rows, remove duplicate rows. df: table ...: variables to use when determining uniqueness. If omitted, will use all variables. If there are multiple rows for a given combination of inputs, only the first row will be preserved. If two variables are provided, then unique combinations of the two are used as key and the first row of each key is preserved. .keep_all: If TRUE, keep all variables in df # unique locations data %&gt;% distinct(lat, lon) dplyr::setdiff(x, y) element that is in x but not in y. x and y are supposed to have the same structure, i.e., same columns if for data frames. 6.1.2 Column Names rename() replaces an old name with a new one. world %&gt;% rename(name = name_long) # renames the lengthy name_long column to simply name magrittr package provides a series of aliases which can be more pleasant to use when composing chains using the %&gt;%operator. Alias Cmd set_colnames colnames&lt;- set_rownames rownames&lt;- set_names names&lt;- set_names() changes all column names at once, and requires a character vector with a name matching each column. 6.1.3 Column Operations Create New Columns add_column(df, ..., .after=NULL, .before=NULL) add new column after the last column. df: Data frame to append to; ...: Name-value pairs to insert; .before, .after: One-based column index or column name where to add the new columns, default: after last column data %&gt;% mutate(column = .[[2]] - .[[1]]) subset by column positions. Here the dot notation in .[[2]] refers to data, the variable you pipe into mutate. Dot is extra useful here because it allows you to use data multiple times. We use it twice in this example. Alternatively, data %&gt;% mutate(column = unlist(pick(2) - pick(1))) unlist here transform the list generated from pick(2)-pick(1) to a vector. Order by Columns arrange() reorders data frame based on specified columns. # sort mtcars by disp in ascending order arrange(mtcars, disp) # sort mtcars by disp in descending order arrange(mtcars, desc(disp)) arrange by column position # sort by first column in the data data %&gt;% arrange(.[1]) 6.1.4 mutate mutate() adds new columns at the penultimate (second last) position in the sf object (the last one is reserved for the geometry): world %&gt;% mutate(pop_dens = pop / area_km2) mutate_at(.tbl, .vars, .funs) applies a function to given columns: .tbl A tbl object; .vars A list of columns generated by vars(), a character vector of column names, a numeric vector of column positions, or NULL. .fun A function fun, a quosure style lambda ~ fun(.) or a list of either form. mtcars &lt;- mtcars %&gt;% mutate_at(c(&quot;hp_wt&quot;, &quot;mpg_wt&quot;), log) # note that `across` use together with `mutate` mtcars &lt;- mtcars %&gt;% mutate(across(c(&quot;hp_wt&quot;, &quot;mpg_wt&quot;), log)) ## factor as numeric, except for `ISO_C3` agg_dummy %&gt;% mutate_at(vars(-ISO_C3), ~as.numeric(levels(.)[.])) across(.cols, .fnc, .names=NULL) apply the same transformation to multiple columns, allowing you to use select() semantics inside in ‚Äúdata-masking‚Äù functions like summarise() and mutate(). .cols Columns to transform. You can NOT select grouping columns because they are already automatically handled by the verb. can specify start and end columns using :. .fnc Functions to apply to each of the selected columns. Possible values are: A function, e.g.¬†mean. A purrr-style lambda, e.g.¬†~ mean(.x, na.rm = TRUE) # divide each col by 100, except for the Date column. FF_factor %&gt;% mutate_at(vars(-Date), ~./100) A named list of functions or lambdas, e.g.¬†‚Å†list(mean = mean, n_miss = ~ sum(is.na(.x))‚Å†. Each function is applied to each column, and the output is named by combining the function name and the column name using the glue specification in .names. mutate() and mutate_at() outputs a new data frame, it does not alter the given data frame. We need to reassign the data frame to be the output of the pipe. mutate_all(.tbl, .funs, ...) is equivalent to apply(x, 2, FUN). But apply() works poor when column types are not unanimous. E.g., when a tibble has character and numeric columns, apply() tends to coerce numeric to character, and won‚Äôt return the result as you expext. 6.1.5 Concatenate rows into a tibble add_row(.data, ..., .before = NULL, .after = NULL) add one or more rows of data to an existing data frame, convenient in the way that you can just specify each column with their values. especially convenient when you just want to add one row. ... &lt;dynamic-dots&gt; Name-value pairs, passed on to tibble(). Values can be defined only for columns that already exist in .data and; unset columns will get an NA value. .before, .after specify the position where you want to add the new row/rows. # add_row --------------------------------- df &lt;- tibble(x = 1:3, y = 3:1) df %&gt;% add_row(x = 4, y = 0) bind_rows(..., .id = NULL) This is an efficient implementation of the common pattern of do.call(rbind, dfs). Match by column names. The output of bind_rows() will contain a column if that column appears in any of the inputs. rbind will throw errors if columns do not match. ... Data frames to combine. Each argument can either be a data frame, a list that could be a data frame, or a list of data frames. When row-binding, columns are matched by name, and any missing columns will be filled with NA. So no column is dropped, which is safe. When column-binding (bind_cols(df1, df2, ...)), rows are matched by position, so all data frames must have the same number of rows. To match by value, not position, see mutate-joins. Need to be careful when you use bind_cols, make sure rows are in the same order in the tables you want to join. bind_cols is equivalent to cbind: match by position. Recommend to use left_join, which is safer. bind_rows is safer than rbind: bind_rows find matched col names .id Data frame identifier. When .id is supplied, a new column of identifiers is created to link each row to its original data frame. The labels are taken from the named arguments to bind_rows(). When a list of data frames is supplied, the labels are taken from the names of the list. If no names are found a numeric sequence is used instead. res = NULL for (i in tibbleList) res = bind_rows(res, i) # or, equivalently, bind_rows(tibbleList) # combine all tibbles in the list 6.1.6 Data Subsetting pull(df, var) pull out a single variable and return a vector. Similar to $, but works well with %&gt;%. var: A variable specified as: a literal variable name a positive integer, giving the position counting from the left a negative integer, giving the position counting from the right. filter(.data, ..., .preserve=FALSE) chooses rows/cases where conditions are true. Unlike base subsetting with [, rows where the condition evaluates to NA are dropped. ... &lt;data-masking&gt; Expressions that return a logical value, and are defined in terms of the variables in .data. If multiple expressions are included, they are combined with the &amp; operator. Only rows for which all conditions evaluate to TRUE are kept. .preserve Relevant when the .data input is grouped. If .preserve = FALSE (the default), the grouping structure is recalculated based on the resulting data, otherwise the grouping is kept as is. # Countries with a life expectancy longer than 82 years world6 = filter(world, lifeExp &gt; 82) # filter based on vector filter(diamonds, cut %in% c(&#39;Ideal&#39;, &#39;Premium&#39;)) Useful filter functions: ==, &gt;, &gt;= etc &amp;, |, !, xor() is.na() between(), near() between(x, left, right) is a shortcut for x &gt;= left &amp; x &lt;= right. Ex. x[between(x, -1, 1)] select() subsets by column, slice subsets by rows. select() selects columns by name or position. world1 = dplyr::select(world, name_long, pop) names(world1) #&gt; [1] &quot;name_long&quot; &quot;pop&quot; &quot;geom&quot; select() also allows subsetting of a range of columns with the help of the : operator: # all columns between name_long and pop (inclusive) world2 = dplyr::select(world, name_long:pop) select() can be used to reorder/drop variables. select(df, year, var, state) # reverses the columns select(df, -state) # drop column by name &#39;state&#39; If you only know you want var in the front and don‚Äôt care about the order of the rest, you can do [move one variable in the front] df %&gt;% select(var, everything()) all_of(vars) is used together with select for strict selection. If any of the variables in the character vector is missing, an error is thrown. vars A vector of character names or numeric locations. any_of(vars) doesn‚Äôt check for missing variables. It is especially useful with negative selections, when you would like to make sure a variable is removed. # select columns may or may not exist the_country %&gt;% select(any_of(c(&quot;isoa2&quot;, &quot;countryCode&quot;)) ) # remove columns may or may not exist the_country %&gt;% select(-any_of(c(&quot;isoa2&quot;, &quot;countryCode&quot;)) ) This flexibility is useful because it won‚Äôt return an error if the variable is not found. Selection with conditions Functions work together with select to choose cols matching certain conditions: starts_with(), ends_with(), contains(). These are selection helpers which match variables according to a given pattern. &gt; iris %&gt;% select(starts_with(&quot;Sepal&quot;)) #&gt; # A tibble: 150 x 2 #&gt; Sepal.Length Sepal.Width #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 5.1 3.5 #&gt; 2 4.9 3 #&gt; 3 4.7 3.2 #&gt; 4 4.6 3.1 #&gt; # i 146 more rows &gt; iris %&gt;% select(ends_with(&quot;Width&quot;)) #&gt; # A tibble: 150 x 2 #&gt; Sepal.Width Petal.Width #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3.5 0.2 #&gt; 2 3 0.2 #&gt; 3 3.2 0.2 #&gt; 4 3.1 0.2 #&gt; # i 146 more rows 6.1.7 Indirection Indirection is when you want to get the data-variable from an env-variable instead of directly typing the data-variable‚Äôs name. There are two main cases: When you have the data-variable in a function argument, you need to¬†embrace¬†the argument by surrounding it in doubled¬†braces, like¬†filter(df, {{ var }}). var_summary &lt;- function(data, var) { data %&gt;% summarise(n = n(), min = min({{ var }}), max = max({{ var }})) } mtcars %&gt;% group_by(cyl) %&gt;% var_summary(mpg) When you have an env-variable that is a character vector, you need to index into the¬†.data¬†pronoun with¬†[[. for (var in names(mtcars)) { mtcars %&gt;% count(.data[[var]]) %&gt;% print() } Note that¬†.data¬†is not a data frame; it‚Äôs a special construct, a pronoun, that allows you to access the current variables either directly, with¬†.data$x¬†or indirectly with.data[[var]]. ref: Programming with dplyr: Indirection 6.1.8 Dynamic Selection A dynamic subset of variables when using select dynamic_var &lt;- &#39;state&#39; df %&gt;% select(year, var, eval(dynamic_var)) # dynamic_var will be parsed as state # --- dplyr version 0.7+--- multipetal &lt;- function(df, n) { varname &lt;- paste(&quot;petal&quot;, n , sep=&quot;.&quot;) mutate(df, !!varname := Petal.Width * n) } # !! unquote # using := to dynamically assign/change parameter names variables in the left hand side plot_v &lt;- &quot;maize&quot; plot_data &lt;- plot_data %&gt;% select(year, plot_v, tmx, pre, rad) %&gt;% mutate(!!plot_v := log(eval(parse(text = plot_v)) )) %&gt;% group_by(year) %&gt;% summarise(across(everything(), ~mean(.x, na.rm = TRUE) ) ) plot_data Use !! to unquote a single argument in a function call. !! takes a single expression, evaluates it, and inlines the result in the AST. x &lt;- expr(-1) expr(f(!!x, y)) #&gt; f(-1, y) rlang::sym(x) take a string as input and turn it into symbols string x -&gt; expression expr(x) -&gt; evaluate!!expr(x) !!! the behaviour of !!! is known as ‚Äúspatting‚Äù in Ruby, Go, PHP, and Julia. It is closely related to *args (star-args) and **kwarg (star-star-kwargs) in Python, which are sometimes called argument unpacking. := rather than interpreting var literally (pronounced colon-equals), we want to use the value stored in the variable called var. tibble::tibble(!!var := val) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 4 #&gt; 2 3 #&gt; 3 9 Note the use of := (pronounced colon-equals) rather than =. Unfortunately we need this new operation because R‚Äôs grammar does not allow expressions as argument names: := is like a vestigial organ: it‚Äôs recognized by R‚Äôs parser, but it doesn‚Äôt have any code associated with it. It looks like an = but allows expressions on either side, making it a more flexible alternative to =. It is used in data.table for similar reasons. ref: Programming with dplyr: Name injection SE-versions of dplyr verbs https://dplyr.tidyverse.org/reference/se-deprecated.html dplyr used to offer twin versions of each verb suffixed with an underscore. These versions had standard evaluation (SE) semantics: rather than taking arguments by code, like NSE verbs, they took arguments by value. Their purpose was to make it possible to program with dplyr. However, dplyr now uses tidy evaluation semantics. NSE verbs still capture their arguments, but you can now unquote parts of these arguments. This offers full programmability with NSE verbs. Thus, the underscored versions are now superfluous. gcm_name &lt;- &quot;CanESM5&quot; mergeData %&gt;% filter(get(gcm_name)&lt;35 ) mergeData %&gt;% filter_(sprintf( &quot;%s&lt;35&quot;, gcm_name ) ) # SE version mergeData %&gt;% select_(gcm_name) # SE version select_(.data, .dots=list() ) .data A data frame. .dots, ... Pair/values of expressions coercible to lazy objects. vars &lt;- list(list(&#39;cyl&#39;, &#39;mpg&#39;), list(&#39;vs&#39;, &#39;disp&#39;)) for (v in vars) { print(mtcars %&gt;% select_(.dots = v) %&gt;% head) } cyl mpg Mazda RX4 6 21.0 Mazda RX4 Wag 6 21.0 Datsun 710 4 22.8 Hornet 4 Drive 6 21.4 Hornet Sportabout 8 18.7 Valiant 6 18.1 vs disp Mazda RX4 0 160 Mazda RX4 Wag 0 160 Datsun 710 1 108 Hornet 4 Drive 1 258 Hornet Sportabout 0 360 Valiant 1 225 Let‚Äôs make something more practical. For each list of variable arguments, we want to group using the first variable and then summarise the grouped data frame by calculating the mean of the second variable. Here, dynamic argument construction really comes into account, because we programmatically construct the arguments of summarise_(), e.g.¬†mean_mpg = mean(mpg) using string concatenation and setNames(): summarise_vars &lt;- list(list(&#39;cyl&#39;, &#39;mpg&#39;), list(&#39;vs&#39;, &#39;disp&#39;)) for (v in summarise_vars) { group_var &lt;- v[1] # group by this variable summ &lt;- paste0(&#39;mean(&#39;, v[2], &#39;)&#39;) # construct summary method, e.g. mean(mpg) summ_name &lt;- paste0(&#39;mean_&#39;, v[2]) # construct summary variable name, e.g. mean_mpg print(paste(&#39;grouping by&#39;, group_var, &#39;and summarising&#39;, summ)) df_summ &lt;- mtcars %&gt;% group_by_(.dots = group_var) %&gt;% summarise_(.dots = setNames(summ, summ_name)) print(df_summ) } # output [1] &quot;grouping by cyl and summarising mean(mpg)&quot; # A tibble: 3 √ó 2 cyl mean_mpg 1 4 26.66364 2 6 19.74286 3 8 15.10000 [1] &quot;grouping by vs and summarising mean(disp)&quot; # A tibble: 2 √ó 2 vs mean_disp 1 0 307.1500 2 1 132.4571 # To refer to column names that are stored as strings, use the `.data` pronoun: vars &lt;- c(&quot;mass&quot;, &quot;height&quot;) cond &lt;- c(80, 150) starwars %&gt;% filter( .data[[vars[[1]]]] &gt; cond[[1]], .data[[vars[[2]]]] &gt; cond[[2]] ) 6.1.9 Merge left_join(x, y, by = NULL, suffix = c(\".x\", \".y\"), ...) x,y tbls to join by a character vector of variables to join by. If NULL, the default, *_join() will do a natural join, using all variables with common colnames across the two tables. To join by the same variables on x and y, use by = c('ID','year'), note that it is a character vector, can‚Äôt use column names directly. To join by different variables on x and y, use a named vector. For example, by = c(\"a\" = \"b\") will match x.a to y.b. suffix If there are non-joined duplicate variables in x and y, these suffixes will be added to the output to disambiguate them. Should be a character vector of length 2. merge(x, y, by = NULL, by.x = NULL, by.y = NULL, all = FALSE, ...) is the base R function for joining two data frames. all = FALSE defaults to FALSE, it performs an inner join, retaining only the rows with matching keys. If TRUE, it performs a full outer join, retaining all rows from both x and y. "],["6.2-pipe-operator.html", "6.2 Pipe Operator", " 6.2 Pipe Operator dplyr works well with the ‚Äòpipe‚Äô operator %&gt;%. The default behavior of %&gt;% when multiple arguments are required in the rhs call, is to place lhs as the first argument, i.e.¬†x %&gt;% f(y) is equivalent to f(x, y). dot (.) works as placeholder in %&gt;% if you want lhs to the rhs call at another position than the first. if you want to use the variable more than once. For example, y %&gt;% f(x, .) is equivalent to f(x, y) and z %&gt;% f(x, y, arg = .) is equivalent to f(x, y, arg = z). The output of a previous function becomes the first argument of the next function, enabling chaining. This is illustrated below, in which only countries from Asia are filtered from the world dataset, next the object is subset by columns (name_long and continent) and the first five rows (result not shown). world7 = world %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% dplyr::select(name_long, continent) %&gt;% slice(1:5) Use the dot for secondary purposes, i.e.¬†you want to use the attributes of x rather than x itself. iris %&gt;% subset(1:nrow(.) %% 2 == 0) is equivalent to iris %&gt;% subset(., 1:nrow(.) %% 2 == 0) In order to avoid plugiging in as the first argument and to call the parameter several times, surround the rhs call with brace brackets {...}. 1:10 %&gt;% {c(min(.), max(.))} is equivalent to c(min(1:10), max(1:10)) 10 %&gt;% { seq(from = .-5, to = .+5) } ## [1] 5 6 7 8 9 10 11 12 13 14 15 all_df %&gt;% group_by(date) %&gt;% group_map(~{ .x %&gt;% drop_na() %&gt;% {crossprod(.[,1003] %&gt;% as.matrix(), .[,3:1002] %&gt;% as.matrix())} }, .keep=TRUE) %&gt;% has higher precedence than arithmetic operations. matrix(c(1,0,0,1), nrow = 2) * 5 %&gt;% as.data.frame() # same as matrix(c(1,0,0,1), nrow = 2) * (5 %&gt;% as.data.frame()) # need to wrap the matrix multiplication before pipe (matrix(c(1,0,0,1), nrow = 2)*5 ) %&gt;% as.data.frame # V1 V2 # 1 5 0 # 2 0 5 %&gt;% can work together with backticks for arithmetic operators x &lt;- 2 x %&gt;% `+`(2) # [1] 4 %&gt;% can use with ! enclosed in backticks &gt; c(TRUE, FALSE) %&gt;% `!` [1] FALSE TRUE &gt; c(TRUE, FALSE) %&gt;% magrittr::not() [1] FALSE TRUE &gt; c(NA,5) %&gt;% Negate(is.na)() [1] FALSE TRUE It is handy to use Aliases in magrittr package, e.g.: extract `[` extract2 `[[` inset `[&lt;-` inset2 `[[&lt;-` use_series `$` add `+` subtract `-` multiply_by `*` raise_to_power `^` multiply_by_matrix `%*%` divide_by `/` divide_by_int `%/%` mod `%%` is_in `%in%` and `&amp;` or `|` equals `==` is_greater_than `&gt;` is_weakly_greater_than `&gt;=` is_less_than `&lt;` is_weakly_less_than `&lt;=` not (`n&#39;est pas`) `!` set_colnames `colnames&lt;-` set_rownames `rownames&lt;-` set_names `names&lt;-` set_class `class&lt;-` set_attributes `attributes&lt;-` set_attr `attr&lt;-` Use {} to specify the precedence, so that they are evaluated the way you want. # calculate the mean and sd for a vector rnorm(100) %&gt;% c(mean(.), sd(.)) # this does not work as the way you want rnorm(100) %&gt;% {c(mean(.), sd(.))} # this works well after enclosing the operation with {} 6.2.1 Native Pipe |&gt; The native pipe was introduced to R in 4.1.0. It is suggested to use the native piple in Tidyverse Style Guide. ‚ùóÔ∏èBUT Caveats using the new native pipe: Code using |&gt; in function reference examples or vignettes will not work on older versions of R, as it is not valid syntax. There are advanced features of %&gt;% which not supported by |&gt;: %&gt;% allows you to change the placement with a . placeholder. R 4.2.0 added a _ placeholder to the base pipe, with one additional restriction: the argument has to be named. For example, x |&gt; f(1, y = _) is equivalent to f(1, y = x). The |&gt; placeholder is deliberately simple and can‚Äôt replicate many features of the %&gt;% placeholder: you can‚Äôt pass it to multiple arguments, and it doesn‚Äôt have any special behavior when the placeholder is used inside another function. For example, df %&gt;% split(.$var) is equivalent to split(df, df$var), and df %&gt;% {split(.$x, .$y)} is equivalent to split(df$x, df$y). %&gt;% allows you to drop the parentheses when calling a function with no other arguments; |&gt;always requires the parentheses. %&gt;% allows you to start a pipe with . to create a function rather than immediately executing the pipe; this is not supported by the base pipe. "],["6.3-categorical-variables.html", "6.3 Categorical Variables", " 6.3 Categorical Variables 6.3.1 Manipulate String Columns reg_dict %&gt;% mutate(def = sapply(strsplit(def,\"\\\\.\"), \"[[\", 2) ) split a string column and select the 2nd item. # alternatively, use separate(def, into, sep, remove=TRUE) reg_dict %&gt;% separate(def, c(&quot;cli_key&quot;, &quot;yr_key&quot;), &quot;.&quot;) separate() function does the opposite of unite(): it splits one column into multiple columns using either a regular expression or character positions. world_separate = world_unite %&gt;% separate(con_reg, c(&quot;continent&quot;, &quot;region_un&quot;), sep = &quot;:&quot;) unite() pastes together existing string columns. world_unite = world %&gt;% unite(&quot;con_reg&quot;, continent:region_un, sep = &quot;:&quot;, remove = TRUE) # remove indicates if the original columns should be removed 6.3.2 Factors forcats is one of the components package in tidyverse; it is useful for working with categorical variables (factors). fct_expand: add additional levels to a factor. fct_drop: drop unused levels. fct_relevel: change the order of a factor by hand. fct_expand(f, ..., after = Inf) add additional levels to a factor. f a factor ... additional levels to add to the factor. after position to place the new level(s). f &lt;- factor(sample(letters[1:3], 20, replace = TRUE)) f #&gt; [1] c a b a b b a c c b b b b c c c a b b c #&gt; Levels: a b c fct_expand(f, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) #&gt; [1] c a b a b b a c c b b b b c c c a b b c #&gt; Levels: a b c d e f fct_expand(f, letters[1:6]) #&gt; [1] c a b a b b a c c b b b b c c c a b b c #&gt; Levels: a b c d e f fct_expand(f, &quot;Z&quot;, after = 0) #&gt; [1] c a b a b b a c c b b b b c c c a b b c #&gt; Levels: Z a b c fct_drop(f, only = NULL) drop unused levels. f &lt;- factor(c(&quot;a&quot;, &quot;b&quot;), levels = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) f #&gt; [1] a b #&gt; Levels: a b c fct_drop(f) #&gt; [1] a b #&gt; Levels: a b # Set only to restrict which levels to drop fct_drop(f, only = &quot;a&quot;) #&gt; [1] a b #&gt; Levels: a b c fct_drop(f, only = &quot;c&quot;) #&gt; [1] a b #&gt; Levels: a b "],["6.4-group-and-apply-functions.html", "6.4 Group and Apply Functions", " 6.4 Group and Apply Functions 6.4.1 group_by() group_by grouping doesn‚Äôt change how the data looks, it changes how it acts with the other dplyr verbs. by_cyl &lt;- mtcars %&gt;% group_by(cyl) # conflict with plyr, specify which to use summarise &lt;- dplyr::summarise summarize &lt;- dplyr::summarize by_cyl %&gt;% summarise( disp = mean(disp), hp = mean(hp), count = n() # count the size of each group ) # A tibble: 3 √ó 4 cyl disp hp count &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 4 105. 82.6 11 2 6 183. 122. 7 3 8 353. 209. 14 n() gives the current group size. Only works inside summarise(). Information about the ‚Äúcurrent‚Äù group or variable These functions return information about the ‚Äúcurrent‚Äù group or ‚Äúcurrent‚Äù variable, so only work inside specific contexts like summarise() and mutate(). n() gives the current group size. Error in n(): ! Must only be used inside data-masking verbs like mutate(), filter(), and group_by(). Cause: R‚Äôs confused with which summarize function (dplyr vs.¬†plyr) it should use. Fix: Explicitly specify dplyr::summarise() cur_group() gives the group keys, a tibble with one row and one column for each grouping variable. Equivalent to .BY in data.table. cur_group_id() gives a unique numeric identifier for the current group. Equivalent to .GBP in data.table. cur_group_rows() gives the row indices for the current group. Equivalent to .I in data.table. cur_column() gives the name of the current column (in across() only). # prioritize functions in `dplyr` to avoid conflicts with `plyr` &gt; needs::prioritize(dplyr) &gt; by_cyl %&gt;% mutate(id = cur_group_id()) # A tibble: 32 √ó 12 # Groups: cyl [3] mpg cyl disp hp drat wt qsec vs am gear carb id &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 2 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 2 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 1 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 2 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 3 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 2 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 3 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 1 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 1 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 2 # ‚Ñπ 22 more rows # ‚Ñπ Use `print(n = ...)` to see more rows &gt; by_cyl %&gt;% summarise(id = cur_group_id()) # A tibble: 3 √ó 2 cyl id &lt;dbl&gt; &lt;int&gt; 1 4 1 2 6 2 3 8 3 &gt; by_cyl %&gt;% summarise(data = cur_group()) # A tibble: 3 √ó 2 cyl data$cyl &lt;dbl&gt; &lt;dbl&gt; 1 4 4 2 6 6 3 8 8 &gt; df &lt;- tibble( g = sample(rep(letters[1:3], 1:3)), x = runif(6), y = runif(6) ) &gt; gf &lt;- df %&gt;% group_by(g) &gt; gf # A tibble: 6 √ó 3 # Groups: g [3] g x y &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 b 0.0246 0.143 2 c 0.478 0.415 3 a 0.758 0.414 4 c 0.216 0.369 5 c 0.318 0.152 6 b 0.232 0.139 # get row indices for the current group &gt; gf %&gt;% reframe(row=cur_group_rows()) # A tibble: 6 √ó 2 g row &lt;chr&gt; &lt;int&gt; 1 a 3 2 b 1 3 b 6 4 c 2 5 c 4 6 c 5 group_by then count non-NA values gdp_df %&gt;% group_by(year) %&gt;% summarise_all(~ sum(!is.na(.))) group_split() split data.frame by groups, good friend with group_by, i.e.¬†it used grouping structure from group_by() and therefore is subject to the data mask. It returns a list of tibbles. Each tibble contains the rows of .tbl for the associated group and all the columns, including the grouping variables. ir &lt;- iris %&gt;% group_by(Species) # returns a list of groups ir %&gt;% group_split() # returns a tibble of group keys ir %&gt;% group_keys() # A tibble: 3 x 1 Species &lt;fct&gt; 1 setosa 2 versicolor 3 virginica # get group_keys() with index, so you can access each specific group with group info df %&gt;% group_by(ID) %&gt;% group_keys() %&gt;% as.data.frame() %&gt;% t() df %&gt;% group_by(ID) %&gt;% group_keys() %&gt;% pull(Species) # assgin a index column to indicate the group number df %&gt;% group_by(ID) %&gt;% group_keys() %&gt;% mutate(id = row_number()) # equivalently df %&gt;% group_by(ID) %&gt;% group_keys() %&gt;% mutate(id = 1:n() ) Group by and count the number of observations in each group with tally # count the number of rows per group ir %&gt;% tally(sort=TRUE) # same as ir %&gt;% summarize(count=n()) # A tibble: 3 √ó 2 Species n &lt;fct&gt; &lt;int&gt; 1 setosa 50 2 versicolor 50 3 virginica 50 dplyr::row_number(x) gives every input a unique rank to a vector x, so that c(10, 20, 20, 30) would get ranks c(1, 2, 3, 4). From smallest to largest. To rank by multiple columns at once, supply a data frame x. Rank by value, ascending: data %&gt;% mutate(rank = row_number(value) ) If want descending, use row_number(desc(.) data %&gt;% mutate(rank = row_number(desc(value) ) ) # equivalently, with a - before value data %&gt;% mutate(rank = row_number(-value) ) Another use is to add row number to a data frame. Here, we assign row number to a variable or column name ‚Äúrow_id‚Äù. data %&gt;% mutate(row_id = row_number()) 6.4.2 Subset rows slice_head(.data, n) and slice_tail(.data, n) select the first or last n rows in .data. slice_sample() randomly selects rows. slice_min(.data, order_by, n) and slice_max() select rows with the smallest or largest values of a variable. # Similar to head(mtcars, 1): mtcars %&gt;% slice(1L) # first two rows every group df %&gt;% group_by(group) %&gt;% slice_head(n = 2) # first row mtcars %&gt;% filter(row_number() == 1L) # last row mtcars %&gt;% filter(row_number() == n()) # Rows with minimum values of a variable mtcars %&gt;% slice_min(mpg, n = 5) 6.4.3 Summarize Scoped verbs (summarize_if, summarize_at, summarize_all) have been superseded by the use of across(.var, .fun) inside summarize(). summarize_at(.tbl, .var, .fun, ...) \\(\\rightarrow\\) summarize(.tbl, across(.var, .fun) ) .tbl a tbl object, eg. a groupped tibble .var A list of columns generated by vars(), a character vector of column names, a numeric vector of column positions, or NULL. ÂèØ‰ª•Áî®ÂèòÈáèÂêçÂ≠ó (character) ÊàñËÄÖ‰ΩçÁΩÆ (numeric)„ÄÇ Note that numeric position start with the group keys, and then followed by ordinary columns. .fun A function fun, a quosure style lambda ~ fun(.) or a list of either form. ... Additional arguments for the function calls in .funs. # `summarize_at` affects variables selected with a character vector or vars() error_continent_all %&gt;% group_by(pct, CON) %&gt;% summarize(across(-r_no, ~mean(.x, na.rm=TRUE)) ) starwars %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) starwars %&gt;% summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE))) by_species &lt;- iris %&gt;% group_by(Species) by_species %&gt;% summarise_all(list(min, max)) by_species %&gt;% summarise(across(everything(), list(min = min, max = max))) # Count size of group by_species %&gt;% summarize(count=n()) # Or use `tally()` by_species %&gt;% tally(sort=TRUE) Select one group # Select one element from a list iris %&gt;% `[[`(&quot;Species&quot;) # select one group by_species %&gt;% group_split() %&gt;% `[[`(1) # can extract by either position or name across(.cols = everything(), .fns = NULL, ..., .names = NULL) makes it easy to apply the same transformation to multiple columns, allowing you to use select() semantics inside in summarise() and mutate(). .cols Columns to transform. could use functions to select columns, eg, starts_with(\"Sepal\"), where(is.factor) .fns Functions to apply to each of the selected columns. .names A glue specification that describes how to name the output columns. # code snippet for counting non-NA values for multiple select columns data_desp %&gt;% group_by(ISO_C3) %&gt;% summarise(across(c(&quot;logDiff&quot;,&quot;tmp&quot;,&quot;pre&quot;,&quot;dtr&quot;), ~sum(!is.na(.))) ) %&gt;% arrange(logDiff) 6.4.4 split-apply-combine Here is a summary of the split-apply-combine approach: dplyr::group_map(), dplyr::group_modify() ‚Ä¶ split ‚Üí purrr::map() plyr::ddply() data.table::setDT() # with `dplyr` df %&gt;% group_by(type) %&gt;% group_map(~tibble(trivial_func(.))) # must do `group_split` before your can `map_dfr` df %&gt;% split(type) %&gt;% # equivalent to `group_by(type)` then `group_split()` purrr::map_dfr(trivial_func) # with `data.table` library(data.table) setDT(df)[, trivial_func(.SD), type] # with `plyr` plyr::ddply(df, .(type), trivial_func) Apply regression to each group with purrr::map # A more realistic example: split a data frame into pieces, fit a # model to each piece, summarise and extract R^2 mtcars %&gt;% split(.$cyl) %&gt;% map(~ lm(mpg ~ wt, data = .x)) %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) # If each element of the output is a data frame, use # map_dfr to row-bind them together: mtcars %&gt;% split(.$cyl) %&gt;% map(~ lm(mpg ~ wt, data = .x)) %&gt;% map_dfr(~ as.data.frame(t(as.matrix(coef(.))))) # (if you also want to preserve the variable names see # the broom package) 6.4.5 dplyr::group_modify() dplyr::group_modify() and dplyr::group_map() are purrr-style functions that can be used to iterate on grouped tibbles. Note that purrr::map, map_dfr, and map_dfc does NOT work on groupped tibbles. You must do group_split to a list, then you can apply the purrr-style functions. Convenient for multistep operations, easy to debug group_modify takes in a groupped tibble; returns a groupped tibble and .f must return a data frame. group_map takes in a groupped tibble and returns a list They takes in groupped tibbles, not good at multistep operations, hard to debug because you need to wrap up your whole calculation in one function. group_modify(.data, .f, ..., .keep = FALSE) returns a grouped tibble. .data A grouped tibble .f A function, formula, or vector to apply to each group. It MUST return a data frame or a tibble! Matices do not work. A workaround is to use tibble::enframe to If a function, it is used as is. It should have at least 2 formal arguments. If your costum function has only one argument, you use the formula specification ~f(.x). econ_data %&gt;% group_by(year) %&gt;% group_modify(~funPer_group(.x)) If a formula, e.g.¬†~ head(.x), it is converted to a function. In the formula, you can use . or .x to refer to the subset of rows of .tbl for the given group ¬†.y to refer to the group key, a one row tibble with one column per grouping variable that identifies the group ... Additional arguments passed on to .f .keep=FALSE whether the grouping variables are kept in .x. Default to drop&gt; the grouping variable. Be mindful of this when you want to subset columns by position. cntry_group &lt;- cntry_agg_df %&gt;% group_by(as.numeric(ISO_N3), year) groups &lt;- cntry_group %&gt;% group_split() cntry_key &lt;- group_keys(cntry_group) # use a lambda function which includes a sequence of calculations cntry_stat &lt;- cntry_group %&gt;% group_modify(~ { .x %&gt;% select(var_vec) %&gt;% map_dfc(cal_stat) %&gt;% # calculate stat mutate(stat = c(&quot;mean&quot;, &quot;std&quot;)) # assign stat names }) %&gt;% ungroup() When using a lambda function inside group_modify: .x can only be used once at the first function; for later use, use . .x and . are useful when the argument being assigned is NOT the first argument. If you use .x instead of ., you will encounter the following error: Error in xj[i] : invalid subscript type ‚Äòlanguage‚Äô Apply CAPM to each company # Apply CAPM to each company # start with `.x` # clearer logic flow, I prefer this approach ‚úì data_group &lt;- data %&gt;% group_by(ISIN) data_group %&gt;% # filter(cur_group_id() %in% c(1,2)) %&gt;% filter(cur_group_id() == 1) %&gt;% group_modify(~ { .x %&gt;% filter(between(p_date, estimation_start, estimation_end)) %&gt;% lm(eRi~rmrf, data=.) %&gt;% tidy(.) }) # ------------------------------------------------ # Equivalently, plug in `.x` in the first func # one line shorter, more concise data_group %&gt;% filter(cur_group_id() == 1) %&gt;% group_modify(~ { filter(.x, between(p_date, estimation_start, estimation_end)) %&gt;% lm(eRi~rmrf, data=.) %&gt;% tidy(.) }) filter(cur_group_id() == 1) select the 1st group to test run your function ‚úÖ Can also define a function and pass to group_modify This approach is easier to debug, whereas long sequence of lambda is hard to debug. factor_model &lt;- function(company_data){ company_data &lt;- company_data %&gt;% filter(between(p_date, estimation_start, estimation_end)) reg_ml &lt;- lm(eRi~rmrf, data=company_data) coef_est &lt;- tidy(reg_ml) return (coef_est) } # Debug with one group groups &lt;- data_group %&gt;% group_split() factor_model(groups[[1]]) # A tibble: 2 √ó 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 0.114 0.0784 1.45 0.147 2 rmrf 0.280 0.0945 2.97 0.00331 # Apply to all groups data_group %&gt;% filter(cur_group_id() %in% c(1,2)) %&gt;% group_modify(~factor_model(.x)) group_modify requires at least 2 formal arguments for the function. Since the custom function factor_model has only 1 argument, we must use the lambda formula to pass to group_modify. group_map(.data, .f, ..., .keep = FALSE) returns a list of results. More examples of group_modify Apply a regression What I like most about group_modify is that it automatically adds the group key to the returned results. You don‚Äôt need to specify identifiers by yourself then. Calculate summary statistics group_modify code snippets # Apply a regression iris %&gt;% group_by(Species) %&gt;% group_modify(~ broom::tidy(lm(Petal.Length ~ Sepal.Length, data = .x))) #&gt; # A tibble: 6 √ó 6 #&gt; # Groups: Species [3] #&gt; Species term estimate std.error statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 setosa (Intercept) 0.803 0.344 2.34 2.38e- 2 #&gt; 2 setosa Sepal.Length 0.132 0.0685 1.92 6.07e- 2 #&gt; 3 versicolor (Intercept) 0.185 0.514 0.360 7.20e- 1 #&gt; 4 versicolor Sepal.Length 0.686 0.0863 7.95 2.59e-10 #&gt; 5 virginica (Intercept) 0.610 0.417 1.46 1.50e- 1 #&gt; 6 virginica Sepal.Length 0.750 0.0630 11.9 6.30e-16 # --------------------------------------------------------------- # descriptive statistics # to use group_modify() the lambda must return a data frame iris %&gt;% group_by(Species) %&gt;% group_modify(~ { quantile(.x$Petal.Length, probs = c(0.25, 0.5, 0.75)) %&gt;% tibble::enframe(name = &quot;prob&quot;, value = &quot;quantile&quot;) }) #&gt; # A tibble: 9 √ó 3 #&gt; # Groups: Species [3] #&gt; Species prob quantile #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 setosa 25% 1.4 #&gt; 2 setosa 50% 1.5 #&gt; 3 setosa 75% 1.58 #&gt; 4 versicolor 25% 4 #&gt; 5 versicolor 50% 4.35 #&gt; 6 versicolor 75% 4.6 #&gt; 7 virginica 25% 5.1 #&gt; 8 virginica 50% 5.55 #&gt; 9 virginica 75% 5.88 # `fivenum()` returns min, lower-hinge (Q1), median (Q1), # upper-hinge (Q3), and max iris %&gt;% group_by(Species) %&gt;% group_modify(~ { .x %&gt;% purrr::map_dfc(fivenum) %&gt;% mutate(nms = c(&quot;min&quot;, &quot;Q1&quot;, &quot;median&quot;, &quot;Q3&quot;, &quot;max&quot;)) }) #&gt; # A tibble: 15 √ó 6 #&gt; # Groups: Species [3] #&gt; Species Sepal.Length Sepal.Width Petal.Length Petal.Width nms #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 setosa 4.3 2.3 1 0.1 min #&gt; 2 setosa 4.8 3.2 1.4 0.2 Q1 #&gt; 3 setosa 5 3.4 1.5 0.2 median #&gt; 4 setosa 5.2 3.7 1.6 0.3 Q3 #&gt; 5 setosa 5.8 4.4 1.9 0.6 max #&gt; 6 versicolor 4.9 2 3 1 min #&gt; 7 versicolor 5.6 2.5 4 1.2 Q1 #&gt; 8 versicolor 5.9 2.8 4.35 1.3 median #&gt; 9 versicolor 6.3 3 4.6 1.5 Q3 #&gt; 10 versicolor 7 3.4 5.1 1.8 max #&gt; 11 virginica 4.9 2.2 4.5 1.4 min #&gt; 12 virginica 6.2 2.8 5.1 1.8 Q1 #&gt; 13 virginica 6.5 3 5.55 2 median #&gt; 14 virginica 6.9 3.2 5.9 2.3 Q3 #&gt; 15 virginica 7.9 3.8 6.9 2.5 max group_walk(.data, .f) calls .f for side effects and returns the input .tbl, invisibly. Can be used to write dplyr groups to separate files. # `.x` refers to the group # `.y` refers to the key # `.keep = TRUE` keeps the key column all_df %&gt;% group_by(ISO_C3) %&gt;% group_walk(~ write_csv(.x, sprintf(&quot;./data/reg_result/bootstrap/%s/pathway_country/country-impact_bootstrap1000_tmp_%s.csv&quot;, ssp, .y$ISO_C3) ), .keep = TRUE) Suset specific groups mtcars %&gt;% group_by(cyl) %&gt;% select_first_n_groups(2) %&gt;% do({&#39;complicated expression&#39;}) data.table implementation setDT(mtcars)[, .SD[.GRP %in% 1:2], by=cyl] dplyr implementation mtcars %&gt;% group_by(cyl) %&gt;% filter(cur_group_id() %in% c(1,2)) "],["7-graphics.html", "Chapter 7 Graphics", " Chapter 7 Graphics Graphic Parameters par() permanently set global graphic parameters in the current session. Doesn‚Äôt apply to external graphic devices, e.g., x11. One nice looking setting: par(oma=c(0.5,0.5,0,2), mar=c(3.5, 3.5, 2, 1), mgp=c(2.4,0.8,0)) mar inner margins inside the plot area. Use this to set spacing between subplots. ÂÜÖËæπË∑ù oma outer margins outside the plot area. If it is a multi-panel figure, use this to set spaing for the whole figure. Â§ñËæπË∑ù It takes a little trial-and-error to hit on margins that produce the desired spacing. If you want all of your panels to have plots that fill the same area, then the bottom+top and left+right margins need to be the same in all panels. In the following example, the sum is 0.4 for both, but bottom+top does not need to equal left+right. oma Âíå mar ÊòØÂè†Âä†ÁöÑÂÖ≥Á≥ª„ÄÇÂ¶ÇÊûúÊúâ‰ªª‰ΩïÂõæÁöÑÂÖÉÁ¥†Ë¢´chop off, Â¢ûÂä†ÂÜÖËæπË∑ù„ÄÇ these parameters can also be set inside plot. When use plot.xts, parameters set using par are not honored, have to specify inside the plot.xts function. plot.xts set mar = c(3, 2, 0, 2). So any settings before are overwritten. Note that par does not work for ggplot figures, must set margins using theme(plot.margin=margin(t = 0, r = 0, b = 0, l = 0, unit = \"pt\")). Q: Axis title got chopped off. A: increase mar to enlarge inner margins. Or, reduce mgp[1] to reduce the margin between title and axis. Q: ÈúÄË¶ÅÊØè‰∏™subplot ËÆæÁΩÆÂêÑËá™ËæπË∑ù A: ÊØè‰∏™ÂõæÂºÄÂßã‰πãÂâçËÆæÁΩÆpar. Q: add title to each subplot. A: use text to add text in the graph area. Q: add text to the whole figure, such as shared y-axis title and figure title. A: use mtext(text, side). 1-bottom, 2-left, 3-right, 4-top. # multipanel example par(mfrow=c(2,2), tcl=-0.5, family=&quot;serif&quot;) # Top left panel par(mai=c(0.2,0.4,0.2,0)) plot(Sepal.Length~Sepal.Width, data=iris, subset=(Species==&quot;virginica&quot;), xlab=&quot; &quot;, ylab=&quot; &quot;, xlim=c(min.width,max.width), ylim=c(min.length,max.length), xaxt=&quot;n&quot;) axis(1, labels=FALSE) # drop the numbers in the x-axis # add title to subplot text((max.width-min.width)/2 + min.width, max.length+0.15, expression(italic(&quot;Iris virginica&quot;))) # Top right panel plot.new() # Bottom left panel par(mai=c(0.4,0.4,0,0)) plot(Sepal.Length~Sepal.Width, data=iris, subset=(Species==&quot;versicolor&quot;), xlab=&quot; &quot;, ylab=&quot; &quot;, xlim=c(min.width,max.width), ylim=c(min.length,max.length)) text((max.width-min.width)/2 + min.width, max.length+0.15, expression(italic(&quot;Iris versicolor&quot;))) # Bottom right panel par(mai=c(0.4,0.2,0,0.2)) plot(Sepal.Length~Sepal.Width, data=iris, subset=(Species==&quot;setosa&quot;), xlab=&quot; &quot;, ylab=&quot; &quot;, xlim=c(min.width,max.width), ylim=c(min.length,max.length), yaxt=&quot;n&quot;) axis(2, labels=FALSE) # drop the number in the y-axis text((max.width-min.width)/2 + min.width, max.length+0.15, expression(italic(&quot;Iris setosa&quot;))) # Shared titles for the whole figure mtext(&quot;Sepal Width&quot;, side=1, outer=T, at=0.5) mtext(&quot;Sepal Length&quot;, side=2, outer=T, at=0.5) A 3x3 multiplot figure Sometimes for loop works strangely, so use repeat ... break loop. par(mfrow=c(3,3)) i &lt;- 1 repeat { plot_data &lt;- confusion_ftable[,,i] mosaicplot(t(plot_data), xlab=&quot;Observation&quot;, ylab=&quot;Prediction&quot;, main=sprintf(&quot;Group %s&quot;, i), cex.axis = 1.2 ) i &lt;- i+1 if (i==10) break } Graphical Parameters par() https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/par ?par to get help page in R. par Parameter Description oma outer margin in text lines for the whole plotting area, c(bottom, left, top, right), clockwise from bottom. Mesured in margin lines.By default, par(oma = c(b=0, l=0, t=0, r=0)). oma is set for each panel. Cannot be used for each subfigure. omi Same as oma, but set outer margins in inches. mar - Inner margin. A numerical vector of the form c(bottom, left, top, right) which gives the number of lines of margin to be specified on the four sides of the plot. The default is c(b=5, l=4, t=4, r=2) + 0.1. Measured in margin lines. - Margin from axis to figure boundaries, cannot be too small, otherwise axis label could be clipped out and invisible for you.- Change this when you have subplots in one panel; mai Same as mar, but set inner margins in inches. By default, par(mai = c(1, 0.8, 0.8, 0.4) + 0.02). pch plot character, 1~25 cex controls the size of text and symbols in the plotting area with respect to the default value of 1.Plot symbol size, if one number, then all symbols scale to the same level; if a vector of the same length with data, then each point scales accordingly. cex.lab x- and y-axis title magnification relative to current setting. cex.axis The magnification to be used for axis text relative to the current setting of cex. cex.main main title magnification relative to current setting col controls the color. col.lab The color to be used for x and y labels. col.axis The color to be used for axis annotation. mgp The margin line (in mex units) for the axis title, axis labels (ticks) and axis line. The default is c(3, 1, 0). Negative distances are allowed, resulting in titles and / or labels inside the plotting region. - mgp[1] defines distance between the axis titles and the axes, - mgp[2] defines distance between the axis labels and the axes. - mgp[3] just leave it at 0;Default values are too large. mgp=c(2,0.8,0) looks good. mfrow=c(1,1) layout of subplots, Multiple Figures drawn by row. c(nr, nc) creates an nr-by-nc layout. xpd=T, default as F it is OK to plot outside the region, - FALSE: only inside the plot; - TRUE: in the outer plotting area‚Äô - NA: everywhere on your plotting device horiz=T you want a horizontal legend inset=c(x,y) move the legend relative to the ‚Äòbottom‚Äô location bty = ‚Äòn‚Äô Specify the box surrounding the graph area.o: complete box (default parameter), n: no box 7: top + right L: bottom + left C: top + left + bottom U: left + bottom + right las=0 numeric in {0,1,2,3}. 0 always parallel to the axis [default], 1 always horizontal, 2 always perpendicular to the axis, 3 always vertical. yaxt / xaxt A character which specifies the y/x axis type. Specifying \"n\" to suppress plotting the axis text. This is useful when you have multiple panels with shared axis labels and titles. Every time you change par, first save the orginal one so that could be easily reset, if needed. op &lt;- par() # save unchanged original defualt one par(newsettings) # change to the way you need, newsettings can be tag=value or a list of tagged values par(op) # reset to the default par() gets reset every time a new device is opened. So use dev.new() and dev.off() to create a new device and close. par() with no arguments is used to get all the graphical parameters as a named list. Several parameters can only be set by a call to par(), others can be set as arguments to plot functions. See ?par for which par can only be changed through par(). Tip: When you cannot change parameters through plot functions, change through par(newsettings). Just put it before the plot command, and leave everthing else the same. After the plot, you can restore the settings to default. For instance, cex.lab doesn‚Äôt work in mosaicplot, you just change the font size through par(cex.lab=1.5) and put it before mosaicplot. Q: How to query values of graphical parameters? A: Two options: par(\"cex\") or par()$cex. # query one parameter &gt; par(&quot;cex&quot;) [1] 0.66 # query multiple parameters &gt; par(c(&quot;cex&quot;, &quot;omi&quot;)) $cex [1] 0.66 $omi [1] 0 0 0 0 par(cex.lab=1.2) set cex.lab to 1.2. This will enlarge font size for the plot. Defaults to 1. Plot example https://r-charts.com/base-r/axes/?utm_content=cmp-true x&lt;-1:10; y&lt;-x*x # Suppress the axis plot(x, y, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;) # Changing x axis xtick&lt;-seq(0, 10, by=5) axis(side=1, at=xtick, labels = FALSE) # Add tick marks text(x=xtick, par(&quot;usr&quot;)[3], labels = xtick, srt = 45, pos = 1, xpd = TRUE) # Add tick mark labels # Changing y axis ytick&lt;-seq(0, 100, by=50) axis(side=2, at=ytick, labels = FALSE) text(par(&quot;usr&quot;)[1], ytick, labels = ytick, srt = 45, pos = 2, xpd = TRUE) plot() function accepts graphical parameters in par to set aesthetics, such as font size, margins. The difference is that settings in plot() apply to current figure, while settings in par apply globally. plot Parameter Description type='l' what type of plot should be drawn. Default p for points. Other types:l for lines; b for both; o for Both points and lines ‚Äúoverplotted‚Äù; main main title xlab x axis title xaxp and yaxp customizing where the ticks of each axis start and end and the number of regions to divide the axis specifying vectors of the form c(start, end, number_regions). xlim and ylim set axis limits lty Line types can either be specified as an integer (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) or as one of the character strings \"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\", \"longdash\", or \"twodash\", where \"blank\" uses ‚Äòinvisible lines‚Äô (i.e., does not draw them). lwd line width relative to the default (default=1). 2 is twice as wide. col default plotting color. - The simplest way is with a character string giving the color name (e.g., \"red\"). A list of the possible colors can be obtained with the function colors. - Alternatively, colors can be specified directly in terms of their RGB components with a string of the form \"#RRGGBB\" where each of the pairs RR, GG, BB consist of two hexadecimal digits giving a value in the range 00 to FF. - Colors can also be specified by giving an index into a small table of colors, the palette: indices wrap round so with the default palette of size 8, 10 is the same as 2. Index 0 corresponds to the background color. Note that the palette (apart from 0 which is per-device) is a per-session setting.- You can use an integer without calling a palette. This refers to the default color palette in R. There are only 8 different colors. 1 is black, 2 is red, 3 is green, 4 is blue. Recycle to index the palette.- Add transparency. The rgb() function has an optional 4th argument: rgb(r,g,b,alpha). The value of alpha is between 0 and 1, with 0 being totally transparent and 1 being totally opaque. line type The lattice graphics is useful for conditioning types of plots. Make separate plots for each group. lattice::xyplot(drinks ~ partyHr | gender, data=survey, ylab=&quot;Drinks/week&quot;, xlab=&quot;Party hours/week&quot;) xyplot() function in the lattice graphics is analogous to as the plot() function in the base graphics. The histogram() function is a lattice graphics function that is analogous to the hist() function in the base graphics. The labels argument of the axis function allows customizing the tick mark labels. # Change X-axis tick labels plot(x, y, pch = 19, xaxt = &quot;n&quot;) axis(1, at = seq(round(min(x)), round(max(x)), by = 1), labels = 1:11) You can join together two plots with different Y-axis scale increasing the margins of the plot, using par(new = TRUE), creating a new plot without axis or labels and setting a new axis with the axis function. # Increase the plot margins par(mar = c(5, 4, 4, 4) + 0.25) # Data plot(x, y, pch = 19, ylab = &quot;Var 1&quot;) # Needed to merge the plots par(new = TRUE) # More data plot(x ^ 2, y ^ 2, col = 4, pch = 19, axes = FALSE, # No axes bty = &quot;n&quot;, # No box xlab = &quot;&quot;, ylab = &quot;&quot;) # No axis labels # New axis axis(4) # Axis label mtext(&quot;Var 2&quot;, side = 4, line = 3, col = 4) mtext(text, side, line, adj, at) write text into the margins of a plot. side on which side of the plot (1=bottom, 2=left, 3=top, 4=right). line set the margin line where to set the text, starting at 0 counting outwards. Defaults to 0. adj alignment for each string in the reading direction from 0 to 1, defaults to 0.5. For strings parallel to the axes, adj = 0 means left or bottom alignment, and adj = 1 means right or top alignment. at to indicate the absolute position where to draw the text based on the corresponding axis. Allows you to specify locations more precisely than adj. adj uses relative positions. Plot Symbols pch plot symbol 20 small solid circle. 21 circle with border and filling. You can colour the inside and outside separately. geom_point(shape=21, fill=&quot;#008BFF&quot;, color=&quot;#008BFF&quot;, alpha=0.8, size=NULL) https://blog.albertkuo.me/post/point-shape-options-in-ggplot/ 1 2 Á©∫ÂøÉÂúÜ, ‰∏âËßíÂΩ¢ 16 17 ÂÆûÂøÉÂúÜ, ‰∏âËßíÂΩ¢ The size aesthetic control the size of points and text, and can be specified with a numerical value (in millimetres) or via a mapping to a continuous variable. pch 16 midium, 19 large, 20 small Linetype The linetype aesthetic can be specified with either an integer (0-6), a name (0 = blank, 1 = solid, 2 = dashed, 3 = dotted, 4 = dotdash, 5 = longdash, 6 = twodash), Save figures png(filename, width = 480, height = 480, units = \"px\", res = 72, ...) unit: one of px, in, cm, or mm. Defaults to px. res: The nominal resolution in dpi (number of dot per inch) for bitmap devices (dpi * inches = pixels). The higher the dpi, the sharper the image. Default res of 72 dpi is too fuzzy. To remove the graininess, use res=300 for a fine-resolution figure. For the same figure size, the higher the resolution, the larger the zoom of the figure png(file=&quot;example1.png&quot;, res=100) plot(1:10) dev.off() Resolution 100 ppi. png(file=&quot;example1.png&quot;, res=200) plot(1:10) dev.off() Resolution 100 ppi. Suggested routine to find the correct size of your figure using base R: Zoom the plot pane, adjust to the size you are happy with, right click and choose Copy Image Address, you will have a url like http://127.0.0.1:39903/graphics/plot_zoom_png?width=913&amp;height=563 The url provides the aspect ratio of your choice and is a good start point for figure size. Use 3 times the given size, i.e., width = 913x3, height = 563x3, set res=300, for a high-resolution figure. "],["7.1-multipanel-plot.html", "7.1 Multipanel Plot", " 7.1 Multipanel Plot https://felixfan.github.io/stacking-plots-same-x/ https://stackoverflow.com/questions/13649473/add-a-common-legend-for-combined-ggplots 7.1.1 With Base R par(mfrow=c(ncol, nrow)) to create a grid for subfigures. text(labels=\"(a)\", x, y, xpd=T, ...) add labels to subfigures. x and y here are absolute positions. Can use grconvertX to convert x-axis aboslute postions to relative positions. Can use grconvertY to convert y-axis aboslute postions to relative positions. # add labels to multi-panel figures put.fig.letter &lt;- function(label, location=&quot;topleft&quot;, x=NULL, y=NULL, offset=c(0, 0), ...) { # offset[1]: x-axis buffer, negative to move to the left; positive to move to the right # offset[2]: y-axis buffer, negative to move downward; positive to move upward if(length(label) &gt; 1) { warning(&quot;length(label) &gt; 1, using label[1]&quot;) } if(is.null(x) | is.null(y)) { coords &lt;- switch(location, topleft = c(0.015,0.98), topcenter = c(0.5525,0.98), topright = c(0.985, 0.98), bottomleft = c(0.015, 0.02), bottomcenter = c(0.5525, 0.02), bottomright = c(0.985, 0.02), c(0.015, 0.98) ) } else { coords &lt;- c(x,y) } this.x &lt;- grconvertX(coords[1] + offset[1], from=&quot;nfc&quot;, to=&quot;user&quot;) this.y &lt;- grconvertY(coords[2] + offset[2], from=&quot;nfc&quot;, to=&quot;user&quot;) text(labels=label[1], x=this.x, y=this.y, xpd=T, ...) } my.locations &lt;- rep(&quot;topleft&quot;, 2) f_name &lt;- &quot;~/Library/CloudStorage/OneDrive-Norduniversitet/FIN5005 2024Fall/OLS lab/images/diagnostic_plot3.png&quot; # ggsave(f_name) f_name png(f_name, width=8.91*ppi, height=4.85*ppi, res=ppi) par(mfrow=c(1,2)) plot(fit.auto2, 1) put.fig.letter(label=&quot;(a)&quot;, location=my.locations[1], font=2, offset = c(0.1, -0.1)) plot(fit.auto2, 2) put.fig.letter(label=&quot;(b)&quot;, location=my.locations[2], font=2, offset = c(0.1, -0.1)) dev.off() plots alignment grid::grid.draw(x) draw a grid grob x An object of class \"grob\" or NULL. grid::grid.draw(rbind(ggplotGrob(CI_plot_converted), ggplotGrob(CI_plot_simple), size = &quot;last&quot;)) # same width, same height # or gridExtra::grid.arrange(CI_plot_converted, CI_plot_simple, ncol=1, heights = c(0.8, 1)) 7.1.2 With cowplot package cowplot::plot_grid(p1, p2, p3, align = 'vh', labels = c(\"A\", \"B\", \"C\"), hjust = -1, nrow = 1) Arrange multiple plots into a grid. plot_grid(..., plotlist = NULL, align = c(\"none\", \"h\", \"v\", \"hv\"), axis = c(\"none\", \"l\", \"r\", \"t\", \"b\", \"lr\", \"tb\", \"tblr\"), nrow = NULL, ncol = NULL, rel_widths = 1, rel_heights = 1, labels = NULL, label_size = 14, label_fontfamily = NULL, label_fontface = \"bold\", label_colour = NULL, label_x = 0, label_y = 1, hjust = -0.5, vjust = 1.5, scale = 1, greedy = TRUE) ... Plots to be arranged into the grid, separated by comma, e.g., plot_grid(p1, p2, ...). plotlist A list of plots to display. Alternatively, the plots can be provided individually as the first n arguments of the function plot_grid. p_list &lt;- list() p_list[[1]] &lt;- p1 p_list[[2]] &lt;- p2 plot_grid(plotlist = p_list) align how graphs in the grids should be aligned. h horizontally v vertically hv both horizontally and vertically none default rel_widths, rel_heights Numerical vector of relative columns widths. For example, in a two-column grid, rel_widths = c(2, 1) would make the first column twice as wide as the second column. labels List of labels to be added to the plots. You can also set labels=\"AUTO\" to auto-generate upper-case labels or labels=\"auto\" to auto-generate lower-case labels. labels=sprintf(\"(%s)\", letters[1:6]) lower-case letters with parentheses. label_size lable font size; hjust left: positive, right: negative; (label position) vjust down: positive, up: negative; ‚Äã # load cowplot library(cowplot) # down-sampled diamonds data set dsamp &lt;- diamonds[sample(nrow(diamonds), 1000), ] # Make three plots. # We set left and right margins to 0 to remove unnecessary spacing in the # final plot arrangement. # plot.margin = unit(c(top, right, bottom, left), &quot;pt&quot;) for outer margins of plotting area p1 &lt;- qplot(carat, price, data=dsamp, colour=clarity) + theme(plot.margin = unit(c(6,0,6,0), &quot;pt&quot;)) p2 &lt;- qplot(depth, price, data=dsamp, colour=clarity) + theme(plot.margin = unit(c(6,0,6,0), &quot;pt&quot;)) + ylab(&quot;&quot;) p3 &lt;- qplot(color, price, data=dsamp, colour=clarity) + theme(plot.margin = unit(c(6,0,6,0), &quot;pt&quot;)) + ylab(&quot;&quot;) # arrange the three plots in a single row prow &lt;- plot_grid( p1 + theme(legend.position=&quot;none&quot;), p2 + theme(legend.position=&quot;none&quot;), p3 + theme(legend.position=&quot;none&quot;), align = &#39;vh&#39;, labels = sprintf(&quot;(%s)&quot;, letters[1:3]), # auto lower case letter for labels label_size = 14, # label font size hjust=0, # left: positive, right: negative (adjust label position) vjust=-1, # down:positive, up:negative nrow = 1 ) # extract the legend from one of the plots # (clearly the whole thing only makes sense if all plots # have the same legend, so we can arbitrarily pick one.) legend_b &lt;- get_legend(p1 + theme(legend.position=&quot;bottom&quot;)) # add the legend underneath the row we made earlier. Give it 10% of the height # of one plot (via rel_heights). p &lt;- plot_grid( prow, legend_b, ncol = 1, rel_heights = c(1, .1)) p Iregular girds can be created using nested plot_grid For instance, the first plot is narrower than the second plot, you can create a buffer for the first plot. # the first figure is half the width of the second figure plot_grid(plot_grid(p_hist, NULL, nrow=1, rel_widths = c(1,1)), p_map_diff + theme(plot.margin = margin(0,0,0,0)), nrow=2, rel_heights = c(1, 1.2), labels=sprintf(&quot;(%s)&quot;, letters[1:4]), label_size=12) "],["7.2-goodlooking-colors.html", "7.2 Goodlooking Colors", " 7.2 Goodlooking Colors Ëâ≤Áõ∏-È•±ÂíåÂ∫¶-‰∫ÆÂ∫¶ (HSL) È¢úËâ≤ÁöÑ‰∏âÂ§ßÂü∫Êú¨Â±ûÊÄßÊòØÔºö Ëâ≤Áõ∏ (Hue): ‰ªÄ‰πàÈ¢úËâ≤ Ë°®Á§∫È¢úËâ≤ÁöÑÁßçÁ±ªÔºåÊòØÊàë‰ª¨ÈÄöÂ∏∏ÊâÄËØ¥ÁöÑ‚ÄúÁ∫¢Ëâ≤„ÄÅÈªÑËâ≤„ÄÅËìùËâ≤‚ÄùÁ≠â„ÄÇ‰æãÂ¶ÇÁ∫¢Ëâ≤ÂíåÁªøËâ≤Â±û‰∫é‰∏çÂêåÁöÑËâ≤Áõ∏„ÄÇËâ≤Áõ∏ÊòØÁî±ÂÖâÁöÑÊ≥¢ÈïøÂÜ≥ÂÆöÁöÑ„ÄÇ È•±ÂíåÂ∫¶ (Saturation / chroma / colorfulness): È¢úËâ≤ÊúâÂ§ö‚ÄúÁ∫Ø‚ÄùÊàñÂ§ö‚ÄúÁÅ∞‚Äù ÂèàÁß∞Á∫ØÂ∫¶ÊàñËâ≥Â∫¶ÔºåÊèèËø∞È¢úËâ≤ÁöÑÈ≤úËâ≥Á®ãÂ∫¶„ÄÇÈ•±ÂíåÂ∫¶Ë∂äÈ´òÔºåÈ¢úËâ≤Ë∂äÈ≤úÊòéÔºõÈ•±ÂíåÂ∫¶Ë∂ä‰ΩéÔºåÈ¢úËâ≤Ë∂äÁÅ∞Êöó„ÄÅÂÅèÂêëÊó†ÂΩ©Ëâ≤ÔºàÂ¶ÇÁÅ∞Ëâ≤Ôºâ„ÄÇ ÊòéÂ∫¶ (Brightness / Lightness / Value): È¢úËâ≤ÊòØ‰∫ÆËøòÊòØÊöó Ë°®Á§∫È¢úËâ≤ÁöÑÊòé‰∫ÆÁ®ãÂ∫¶„ÄÇÊòéÂ∫¶È´òÁöÑÈ¢úËâ≤ÁúãËµ∑Êù•Êõ¥Êé•ËøëÁôΩËâ≤ÔºåÊòéÂ∫¶‰ΩéÁöÑÈ¢úËâ≤ÁúãËµ∑Êù•Êõ¥Êé•ËøëÈªëËâ≤„ÄÇ HSC (Hue-Saturation-Value) is a simple transformation of the RGB (red-green-blue) space. However, HSV colors capture the perceptual properties hue, colorfulness / saturation / chroma, and lightness / brightness / luminance/ value only poorly and consequently the corresponding palettes are typically not a good choice for statistical graphics and data visualization. EndRainbow. HCL (Hue-Chroma-Luminance) color space is believed to accords with human perception of color. HCL are much more suitable for capturing human color perception. ‚úÖ Color picking guidelines: Recoloring primary data, such as fluorescence images (ÈÅøÂÖç‰ΩøÁî®ËçßÂÖâËâ≤), to color-safe combinations such as green and magenta, turquoise and red, yellow and blue or other accessible color palettes is strongly encouraged. Use of the rainbow color scale should be avoided. Other figure guidelines Figures divided into parts should be labeled with a lower-case, boldface ‚Äòa‚Äô, ‚Äòb‚Äô, etc in the top-left corner. Labeling of axes, keys and so on should be in ‚Äòsentence case‚Äô (first word capitalized only) with no full stop. Units must have a space between the number and the unit, and follow the nomenclature common to your field. Commas should be used to separate thousands. Resources to find your preferred colors: HTML color names Preview color palettes: http://colrd.com/palette/19002/ my color space: recommended, good for generating color palettes Color picker tools: System Color Picker (Mac): Pick colors from anywhere using the built-in color picker. If you don‚Äôt want to install additional apps you can use this website, it allows you to upload an image and choose color from. Preview of classes of colormaps: Sequential: use a single hue (Âçï‰∏ÄËâ≤Áõ∏); the lightness and saturation value increases monotonically (‰∫ÆÂ∫¶ È•±ÂíåÂ∫¶); used for ordered data. Diverging: use two contrasting hues (ÂØπÊØîËâ≤); the lightness and saturation value increases monotonically from the center to the edges; used for data with a meaningful center point, such as topography or when the data deviates around zero. Cyclic: start and end on the same color, and meet a symmetric center point in the middle; should be used for values that wrap around at the endpoints, such as phase angle, wind direction, or time of day. Qualitative: often are miscellaneous colors; should be used to represent information which does not have ordering or relationships. Verify readability of colors. Colors should stand out (large contrast ratio), but not be too harsh on the eyes. Check WVAG contrast ratio for text Check the notion icon colors on both light and dark background Note that some times you need to convert colors from one format to another, e.g., from hexadecimal to RGB or vice versa. Use this Color conversion tool. Color exploration web: HTML color codes: https://htmlcolorcodes.com/color-chart/ Picker: can used to generate color palette given a start point using the following approaches. ÂèØÊ†πÊçÆÈúÄË¶Å‰∫ßÁîü‰∏çÂêåÁöÑËâ≤Êùø„ÄÇ complementary colors / ‰∫íË°•Ëâ≤ Ëâ≤ËΩÆ‰∏äÁõ∏ÂØπÁöÑÈ¢úËâ≤Ôºà‰æãÂ¶ÇÁ∫¢Ëâ≤ÂíåÁªøËâ≤ÔºâÔºåÂπ∂Âàó‰ΩøÁî®Êó∂‰ºö‰∫ßÁîüÂº∫ÁÉàÂØπÊØî„ÄÇ Triadic Colors / ‰∏âÂÖÉËâ≤ Âú®Ëâ≤ËΩÆ‰∏äÁ≠âÈó¥Ë∑ùÂàÜÂ∏ÉÁöÑ‰∏âÁßçÈ¢úËâ≤ÔºàÂ¶ÇÁ∫¢„ÄÅÈªÑ„ÄÅËìùÔºâÔºåÊó¢ÊúâÂØπÊØî‰πüÊúâÂçèË∞ÉÊÑü„ÄÇ Tetradic Colors / ÂõõÊñπËâ≤ (Âèå‰∫íË°•Ëâ≤) ‰∏§ÁªÑ‰∫íË°•Ëâ≤ÁªÑÊàêÁöÑÈÖçËâ≤ÊñπÊ°àÔºàÂ¶ÇÁ∫¢+Áªø Âíå Ëìù+Ê©ôÔºâÔºåÈ¢úËâ≤‰∏∞ÂØåÔºåÂ±ÇÊ¨°ÊÑüÂº∫„ÄÇ Analogous Colors / Á±ª‰ººËâ≤ Á±ª‰ººËâ≤ÊòØËâ≤ËΩÆ‰∏äÁõ∏ÈÇªÁöÑÈ¢úËâ≤ÔºàÂ¶ÇËìù„ÄÅËìùÁªø„ÄÅÁªøÔºâÔºåÈÖçËâ≤ÊÑüËßâÂíåË∞êÊüîÂíå„ÄÇ Neutral Colors / ‰∏≠ÊÄßËâ≤ Neutral schemes, like analogous harmonies, are formed by taking the colors on either side of a chosen color but at half the distance. While analogous schemes typically use colors 30 degrees apart, neutral harmonies use colors 15 degrees apart. Tones / Ëâ≤Ë∞É Ëâ≤Ë∞ÉÊòØÂ∞ÜÈ¢úËâ≤‰∏éÁÅ∞Ëâ≤ÔºàÈªëËâ≤ÂíåÁôΩËâ≤Ê∑∑ÂêàÔºâÊ∑∑ÂêàÂêéÂæóÂà∞ÁöÑÈ¢úËâ≤Ôºå‰ΩøÂéüËâ≤ÂèòÂæóÊüîÂíå„ÄÇ Shades / ÊöóËâ≤ (Âä†Èªë) A shade is a color mixed with black, making it darker. Tints / ÊµÖËâ≤ (Âä†ÁôΩ) A tint is a color mixed with white, making it lighter. Color Chart: flat design color chart [standard colors] HTML Color Names 7.2.1 Color models hexadecimal color ‚Äì RGB Hexadecimal is a base-16 number system used to describe color. Red, green, and blue are each represented by two characters (#rrggbb), start with #. Each character has 16 possible symbols: 0,1,2,3,4,5,6,7,8,9,A,B,C,D,E,F: i.e., red= #FF0000 , black=#000000, white = #FFFFFF The RGB is given as coordinates, for example, rgb(red, green, blue). The three different numbers in the RGB code is what determines the intensity of each of the colours red, green, and blue. The range of the intensity goes from 0 to 255. RGBA color values are an extension of RGB color values with an Alpha channel - which specifies the opacity for a color, e.g., rgb(red, green, blue, alpha). The alpha parameter is a number between 0.0 (fully transparent) and 1.0 (not transparent at all). Sometimes you see opaque used to denote transparency. Opacity is the opposite of transparency, 1 means opaque (ÂÆåÂÖ®‰∏çÈÄèÊòé), 0 means transparent (ÂÆåÂÖ®ÈÄèÊòé). The hex-colour is written with # and then a 6-digit code combining letters and numbers, e.g., #RRGGBB. It is a short code for RGB color. The first two numbers represent red, the middle two represent green, and the last two represent blue. Each can take values from 00 to FF, which is 0 to 255 in decimal. Sometimes you encounter 8-digit code, e.g., #RRGGBBAA. AA represents the alpha channel for transparency; takes value from 00 (full transparency, invisible) to FF (fully opaque, completely solid). Anything in between is semi-transparent. 7.2.2 Base R functions to specify colors RGB (red, green, blue): The default intensity scale in R ranges from 0-1; but another commonly used scale is 0‚Äì255. alpha decides transparency. rgb(r, g, b, maxColorValue=255, alpha=255) grDevices comes with the base installation and colorRamps must be installed. Each palette‚Äôs function has an argument for the number of colors and transparency (alpha): heat.colors(4, alpha=1) grDevices palettes: cm.colors, topo.colors, terrain.colors, heat.colors, rainbow. colorRampPalette(colors) returns functions that interpolate a set of given colors to create new color palettes (like topo.colors) and color ramps. cls &lt;- colorRampPalette(c(&quot;black&quot;,&quot;red&quot;,&quot;yellow&quot;,&quot;white&quot;)) levelplot(globalRAD[,,timeLayer], row.values=lonVec, column.values=latVec, main=paste(&quot;predicted radiation:&quot;,timeVals[timeLayer]), ylab=&quot;latitude&quot;,xlab=&quot;longitude&quot;, col.regions=cls(256), cuts=255, at=seq(0,450,length.out=256)) palette() 8 colors available in base R, col=1:8 responds to palette()[1:8] palette() # obtain the current palette palette(rainbow(6)) # six color rainbow palette(&quot;default&quot;) # reset back to the default An example: # set a palette of your choice palette(gray(seq(0,.9,len = 25))) # gray scales; print old palette # then you can use color by numeric vector to access colors in your palette matplot(outer(1:100, 1:30), type = &quot;l&quot;, lty = 1,lwd = 2, col = 1:30, main = &quot;Gray Scales Palette&quot;, sub = &quot;palette(gray(seq(0, .9, len=25)))&quot;) palette(&quot;default&quot;) # reset back to the default Commonly used colors: #FF0099 - 255, 0, 153 - PinkPurple #FF9900 - 255, 153, 0 - OrangeYellow #66CC00 - 102, 204, 0 - GreenBlue #9900FF - 153, 0, 255 - PurpleViolet #0099FF - 0, 153, 255 - BlueCyan #008B45 - 0, 204, 102 - BlueGreen #008B45FF DarkGreen Good on light, not good on dark theme #337ab7 DarkBlue Good on light, not good on dark theme Two series: c('darkred', 'steelblue2'); c(\"#00BFC4\",\"#F8766D\"), #00BFC4 is bluish, #F8766D is coral. Three lines: c('green','blue','red') Five lines: c('black', 'magenta', 'blue', 'cyan', 'red') (cyan is a greenish-blue color) # palette() get the current color palette # insert() insert colors at certain positions colvec &lt;- R.utils::insert(palette(), 6, c(&#39;steelblue&#39;,&#39;steelblue2&#39;)) colvec &lt;- insert(colvec, 3, &#39;darkred&#39;) colvec plot(1:length(colvec), 1:length(colvec), pch=19, col=colvec) cols &lt;- c(&quot;precip.cru&quot;=&quot;#00BFC4&quot;, &quot;precip.udel&quot;=&quot;#F8766D&quot;) labels &lt;- c(&quot;precip.cru.#00BFC4&quot;, &quot;precip.udel.#F8766D&quot;) p_global &lt;- ggplot(global_data_plot, aes(x=year, y=value, col=variable)) + geom_line() + labs(y=pre_label) + scale_color_manual(values=cols, labels = labels) + theme(legend.position = c(0.12, 0.9), legend.title = element_blank() ) 7.2.3 colorspace Package colorspace::hcl_palettes() retrieve all available palettes names. Useful functions to choose palette: pal &lt;- colorspace::choose_palette(gui='shiny') will give you an GUI interface to choose colors manually. colvec &lt;- colorspace::rainbow_hcl(5) automatically choose 5 colors from rainbow palette. colorspace provides 4 types of color palettes: qualitative, sequential single-hue, sequential multi-hue, diverging. Use sequential_hcl(), and diverging_hcl() to choose palettes. can use colorspace palette names to specify colors in ggplot. library(&quot;ggplot2&quot;) ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_density(alpha = 0.6) + scale_fill_discrete_qualitative(palette = &quot;Dark 3&quot;) use hcl_palettes(palette=\"Blue-Yellow\", plot=TRUE, n=5) to plot the color palette. Use diverge_hcl(n=5, palette=\"Blue-Yellow\") to print the colors names. &gt; diverge_hcl(5, &quot;Blue-Yellow&quot;) [1] &quot;#4F53B7&quot; &quot;#AAABD5&quot; &quot;#F1F1F1&quot; &quot;#B5AF80&quot; &quot;#6B6100&quot; &gt; hcl_palettes(palette=&quot;Blue-Yellow&quot;, plot=TRUE, n=5) use scales::show_col(colours) to preview colors. sequential_hcl(5, &quot;Blue-Yellow&quot;) # [1] &quot;#26A63A&quot; &quot;#9BB306&quot; &quot;#E1BB4E&quot; &quot;#FFC59E&quot; &quot;#F1F1F1&quot; sequential_hcl(5, &quot;Terrain&quot;) %&gt;% show_col() ggplot() color space, scale_colour_manual and scale_colour_gradient. ggplot_build() takes the plot object, and performs all steps necessary to produce an object that can be rendered. This function outputs two pieces: a list of data frames (one for each layer), and a panel object, which contain all information about axis limits, breaks etc. # get color scheme/palettes used in ggplot figure p_data &lt;- ggplot2::ggplot_build(p)$data[[1]] mask &lt;- !colnames(p_data) %in% c(&#39;x&#39;, &#39;y&#39;) p_color &lt;- p_data[,mask] %&gt;% distinct &gt; p_color # alpha size colour PANEL group linetype # 1 1.00 1.2 #F8766D 1 1 1 # 2 0.75 0.7 #C49A00 1 2 1 # 3 0.75 0.7 #53B400 1 3 1 # 4 0.75 0.7 #00C094 1 4 1 # 5 0.75 0.7 #00B6EB 1 5 1 # 6 0.75 0.7 #A58AFF 1 6 1 # 7 1.00 1.2 #FB61D7 1 7 1 Choose color around the color wheel n = 4 colVec = hue_pal()(n) plot(1:n, pch = 16, cex = 2, col = colVec) 7.2.4 RColorBrewer Sequential palettes names are Blues BuGn BuPu GnBu Greens Greys Oranges OrRd PuBu PuBuGn PuRd Purples RdPu Reds YlGn YlGnBu YlOrBr YlOrRd. All the sequential palettes are available in variations from 3 different values up to 9 different values. Diverging palettes are BrBG PiYG PRGn PuOr RdBu RdGy RdYlBu RdYlGn Spectral. All the diverging palettes are available in variations from 3 different values up to 11 different values. Suited to centered data with extremes in either direction. For temperature and radiation data visualization, it‚Äôs recommended to use diverging palettes from blue to red. Qualitative palettes, the lowest number of distinct values available always is 3, but the largest number is different for different palettes. { Accent 8 Dark2 8 Paired 12 Pastel1 9 Pastel2 8 Set1 9 Set2 8 Set3 12 } library(RColorBrewer) ## display a divergent palette RColorBrewer::display.brewer.pal(7,&quot;BrBG&quot;) ## display a qualitative palette display.brewer.pal(7,&quot;Accent&quot;) ## display a gradient color palette, used for sequential series RColorBrewer::brewer.pal(9,&quot;Blues&quot;) # show color names display.brewer.pal(9,&quot;Blues&quot;) # show a color strip From top to bottom, represent sequential, qualitative, and diverging color palettes, respectively. 7.2.4.1 ggplot + ColorBrewer scale_colour_brewer, scale_fill_brewer used with ggplot() to specify color palettes. scale_color_brewer(type=\"seq\", direction = 1) type One of \"seq\" (sequential), \"div\" (diverging) or \"qual\" (qualitative) direction = 1 Sets the order of colours in the scale. If 1, the default, colours are as output by RColorBrewer::brewer.pal(). If -1, the order of colours is reversed. 7.2.4.2 Reverse color palette Use rev(brewer_pal()) when specifying colors. library(ggplot2) library(RColorBrewer) ggplot(mtcars,aes(x = mpg, y = disp)) + geom_point(aes(colour = factor(cyl))) + scale_colour_manual(values = rev(brewer.pal(3, &quot;BuPu&quot;))) use factor to change the level order of the data. Note that here factor(-cyl) is used. ggplot(mtcars, aes(x = mpg, y = disp)) + geom_point(aes(colour = factor(-cyl))) + scale_colour_manual(values = brewer.pal(3, &quot;BuPu&quot;) ) 7.2.5 Grey Scale Grey scale is colour blind friendly and can still be distinguishable even if you print black and white. If you are intending a discrete colour scale to be printed in black and white, it is better to explicitly use scale_fill_grey() which maps discrete data to grays, from dark to light. start/end Êï∞ÂÄºË∂äÂ∞èÔºåÈ¢úËâ≤Ë∂äÊ∑±„ÄÇÊï∞ÂÄºË∂äÂ§ßÔºåÈ¢úËâ≤Ë∂äÊµÖ„ÄÇ 1: white; 0: black Default scales are from dark (0) to light (1) bars + scale_fill_grey() bars + scale_fill_grey(start = 0.5, end = 1) bars + scale_fill_grey(start = 0, end = 0.5) From light (1) to dark (0). Can just start at a larger number and end at a small number. bars + scale_fill_grey(start = 1, end = 0.5) bars + scale_fill_grey(start = 0.5, end = 0) 7.2.6 viridis The viridis package contains four sequential color scales: ‚ÄúViridis‚Äù (the primary choice) and three alternatives with similar properties (‚Äúmagma‚Äù, ‚Äúplasma‚Äù, and ‚Äúinferno‚Äù). viridis(n, alpha = 1, begin = 0, end = 1, direction = 1, option = \"D\") n The number of colors (‚â• 1) to be in the palette. alpha The alpha transparency, a number in [0,1], see argument alpha in hsv. begin, end The (corrected) hue in [0,1] at which the viridis colormap begins/ends. direction Sets the order of colors in the scale. If 1, the default, colors are ordered from darkest to lightest. If -1, the order of colors is reversed. option ‚Äúmagma‚Äù (or ‚ÄúA‚Äù), ‚Äúinferno‚Äù (or ‚ÄúB‚Äù), ‚Äúplasma‚Äù (or ‚ÄúC‚Äù), ‚Äúviridis‚Äù (or ‚ÄúD‚Äù, the default option) and ‚Äúcividis‚Äù (or ‚ÄúE‚Äù). ## Choose color from viridis # n &lt;- 20 # number of colors you want # mycolor &lt;- viridis(n, alpha = 1, begin = 0, end = 1, direction = 1, option = &quot;A&quot;) # mycolor[10] # show_palette(mycolor[10]) 7.2.7 ggsci Scientific Journal and Sci-Fi Themed Color Palettes for ggplot2 library(ggsci) # color palette functions library(scales) # display color palette in a panel scales::show_col(pal_aaas()(3)) # use `pal_aaas()` palette scales::show_col(pal_d3()(4)) # use `pal_d3()` palette # `npg` and `simpsons` look good show_col(pal_npg(alpha = 0.6)(9)) show_col(pal_simpsons(&quot;springfield&quot;)(9)) ggsci color palettes are available as ggplot2 scales. For all the color palettes, the corresponding scales are named as: scale_color_palname() scale_fill_palname() For instance, if you already have a ggplot object p1 and p2, you can add the nejm color palette to it by: p1_nejm &lt;- p1 + scale_color_nejm() p2_nejm &lt;- p2 + scale_fill_nejm() grid.arrange(p1_nejm, p2_nejm, ncol = 2) 7.2.8 Generate and Show Color Palettes gg_color_hue &lt;- function(n) { # generates a sequence of color hues = seq(15, 375, length=n+1) hcl(h=hues, l=65, c=100)[1:n] } &gt; gg_color_hue(2) # &quot;#F8766D&quot;(coral) &quot;#00BFC4&quot;(bluish) hcl(h = 0, c = 35, l = 85, alpha, fixup = TRUE) Create a vector of colors from vectors specifying hue, chroma and luminance. h The hue of the color specified as an angle in the range [0,360]. 0 yields red, 120 yields green 240 yields blue, etc. Ëâ≤Áõ∏ÔºåËâ≤Ë∞É (È¢úËâ≤ÁöÑÊÄßË¥®) c chrome (purity of intensy of color) ÊµìÊ∑°Â∫¶ÔºåËâ≤Â∫¶ l A value in the range [0,100] giving the luminance of the colour. ÊòéÊöóÂ∫¶ alpha numeric vector of values in the range [0,1] for alpha transparency channel (0 means transparent and 1 means opaque). fixup a logical value which indicates whether the resulting RGB values should be corrected to ensure that a real color results. if fixup is FALSE RGB components lying outside the range [0,1] will result in an NA value. hcl.colors(n, palette = \"viridis\", alpha = NULL, rev = FALSE) provides a basic and lean implementation of the pre-specified palettes in the colorspace package. Show color palette show_palette &lt;- function(colors) { n &lt;- length(colors) image(1:n, 1, as.matrix(1:n), col = colors, xlab = &quot;&quot;, ylab = &quot;&quot;, xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, bty = &quot;n&quot;) } show_palette(heat.colors(6)) # Or equivalently using `scales:: show_col(colors)` scales::show_col(colors) ## radiation color palette cold &lt;- colorRampPalette(c(&quot;#172DA6&quot;, &quot;#7AACED&quot;, &quot;white&quot;))(10) # from blue to white col_vec &lt;- c( &quot;#FFD4D4&quot;, &quot;#FF7F7F&quot;, &quot;#FF2A2A&quot;, &quot;#FF7F00&quot;, &quot;#FFD400&quot;) cl &lt;- colorRampPalette(col_vec) warm &lt;- cl(10+1)[c(1:5,8:11)] myColors &lt;- c(cold, warm) show_palette(myColors) ## use package RColorBrewer library(RColorBrewer) brewer.pal(11,&quot;Spectral&quot;) # generate a series of color display.brewer.pal(11,&quot;Spectral&quot;) # show a legend of colors 7.2.9 Color Interpolation colorRampPalette(colors, ...) interpolate a set of given colors to create new color palettes col_vec &lt;- c(&quot;#172DA6&quot;, &quot;#7AACED&quot;, &quot;white&quot;,&quot;#FFFF00&quot;,&quot;red&quot;) cl &lt;- colorRampPalette(col_vec) colors &lt;- cl(n) colorRampPalette(colors, bias = 1, space = c(\"rgb\", \"Lab\"), interpolate = c(\"linear\", \"spline\"), alpha = FALSE)) Color interpolation function. Interpolate a set of given colors to create new color palettes. colors vector of any of the three kinds, i.e., either a color name (as listed by colors() ), a hexadecimal string of the form \"#rrggbb\" or \"#rrggbbaa\" (see rgb), or a positive integer i meaning palette()[i]. bias a positive number. Higher values give more widely spaced colors at the high end. space character string; interpolation in RGB or CIE Lab color spaces. ## Gradient of n colors ranging from color 1 and color 2 start &lt;- &quot;#C6DBEF&quot; end &lt;- &quot;#08519C&quot; n &lt;- 10 colorRampPalette(c(start, end))(n) ## specify vector color with mapview # col.regions can be a function to interpolate colors tracts &lt;- tracts(&quot;IN&quot;, &quot;St. Joseph County&quot;, year = 2021) reds = colorRampPalette(c(&#39;pink&#39;, &#39;dark red&#39;)) mapview(tracts, zcol=&quot;AWATER&quot;, col.regions = reds, at=seq(0, 2514920, 500000)) "],["7.3-math-expression-in-figures.html", "7.3 Math Expression in Figures", " 7.3 Math Expression in Figures Main idea: to use expression or convert latex to expression; Two options expression(CO[2]) show math equation style as \\(CO_2\\). https://www.dataanalytics.org.uk/axis-labels-in-r-plots-using-expression/#expression_comm latex2exp::TeX(\"$\\\\alpha^\\\\beta$\") show as \\(\\alpha^{\\beta}\\); Trick: use \\\\, or \\\\; to show white space in math mode. \\\\; is a larger space than \\\\,. # There is an R package called latex2exp which may be helpful. It has function TeX which accepts some LaTeX expressions enclosed with dollar sign $ as in this example: library(latex2exp) library(ggplot2) qplot(1, &quot;A&quot;)+ ylab(TeX(&quot;Formula: $\\\\frac{2hc^2}{\\\\lambda^\\\\beta}$&quot;))+ xlab(TeX(&quot;$\\\\alpha$&quot;)) TeX only put the part that needs latex interpretation between $‚Ä¶$ , and there are several escape characters that needs to be carefully treated. Symbol TeX \\ \\\\ [ \\[ ] \\] Check out other escaping characters: https://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/plotmath.html TeX in legend scales, the use of unname() is necessary: # have to use unname(TeX(&quot;$...$&quot;)) labels=c(&quot;CRU&quot;, unname( TeX(&quot;$Burke \\\\times 10$&quot;) ) ) When using TeX inside geom_text(aes(x, y, label=TeX(\"\", output = \"character\") ), parse=TRUE ), specifying the output of TeX() as character, although ‚Äúcharacter‚Äù is not one of the values that the output argument can take, and turning the parse argument in geom_text() to TRUE. Using variables in aes aes_string(x=\"TCS_reported\", y=as.name(target_v)) target_v can be a variable which will be parsed in aes; as.name() first coerces its argument internally to a character vector; then takes the first element and returns a symbol of that name. A name (also known as a ‚Äòsymbol‚Äô) is a way to refer to R objects by name (rather than the value of the object, if any, bound to that name). It will escape special characters that are otherwise reserved or illegal; equivalent to 'target_v' target_v &lt;- &quot;#Obs&quot; aes_string(x=&quot;TCS_reported&quot;, y=as.name(target_v)) # or equivalently replace `as.name()` with backticks `` target_v &lt;- &quot;`#Obs`&quot; aes_string(x=&quot;TCS_reported&quot;, y=target_v) # or using get() aes(x=TCS_reported, y=get(target_v)) "],["7.4-control-graphics-devices.html", "7.4 Control Graphics Devices", " 7.4 Control Graphics Devices dev.cur() returns a length-one named integer vector giving the number and name of the active device, or 1, the null device, if none is active. dev.new() opens a new RStudioGD device. dev.list() returns the numbers of all open devices, except device 1, the null device. This is a numeric vector with a names attribute giving the device names, or NULL is there is no open device. Usually 2 is RStudioGD 2. dev.set(which = dev.next()) makes the specified device the active device. If there is no device with that number, it is equivalent to dev.next. If which = 1 it opens a new device and selects that. dev.prev(which = dev.cur()) dev.next and dev.prev select the next/previous open device in the appropriate direction, unless no device is open. dev.off(which = dev.cur()) shuts down the specified (by default the current) device. graphics.off() shuts down all open graphics devices. Save the current graphic object library(grid) grab &lt;- grid.grab() ggsave(grab, filename = f_name, width=10.9, height=5.82) pdf device uses inches to specify width and height. png and jpeg use pixels to specify dimensions. 7.4.1 Save PNG plot_png &lt;- function(p, f_name, width, height, ppi=300){ # a plot wrapper png(f_name, width=width*ppi, height=height*ppi, res=ppi) print (p) dev.off() } Sometimes the title got cut off. You can change oma to add more top margin. If it doesn‚Äôt help, change the dimension of your figure, make it higher by changing the height of the figure output. par(oma=c(bottom, left, top, right)) only works for base-R graphics; use theme(plot.margin = margin(t=7, b=7, r=12, l=7, unit=\"pt\") ) to add more margins to ggplot object. You can check the dimension of your figure by right-click on the plot, then ‚ÄúCopy Image Address‚Äù, paste the address somewhere, you get the width and height information in the address. You can use this aspect ratio as a start. unit: px. Don‚Äôt make the figure too big as the title, axis ticks, will become too small to see in proportion. Forward display x11() opens an interactive graphics device. x11 is Apple‚Äôs X server. It is reliable and provides hardware OpenGL acceleration. https://www.rdocumentation.org/packages/grDevices/versions/3.6.2/topics/x11 https://www.cgl.ucsf.edu/chimera/data/downloads/1.11.2/mac_x11.html When you are using a native Mac application (not X windows) and then click on a menu within the X window, the mouse click does not bring up the menu. It just activates the X window and another mouse click is needed to show the menu. To make this work with a single mouse click, use the Mac X server wm_click_through preference by typing the following command in a Mac Terminal window: defaults write org.x.x11 wm_click_through -bool true Disables the default behavior of swallowing window-activating mouse events. Normally Mac OS X swallows window-activating mouse events. This preference causes a window-activating mouse click on an X window to also be processed by the application. X11 must be restarted after any of these settings are changed. The settings are saved in your ~/Library/Preferences/org.x.x11.plist file, so they will apply to future sessions. Reissuing the commands with false instead of true will restore the default preference settings. An item that provides click-through is one that a user can activate with one click, even though the item is in an inactive window. (To activate an item that does not support click-through, the user must first make the containing window active and then click the item.) Although click-through can make some user tasks easier, it can also confuse users if they click items unintentionally. x11 preferences Input windows security Bug: X11 windows do not raise (come to the front) when the application is activated. http://hints.macworld.com/article.php?story=20050714011418999 bind a new key to an AppleScript mac Opt+Tab https://apple.stackexchange.com/questions/175215/how-do-i-assign-a-keyboard-shortcut-to-an-applescript-i-wrote For the Quartz device, you can use quartzFonts() to see what the default font for each of these keywords is quartzFonts() # $serif # [1] &quot;Times-Roman&quot; &quot;Times-Bold&quot; &quot;Times-Italic&quot; &quot;Times-BoldItalic&quot; # # $sans # [1] &quot;Helvetica&quot; &quot;Helvetica-Bold&quot; &quot;Helvetica-Oblique&quot; &quot;Helvetica-BoldOblique&quot; # # $mono # [1] &quot;Courier&quot; &quot;Courier-Bold&quot; &quot;Courier-Oblique&quot; &quot;Courier-BoldOblique&quot; Table of font http://www.cookbook-r.com/Graphs/Fonts/ Font support in R is generally not very good. It varies between systems, and between output formats. Sometimes what you see on the screen isn‚Äôt necessarily the same as what you get when you output to PNG or PDF. PNG output has less suport for font variety; PDF has better support. Short Name Canonical Name mono Courier sans Helvetica serif Times AvantGarde Bookman Helvetica-Narrow NewCenturySchoolbook Palatino URWGothic URWBookman NimbusMon URWHelvetica NimbusSan NimbusSanCondNimbusSanCond CenturySchCenturySch URWPalladio URWTimes NimbusRom extrafont package https://github.com/wch/extrafont "],["7.5-ggplot.html", "7.5 ggplot", " 7.5 ggplot Reference: https://ggplot2.tidyverse.org/reference/index.html Change global settings theme_set(theme_bw()) # change default theme to theme_bw() globally You start with ggplot(), supply a dataset and aesthetic mapping (with aes()). You then add on layers (like geom_point() or geom_histogram()), scales (like scale_colour_brewer()), faceting specifications (like facet_wrap()) and coordinate systems (like coord_flip()). Default theme: theme_gray() grey background and white gridlines. ggplot(data = NULL, mapping = aes(), ...) is used to construct the initial plot object, and is almost always followed by a plus sign (+) to add components to the plot. Doc. The data = and mapping = specifications in the arguments are optional (and are often omitted in practice), so long as the data and the mapping values are passed into the function in the right order. There are three common patterns used to invoke ggplot(): ggplot(data = df, mapping = aes(x, y, other aesthetics)) Recommended if all layers use the same data and the same set of aesthetics, although this method can also be used when adding a layer using data from another data frame. ggplot(data = df) This is useful when one data frame is used predominantly for the plot, but the aesthetics vary from one layer to another. Different x/y-axis. ggplot() This is useful when multiple data frames are used to produce different layers, as is often the case in complex graphics. Create a customized figure theme code snippet that could be used repetitively. theme() note that when you call it, just use + mytheme w/o parentheses as it is not a function ‚Äî it is a theme setting. mytheme &lt;- theme(legend.position = &quot;none&quot;, # disable legend legend.spacing.y = unit(0, &#39;mm&#39;), # spacing between legend title and legend items legend.key.height = unit(0.8,&quot;line&quot;), # vertical spacing between legend items legend.margin = margin(t=0, b=0, unit=&quot;mm&quot;), # legend box margins title = element_text(size=8), axis.title = element_text(size=rel(1.2)), # use `rel()` to change proportionally to base font size; or a number to specify absolute size as follows; axis.text = element_text(size=8), # tick labels along axes panel.grid.minor = element_blank() # remove minor gridlines ) # p is a ggplot() subject p + mytheme rel(x) specify sizes relative to the parent. theme_bw(base_size = 14) the default font size is 11 pt, which can be too small. Set base_size=14 to enlarge the font size for a specific theme. theme_get() returns the current active theme. # get default values of theme parameters theme_get()$plot.margin Check default options for a specific theme theme_bw()$legend.spacing Legend layout p + guides(colour = guide_legend(nrow = 1)) 7.5.0.1 Theme Explore themes: https://ggplot2.tidyverse.org/reference/ggtheme.html default: theme_gray() (with grey background) normal: theme_bw() (recommended ÁªìÊûÑÊ∏ÖÊô∞) coordinates: theme_minimal() (no axis borders ÊûÅÁÆÄÈ£é) Q: Ê≤°ÊúâËÉåÊôØÂ°´ÂÖÖËâ≤ÔºåÊ∑±Ëâ≤Ê®°Âºè‰∏ãÈ¢ÑËßàÂõ∞Èöæ„ÄÇ A: use theme to apply white background: theme(plot.background = element_rect(fill = &quot;white&quot;, color = NA)) plot.background Background of the entire plot area. panel.background Background of the panel area (the area where the data is plotted). with no grid: theme_classic() (hard to read) 7.5.1 Wide table to Long table tidyr::gather(data, key = \"key\", value = \"value\", ...) convert data frame to key-value long format. ... is a selection of columns. If empty, all variables are selected. You can supply bare variable names, select all variables between x and z with x:z, exclude y with -y. key one identifier column that you use to identify groups, store the names of columns that you want to gather/stack; value one value column name that put values in key columns; ... specification of columns to gather/stack. Allowed values are: variable names, put them just in a sequence, do not need to wrap it in a vector if you want to select all variables between a and e, use a:e. Or could use position index e.g., 1:3. if you want to exclude a column name y use -y, usually the index/identifier of the columns. df %&gt;% pivot_longer(c(x, y, z), names_to = &quot;key&quot;, values_to = &quot;value&quot;) is equivalent to df %&gt;% gather(\"key\", \"value\", x, y, z), more recommended as gather is deprecated now. Long table to wide table spread, reverses gather() spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE, sep = NULL) key the column in the long table that will become factor/columns in wide table; the number of new columns equals to the number of categories in key; the columns are filled by value; fill what value to fill in the new columns when there is no data for that category; default to NA; It is recommended switching to pivot_wider(), which is easier to use, more featureful, and still under active development. pivot_wider(data, names_from = key, values_from = value, names_prefix = &quot;&quot;) names_prefix optional prefix to add to the names of the new columns, useful when you want to avoid name conflicts with existing columns. df %&gt;% spread(key, value) # equivalent to df %&gt;% pivot_wider(names_from = key, values_from = value) df &lt;- data.frame(month=rep(1:3,2), student=rep(c(&quot;Amy&quot;, &quot;Bob&quot;), each=3), A=c(9, 7, 6, 8, 6, 9), B=c(6, 7, 8, 5, 6, 7)) # month student A B # 1 1 Amy 9 6 # 2 2 Amy 7 7 # 3 3 Amy 6 8 # 4 1 Bob 8 5 # 5 2 Bob 6 6 # 6 3 Bob 9 7 ## 1. construct long table df %&gt;% gather(variable, value, -(month:student)) # month student variable value # 1 1 Amy A 9 # 2 2 Amy A 7 # 3 3 Amy A 6 # 4 1 Bob A 8 # 5 2 Bob A 6 # 6 3 Bob A 9 # 7 1 Amy B 6 # 8 2 Amy B 7 # 9 3 Amy B 8 # 10 1 Bob B 5 # 11 2 Bob B 6 # 12 3 Bob B 7 ## 2. create identifier column that will become columns in the `spread` step df %&gt;% gather(variable, value, -(month:student)) %&gt;% unite(temp, student, variable) # month temp value # 1 1 Amy_A 9 # 2 2 Amy_A 7 # 3 3 Amy_A 6 # 4 1 Bob_A 8 # 5 2 Bob_A 6 # 6 3 Bob_A 9 # 7 1 Amy_B 6 # 8 2 Amy_B 7 # 9 3 Amy_B 8 # 10 1 Bob_B 5 # 11 2 Bob_B 6 # 12 3 Bob_B 7 ## 3. `spread` long table, spread the column containing the factor/identifier info df %&gt;% gather(variable, value, -(month:student)) sub%&gt;% unite(temp, student, variable) %&gt;% # first unite spread(temp, value) # then spread # month Amy_A Amy_B Bob_A Bob_B # 1 1 9 6 8 5 # 2 2 7 7 6 6 # 3 3 6 8 9 7 unite(data, col, ..., sep = \"_\", remove = TRUE, na.rm = FALSE) paste together multiple columns into a single column or variable (often used as factor identifier for long table); return a string column. annotate(geom = 'text', label = 'Africa', x = Inf, y = Inf, hjust = 2, vjust = 2) Add text/annotation to a designated position. Justification (hjust, vjust): Horizontal and vertical justification have the same parameterisation, either a string (‚Äútop‚Äù, ‚Äúmiddle‚Äù, ‚Äúbottom‚Äù, ‚Äúleft‚Äù, ‚Äúcenter‚Äù, ‚Äúright‚Äù) or a number between 0 and 1: top = 1, middle = 0.5, bottom = 0 left = 0, center = 0.5, right = 1 Note that you can use numbers outside the range (0, 1), but it‚Äôs not recommended. If label is a TeX expression, then should use annotate(&quot;text&quot;, x=3, y=40, label=TeX(eqn, output=&quot;character&quot;), hjust=0, size = 4, parse = TRUE) 7.5.2 Dual y-axis plot rescaleY &lt;- function(y1, y2){ # useful for plotting figures with dual axis but with different range # y1 is the primary axis # target to # y2 is the secondary axis # origin from # rescale y2 to match y1 range # return a-intercept, b-slope ylim1 &lt;- c(min(y1), max(y1)) ylim2 &lt;- c(min(y2), max(y2)) b &lt;- (ylim1[2]-ylim1[1])/(ylim2[2]-ylim2[1]) a &lt;- ylim1[1]-b*ylim2[1] return (c(a,b)) } scaleFactor &lt;- rescaleY(the_model_df$temp, the_model_df$rsds) # rescale rsds to match the range with temp dev.new() p &lt;- ggplot(the_model_df, aes(x = yr)) + geom_line(aes(y = temp, colour = &quot;tmp&quot;)) + # bluish geom_line(aes(y = scaleFactor[1]+rsds*scaleFactor[2], colour = &quot;rsds&quot;)) + # orange scale_color_discrete(name = &quot;Y series&quot;, # legend name values = c(&quot;tmp&quot;=&quot;#00BFC4&quot;, &quot;rsds&quot;=&quot;#F8766D&quot;), # named vector for color scale, specifying colors you want for each series breaks = c(&quot;tmp&quot;, &quot;rsds&quot;), # specify order of scales showing up in legend labels = c(&quot;temperature&quot;, &quot;radiation&quot;) # names/text show up at scales ) + scale_y_continuous(name = &quot;Temperature [K]&quot;, sec.axis = sec_axis(~(.-scaleFactor[1])/scaleFactor[2], name = &quot;Radiation [Wm^-2]&quot;)) + labs(x=&#39;year&#39;, title=the_model) + theme(legend.title = element_blank(), # remove legend title legend.text = element_text(size=8), # change legend font size to smaller # axis.title.x = element_blank(), # remove x axis title axis.title.y.right=element_text(color=&quot;#F8766D&quot;), # y axis label axis.text.y.right=element_text(color=&quot;#F8766D&quot;)) p # save to file ggsave(&quot;name.png&quot;, path=fig_dir, width=8.93, height=5.74, units=&quot;in&quot;, dpi=300) f_name &lt;- paste0(&quot;./figures/&quot;, &quot;pre_2.png&quot;) f_name ppi &lt;- 300 png(f_name, width=7.96*ppi, height=4.19*ppi, res=ppi) print (p) dev.off() plot_png &lt;- function(p, f_name, width, height, ppi=300){ # a plot wrapper png(f_name, width=width*ppi, height=height*ppi, res=ppi) print (p) invisible(dev.off()) } The second axis could be either the second x or y axis. sec.axis(trans = NULL,name = waiver(), breaks = waiver(), labels = waiver(), guide = waiver()) trans A formula or function of transformation from right to left. name The name/title of the secondary axis. labels A character vector giving labels (must be same length as breaks); or a function that takes the breaks as input and returns labels as output dup.axis(trans = ~.,name = waiver(), breaks = waiver(), labels = waiver(), guide = waiver()) is provide as a shorthand for creating a secondary axis that is a duplication of the primary axis, effectively mirroring the primary axis. dual axes rescale for dual axes, a major problem is to rescale the second axis so that two axes could be visualized normally. https://stackoverflow.com/questions/31953747/r-ggplot2-dual-y-axis-facet-wrap-one-histogram-and-other-line scale_*_log10(), scale_*_sqrt() and scale_*_reverse() are useful for axis transformation. The principal components of every plot can be defined as follow: data is a data frame Aesthetics is used to indicate x and y variables. It can also be used to control the color, the size or the shape of points, the height of bars, fill (‚Äúinside‚Äù color), linetype, etc‚Ä¶.. Geometry defines the type of graphics (histogram, box plot, line plot, density plot, dot plot, ‚Ä¶.) In most cases you start with ggplot(), supply a dataset and aesthetic mapping (with aes()). You then add on layers (like geom_point() or geom_histogram()), scales (like scale_colour_brewer()), faceting specifications (like facet_wrap()) and coordinate systems (like coord_flip()). library(ggplot2) ggplot(mpg, aes(displ, hwy, colour = class)) + # plot geom_point() # layer aes(x, y, ...) function that sets aesthetic mappings. aes() can be specified either in plot or in layers. color when in bar plot, color means for border, if you want to sepecify bar, you should use fill 7.5.3 aes() Aesthetic mappings aes(x, y, ...) describe how variables in the data are mapped to visual properties (aesthetics) of geoms. Aesthetic mappings can be set in ggplot() and in individual layers. aes() is a quoting function. This means that its inputs are quoted to be evaluated in the context of the data. This makes it easy to work with variables from the data frame because you can name those directly. The flip side is that you have to use quasiquotation to program with aes(). See a tidy evaluation tutorial such as the dplyr programming vignette to learn more about these techniques. Specifying the aesthetics in the plot vs.¬†in the layers https://ggplot2-book.org/layers.html If you only have one layer in the plot, the way you specify aesthetics doesn‚Äôt make any matter. However, the distinction is important when you start adding additional layers. These two plots are both valid and interesting, but focus on quite different aspects of the data: ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point() + geom_smooth(se = FALSE) ggplot(mpg, aes(displ, hwy)) + geom_point(aes(colour = class)) + geom_smooth(se = FALSE) Not easy way to know the most correct method, only trial and error‚Ä¶ üòå Generally, you want to set up the mappings to illuminate the structure underlying the graphic and minimise typing. It may take some time before the best approach is immediately obvious, so if you‚Äôve iterated your way to a complex graphic, it may be worthwhile to rewrite it to make the structure more clear. Default method for geom_smooth: spline smoothing. geom_smooth(stat = &#39;smooth&#39;, color = &#39;Red&#39;, method = &#39;gam&#39;, formula = y ~ s(x, bs = &quot;cs&quot;)) Setting vs.¬†mapping Instead of mapping an aesthetic property to a variable, you can set it to a single value by specifying it in the layer parameters. We map an aesthetic to a variable (e.g., aes(colour = cut)) or mapping will generate each layer for every class in cut set it to a constant (e.g., colour = \"red\"). (See par() for a mapping list) assign values for aesthetics # specifying aesthetics inside `aes()` or outside is very different # outside `aes()`, passing aesthetic arguments by variable-value pairs ggplot(mpg, aes(cty, hwy)) + geom_point(colour = &quot;darkblue&quot;) # inside `aes()` # maps (not sets) the colour to the value ‚Äòdarkblue‚Äô. This effectively creates a new variable containing only the value ‚Äòdarkblue‚Äô and then scales it with a colour scale. Because this value is discrete, the default colour scale uses evenly spaced colours on the colour wheel, and since there is only one value this colour is pinkish. ggplot(mpg, aes(cty, hwy)) + ggplot(mpg, aes(cty, hwy)) + geom_point(aes(colour = &quot;darkblue&quot;)) A third approach is to map the value, but override the default scale: ggplot(mpg, aes(cty, hwy)) + geom_point(aes(colour = &quot;darkblue&quot;)) + scale_colour_identity() # Use this set of scales when your data has already been scaled, i.e. it already represents aesthetic values that ggplot2 can handle directly. The functions scale_colour_identity(), scale_fill_identity(), scale_size_identity(), etc. work on the aesthetics specified in the scale name: colour, fill, size, etc. Use this set of scales when your data has already been scaled, i.e.¬†it already represents aesthetic values that ggplot2 can handle directly. This is most useful if you always have a column that already contains colours. assign names for multiple layers It‚Äôs sometimes useful to map aesthetics to constants. For example, if you want to display multiple layers with varying parameters, you can ‚Äúname‚Äù each layer through ‚Äúcolor/colour‚Äù: ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(aes(colour = &quot;loess-name&quot;), method = &quot;loess&quot;, se = FALSE) + # a layer called &quot;loess-name&quot;, generating a colour scale named &quot;loess-name&quot;, not actually assigning colors to the series. Need to assign color through a color scale, ex. `scale_color_discrete()`. geom_smooth(aes(colour = &quot;lm-name&quot;), method = &quot;lm&quot;, se = FALSE) # a layer called &quot;lm-name&quot; aes() also standardises aesthetic names by converting color to colour (also in substrings, e.g., point_color to point_colour) and translating old style R names to ggplot names (e.g., pch to shape and cex to size). Continuous scales for data (customize x &amp; y axis) Define your own axis preferences, breaks, limits, labels, ‚Ä¶ scale_x_continuous() and scale_y_continuous() are the default scales for continuous x and y aesthetics. There are three variants that set the trans argument for commonly used transformations: scale_*_log10(), scale_*_sqrt() and scale_*_reverse(). scale_x_continuous(name = waiver(),breaks = waiver(), minor_breaks = waiver(),n.breaks = NULL,labels = waiver(), limits = NULL,expand = waiver(),oob = censor, na.value = NA_real_, trans = \"identity\", guide = waiver(),position = \"bottom\", sec.axis = waiver() ) limits¬† NULL to use the default scale range. e.g., c(0,1) A numeric vector of length two providing limits of the scale. Use NA to refer to the existing minimum or maximum. A function that accepts the existing (automatic) limits and returns new limits. Also accepts rlang lambda function notation. Note that setting limits on positional scales will remove data outside of the limits. If the purpose is to zoom, withouting clipping, use the limit argument in the coordinate system (see coord_cartesian(xlim=c(0, 100), ylim=c(10, 20))). alternatively, could use + xlim(0, 100) + ylim(10, 20) to achieve the same effects. This is the same as scale_x_continuous(limits=c(0, 100)) + scale_y_continuous(limits=c(10, 20)) breaks NULL for no breaks waiver() for the default breaks computed by the transformation object A numeric vector of positions. Note that the vector will be cut off if the range exceeds the data coverage. A function that takes the limits as input and returns breaks as output (e.g., a function returned by scales::extended_breaks()). Also accepts rlang lambda function notation. Other position scales: scale_x_binned(), scale_x_date(), scale_x_discrete(). scale_x_date() : class Date scale_x_datetime() : class POSIXct scale_x_datetime(labels = scales::date_format(&quot;%Y&quot;, tz = &quot;CET&quot;), breaks = seq(as.POSIXct(&quot;1960-12-31 01:00:00 CET&quot;), as.POSIXct(&quot;2015-02-11 01:00:00 CET&quot;), &quot;10 years&quot;) ) scales::date_format(\"%Y\", tz = \"CET\") is a wrapper for formatting dates on the axis. scale_x_time() : class hms scale_(x|y)_binned() are scales that discretize continuous position data. You can use these scales to transform continuous inputs before using it with a geom that requires discrete positions. An example is using scale_x_binned() with geom_bar() to create a histogram. scale_*_manual(..., values, breaks = waiver()) specify your own set of mappings from levels in the data to aesthetic values. * could be one of color, fill, size, shape, linetype, alpha, discrete ; when using discrete, have to specify aesthetics scale_colour_manual(..., values, aesthetics = \"colour\", breaks = waiver()) ... Arguments passed on to discrete_scale, which is a discrete scale constructor. palette A palette function that when called with a single integer argument (the number of levels in the scale) returns the values that they should take. limits A character vector that defines possible values of the scale and their order drop Should unused factor levels be omitted from the scale? The default, TRUE, uses the levels that appear in the data; FALSE to keep all the levels in the factor. often set to drop=FALSE na.value what aesthetic value should the missing values be displayed as? Useful to remove grey NA value area in figures. often set to na.value = NA breaks A character vector of breaks labels A character vector giving scale labels (must be same length as breaks) guide A function used to create a guide or its name. guide = \"legend\" in scale_* is syntactic sugar for guide = guide_legend() (e.g.¬†scale_color_manual(guide = \"legend\")). As for how to specify the guide for each scale in more detail, see guides(). Guides can be specified in each scale_* or in guides(). name The name of the scale. Used as the axis or legend title. values a set of aesthetic values to map data values to. The values will be matched in order (usually alphabetical) with the limits of the scale, or with breaks if provided. If this is a named vector, then the values will be matched based on the names instead. Data values that don‚Äôt match will be given na.value. aesthetics Character string or vector of character strings listing the name(s) of the aesthetic(s) that this scale works with. This can be useful, for example, to apply colour settings to the colour and fill aesthetics at the same time, via aesthetics = c(\"colour\", \"fill\"). scale_fill_brewer(palette=\"Dark2\") use brewer color palettes. scale_fill_grey() Use grey scale discrete_scale Discrete Scale Constructor. discrete_scale(aesthetics, scale_name, palette, name = NULL, breaks = waiver(), labels = waiver(), legend = NULL, limits = NULL, expand = waiver(), na.value = NA, drop = TRUE, guide = \"legend\") common discrete scale parameters: name, breaks, labels, na.value, limits and guide. p &lt;- ggplot(mtcars, aes(mpg, wt)) + geom_point(aes(colour = factor(cyl))) p + scale_colour_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;)) # It&#39;s recommended to use a named vector cols &lt;- c(&quot;8&quot; = &quot;red&quot;, &quot;4&quot; = &quot;blue&quot;, &quot;6&quot; = &quot;darkgreen&quot;, &quot;10&quot; = &quot;orange&quot;) p + scale_colour_manual(values = cols) # You can set color and fill aesthetics at the same time ggplot( mtcars, aes(mpg, wt, colour = factor(cyl), fill = factor(cyl)) ) + geom_point(shape = 21, alpha = 0.5, size = 2) + scale_colour_manual( values = cols, aesthetics = c(&quot;colour&quot;, &quot;fill&quot;) ) # or you could just choose to plot a subset of category, ex. drop &quot;10&quot; # As with other scales you can use breaks to control the appearance/order # of the legend. p + scale_colour_manual( # name = &quot;Y series&quot;, # specify legend name here values = cols, breaks = c(&quot;4&quot;, &quot;6&quot;, &quot;8&quot;), labels = c(&quot;four&quot;, &quot;six&quot;, &quot;eight&quot;) ) # And limits to control the possible values of the scale p + scale_colour_manual(values = cols, limits = c(&quot;4&quot;, &quot;8&quot;)) p + scale_colour_manual(values = cols, limits = c(&quot;4&quot;, &quot;6&quot;, &quot;8&quot;, &quot;10&quot;)) Enlarge geom_point() dot size by setting size=3, default to 1. # Set aesthetics to fixed value ggplot(mtcars, aes(wt, mpg)) + geom_point(colour = &quot;red&quot;, size = 3) ggplot(aes(x = Sepal.Length), data = iris) + geom_histogram(color = &#39;black&#39;, fill = NA) + geom_vline(aes(xintercept=median(iris$Sepal.Length), color=&quot;median&quot;), linetype=&quot;dashed&quot;, size=1) + geom_vline(aes(xintercept=mean(iris$Sepal.Length), color=&quot;mean&quot;), linetype=&quot;dashed&quot;, size=1) + scale_color_manual(name = &quot;statistics&quot;, values = c(median = &quot;blue&quot;, mean = &quot;red&quot;)) Reference lines: horizontal, vertical, and diagonal geom_hline(slope, intercept), geom_vline(xintercept), geom_abline(yintercept) add reference lines (sometimes called rules) to a plot, either horizontal, vertical, or diagonal (specified by slope and intercept). 7.5.4 Add regression line geom_smooth(mapping = NULL,data = NULL, stat = \"smooth\",position = \"identity\", ..., method = NULL,formula = NULL,se = TRUE, na.rm = FALSE,orientation = NA, show.legend = NA,inherit.aes = TRUE) Addds a trend line over an existing plot. By default, it uses a LOESS smooth line. If you want a straight ‚Äúlinear model‚Äù line, you can use method=lm. method Smoothing method (function) to use, accepts either NULL or a character vector, e.g.¬†\"lm\" (linear model), \"glm\", \"gam\", \"loess\" or a function, e.g.¬†MASS::rlm or mgcv::gam, stats::lm, or stats::loess. \"auto\" is also accepted for backwards compatibility. It is equivalent to NULL. Defaults to loess (Locally Estimated Scatterplot Smoothing) when there are fewer than 1000 observations, and a gam when there are more observations. loess method stands for local regression fitting. formula Formula to use in smoothing function, eg. y ~ x, y ~ poly(x, 2), y ~ log(x). NULL by default, in which case method = NULL implies formula = y ~ x when there are fewer than 1,000 observations and formula = y ~ s(x, bs = \"cs\") otherwise. se=TRUE Whether to show the uncertainty band. level=.95 Level of confidence interval (CI) to use (0.95 by default). color color for the regression line fill color for the CI alpha transparency for the CI show.legend logical. Should this layer be included in the legends? NA, the default, includes if any aesthetics are mapped. FALSE never includes, and TRUE always includes. It can also be a named logical vector to finely select the aesthetics to display. # fit a linear regression geom_smooth(data=subset(long_format,key==&quot;values_ma&quot;), aes(color=&#39;lm1&#39;, linetype=&#39;lm1&#39;, size=&#39;lm1&#39;), method=lm, se=FALSE) # fit a quadratic function ggplot(CEF, aes(x=engine.size, y=mean_city.distance)) + geom_point() + geom_smooth(method=&#39;lm&#39;, formula = y ~ x + I(x^2), size = 1, se=FALSE) + theme(axis.text = element_text(size=rel(1.5)), axis.title = element_text(size=rel(1.5))) # fit a degree 3 polynomial regression p + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 3), se = FALSE) Add regression equation to scatter plot ggpubr::stat_regline_equation(label.y=NULL) library(ggpubr) # add regression eq to figure ggplot(df, aes_string(x=&quot;true value&quot;, y=&#39;predicted value&#39;)) + geom_point(shape=1) + # Use hollow circles, default solid dot geom_text(aes(label=ifelse(Model %in% c(&#39;GFDL-ESM4&#39;, &#39;MIROC6&#39;), as.character(Model),&#39;&#39;)), hjust=-0.05, vjust=0, size=2) + # add text to outliers labs(subtitle=&quot;predicted v.s. true values&quot;, x=&#39;Simple global TCS [K]&#39;, y=&#39;Converted global TCS [K]&#39;) + geom_smooth(method=lm, se=TRUE) + geom_abline(intercept=0, slope=1, size=0.5, color=&#39;red&#39;, linetype=&quot;dashed&quot;) + stat_regline_equation(label.y=3.3) + # add regression equation theme_bw() geom_text(data, mapping, check_overlap = FALSE, ...) add text to the plot ggplot(mtcars, aes(wt, mpg)) + geom_text(aes(label = rownames(mtcars))) ggrepel::geom_text_repel(aes(label=Model), size=3.5, fontface=\"bold\") avoid overlap among labels. geom_label() works similar to geom_text, except for that text is wrapped in a box. Q: How to remove ‚Äòa‚Äô from legend when using aesthetics and geom_text? A: Set show.legend = FALSE in geom_text. legends for geom_text can only be called via color. If color is used before, then in order to keep the current color scheme, we have to add a new color scale, using ggnewscale::new_scale_color()+ and carrying on what you have to do afterwards. https://stackoverflow.com/questions/59091627/add-new-legend-for-geom-text-with-text-labels-as-legend-key library(ggplot2) library(grid) library(ggnewscale) pfda_plot &lt;- ggplot(data=pfdavar,aes(x=X1,y=X2,group=groups))+ geom_point(aes(colour=groups))+ geom_polygon(data=hulls,alpha=0.2,aes(fill=groups))+ xlab(&quot;pFDA1&quot;)+ ylab(&quot;pFDA2&quot;)+ theme_classic()+ theme(legend.title=element_blank())+ new_scale_color()+ # define a ne color scheme geom_text(aes(label=labels,col=Species), fontface=1,hjust=0,vjust=0,size=3)+ scale_color_manual(values=rep(&quot;black&quot;,18)) The above gives you something close, just that it is all ‚Äòa‚Äô for geom_text legend. What we need to do now, is change the default ‚Äòa‚Äô, and for this I used @MarcoSandri‚Äôs solution to change the default ‚Äúa‚Äù in legend for geom_text() g &lt;- ggplotGrob(pfda_plot) lbls &lt;- 1:18 idx &lt;- which(sapply(g$grobs[[15]][[1]][[1]]$grobs,function(i){ &quot;label&quot; %in% names(i)})) for(i in 1:length(idx)){ g$grobs[[15]][[1]][[1]]$grobs[[idx[i]]]$label &lt;- lbls[i] } grid.draw(g) geom(text) With geom_text or annotate in ggplot2, you can set a number of properties of the text. geom_text is used to add text from the data frame, and annotate is used to add a single text element. Name Default value size 5 family \"\" (sans) fontface plain lineheight 1.2 angle 0 hjust 0.5 vjust 0.5 7.5.5 Subplots Note that the group must be called in the X argument of ggplot(aes(x = group, fill = subgroup)). The subgroup is called in the fill argument. The facet_wrap(~class, nrow = NULL, ncol = NULL, scales = \"fixed\") ( Multiple plots by factor in ggplot (facets) ) gives out each variable in an individual panel grouped by class. ~class can also be vars(class). facet_grid() function will produce a grid of plots for each combination of variables that you specify, even if some plots are empty. scales='fixed' if subplots share x-axes or y-axes; scales='free' for each plot having its own axes. nrow, ncol define #of rows/cols geom_errorbar() A geom that draws error bars, defined by an upper and lower value. This is useful e.g., to draw confidence intervals. 7.5.6 Parameters x - (required) x coordinate of the bar ymin - (required) y coordinate of the lower whisker ymax - (required) y coordinate of the upper whisker size - (default: 0.5) thickness of the lines linetype - (default: 1=solid) the type of the lines colour - (default: ‚Äúblack‚Äù) the color of the lines width - (default: 0.9) width of the whiskers alpha - (default: 1=opaque) the transparency of the lines plot_data &lt;- Rad_trend_decade_allCON %&gt;% gather(seasons, values, -CON) %&gt;% mutate(seasons = factor(seasons, levels=c(&quot;DJF&quot;, &quot;MAM&quot;, &quot;JJA&quot;, &quot;SON&quot;, &quot;ANN&quot;) ), CON = factor(CON, levels=c(CON_levels, &quot;WD&quot;) ) ) plot_data ## group by season p_season_box &lt;- ggplot(plot_data, aes(x=CON, y=values, fill=CON) ) + stat_boxplot(geom =&#39;errorbar&#39;, position = position_dodge(width = 0.9) ) + geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.9) ) + scale_y_continuous(limits = c(-7, 7), # set limits of y-axis breaks = seq(-7, 7, by=2) ) + facet_wrap(~seasons, nrow=1) p_season_box Ignore outliers Sometimes it can be useful to hide the outliers, for example when overlaying the raw data points on top of the boxplot. Hiding the outliers can be achieved by setting outlier.shape = NA. But the outliers would still affect the y-axis scale and make your box condensed. You need to set outliers = FALSE too. # set y-axis limits mannually ggplot(the_variable, aes(x=Water_receive, y=water_stress, fill=year)) + geom_boxplot(outlier.shape = NA) + scale_y_continuous(limits = quantile(the_variable$water_stress, c(0.1, 0.9))) # set outliers = FALSE by discarding outliers from the plot ggplot(the_variable, aes(x=Water_receive, y=water_stress, fill=year)) + geom_boxplot(outlier.shape = NA, outliers = FALSE) Deal with Outliers One idea would be to winsorize the data in a two-pass procedure: run a first pass, learn what the bounds are, e.g.¬†cut of at given percentile, or N standard deviation above the mean, or ‚Ä¶ in a second pass, set the values beyond the given bound to the value of that bound I should stress that this is an old-fashioned method which ought to be dominated by more modern robust techniques but you still come across it a lot. Grouped bar plot The items on the x-axis have x values of 1, 2, 3, and so on, though you typically don‚Äôt refer to them by these numerical values. When you use geom_bar(width = 0.9), it makes each group take up a total width of 0.9 on the x-axis. ÁªÑÈó¥ÂÆΩÂ∫¶„ÄÇ When you use position_dodge(width = 0.9), it spaces the bars so that the middle of each bar is right where it would be if the bar width were 0.9 and the bars were touching. ÂÄºË∂äÂ§ßÔºåÂêå‰∏ÄÁªÑÁöÑbar‰πãÈó¥Ë∂äËøú„ÄÇÁªÑÂÜÖbar‰πãÈó¥ÁöÑÈó¥Ë∑ù„ÄÇ Another option is to calculate stats first, and plot geom_crossbar. It is much faster this way. Rad_trend_decade_group &lt;- Rad_trend_decade_allCON %&gt;% group_by(CON) groups &lt;- Rad_trend_decade_group %&gt;% group_split() group_keys(Rad_trend_decade_group) groups[[1]] %&gt;% select(-CON) %&gt;% apply(2, function(group) boxplot.stats(group)$stats) trend_summary_tibble &lt;- Rad_trend_decade_group %&gt;% group_modify(~{ .x %&gt;% select(c(&quot;DJF&quot;, &quot;MAM&quot;, &quot;JJA&quot;, &quot;SON&quot;, &quot;ANN&quot;)) %&gt;% apply(2, function(group) boxplot.stats(group)$stats) %&gt;% as_tibble() }) %&gt;% ungroup() trend_summary_tibble &lt;- trend_summary_tibble %&gt;% mutate(stats = rep(c(&quot;low.whisker&quot;, &quot;1st.Q&quot;, &quot;median&quot;, &quot;3rd.Q&quot;, &quot;upper.whisker&quot;), 7 ) ) trend_summary_tibble # A tibble: 35 x 7 # CON DJF MAM JJA SON ANN stats # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; # 1 AF -1.43 -1.44 -2.43 -1.29 -0.768 low.whisker # 2 AF 0.428 0.506 -0.160 0.423 0.543 1st.Q # 3 AF 1.02 1.11 0.393 0.966 0.988 median # 4 AF 1.68 1.84 1.41 1.58 1.42 3rd.Q # 5 AF 3.54 3.84 3.77 3.32 2.73 upper.whisker # 6 AS -1.72 -4.17 -6.84 -3.45 -2.96 low.whisker # 7 AS -0.282 -0.774 -1.65 -0.974 -0.652 1st.Q # 8 AS 0.192 0.296 0.0631 -0.116 0.168 median # 9 AS 0.673 1.49 1.81 0.674 0.888 3rd.Q # 10 AS 2.11 4.89 7.00 3.15 3.18 upper.whisker CON_levels &lt;- trend_summary_tibble %&gt;% filter((stats==&quot;median&quot;) &amp;(CON!=&quot;WD&quot;) ) %&gt;% arrange(ANN) %&gt;% pull(CON) CON_levels plot_data &lt;- trend_summary_tibble %&gt;% gather(seasons, values, -CON, -stats) %&gt;% spread(stats, values) plot_data &lt;- plot_data %&gt;% mutate(seasons = factor(seasons, levels=c(&quot;DJF&quot;, &quot;MAM&quot;, &quot;JJA&quot;, &quot;SON&quot;, &quot;ANN&quot;) ), CON = factor(CON, levels=c(CON_levels, &quot;WD&quot;) ) ) plot_data # A tibble: 9,072,000 x 3 # CON seasons values # &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; # 1 EU DJF NA # 2 EU DJF NA # 3 EU DJF NA # 4 EU DJF NA # 5 EU DJF NA x11() dodge &lt;- position_dodge(width=0.9) ## Group by continent # dev.set(dev.prev()) p_box &lt;- ggplot(plot_data, aes(x=seasons, y=median, fill = seasons) ) + geom_crossbar(aes(ymin = `1st.Q`, ymax = `3rd.Q`), width = 0.8, size=0.35, position = dodge) + geom_errorbar(aes(ymin = low.whisker, ymax = upper.whisker, ), width = 0.8, position = dodge) + facet_wrap(~CON, nrow=1) + # subplots group by &#39;CON&#39; labs(y=TeX(&quot;SSR Trend \\\\[$Wm^{-2}/dec$\\\\]&quot;)) + scale_y_continuous(breaks=seq(-6,6,2)) + guides(fill = guide_legend(title=&quot;Seasons&quot;) ) + # change legend title theme(strip.text = element_text(size=10, face=&quot;bold&quot;), axis.text.x = element_text(size=10, face=&quot;bold&quot;, angle = 90), axis.text.y = element_text(size=10, face=&quot;bold&quot;), axis.title.x = element_blank(), legend.position = &quot;bottom&quot;, legend.title = element_text(size=10, face=&quot;bold&quot;), legend.text = element_text(face=&quot;bold&quot;) ) p_box 7.5.7 guides(), together with guide_legend() https://ggplot2-book.org/guides.html#sub-layers-legends Guides for each scale can be set scale-by-scale with the guide argument in scale_*_manual(), or en masse with guides(...). ... List of scale name-guide pairs. The guide can either be a string (i.e.¬†‚Äúcolorbar‚Äù or ‚Äúlegend‚Äù), or a call to a guide function (i.e.¬†guide_colourbar() or guide_legend() ) specifying additional arguments. # ggplot object dat &lt;- data.frame(x = 1:5, y = 1:5, p = 1:5, q = factor(1:5), r = factor(1:5)) p &lt;- ggplot(dat, aes(x, y, colour = p, size = q, shape = r)) + geom_point() # without guide specification p # Show colorbar guide for colour. # All these examples below have a same effect. p + guides(colour = &quot;colorbar&quot;, size = &quot;legend&quot;, shape = &quot;legend&quot;) p + guides(colour = guide_colorbar(), size = guide_legend(), shape = guide_legend()) p + scale_colour_continuous(guide = &quot;colorbar&quot;) + scale_size_discrete(guide = &quot;legend&quot;) + scale_shape(guide = &quot;legend&quot;) # Remove some guides p + guides(colour = &quot;none&quot;) p + guides(colour = &quot;colorbar&quot;, size = &quot;none&quot;) ## # Guides are `integrated` where possible ## p + guides(colour = guide_legend(&quot;title&quot;), size = guide_legend(&quot;title&quot;), shape = guide_legend(&quot;title&quot;)) # same as (more concise) g &lt;- guide_legend(&quot;title&quot;) p + guides(colour = g, size = g, shape = g) p + theme(legend.position = &quot;bottom&quot;) # Set order for multiple guides/legends # ggplot(mpg, aes(displ, cty)) + geom_point(aes(size = hwy, colour = cyl, shape = drv)) + guides( colour = guide_colourbar(order = 1), shape = guide_legend(order = 2), size = guide_legend(order = 3) ) guide_legend(title = waiver(), label = TRUE, keywidth = NULL, keyheight = NULL, override.aes = list(), nrow = NULL, ncol = NULL, byrow = FALSE, reverse = FALSE,order = 0, ...) Legend type guide shows key (i.e., geoms) mapped onto values. Legend guides for various scales are integrated if possible. title A character string or expression indicating a title of guide. NULL, the title is not shown. By default (waiver()), the name of the scale object or the name specified in labs() is used for the title. override.aes Takes a list of aesthetic parameters that will override the default legend appearance. nrow, ncol The desired number of rows/columns of legends. reverse logical. If TRUE the order of legends is reversed. order positive integer less than 99 that specifies the order of this guide among multiple guides. This controls the order in which multiple guides are displayed, not the contents of the guide itself. 7.5.8 Template for multi-series Multiple groups aesthetics, e.g., color, size, linetype, ‚Ä¶, if you want an integrated legend, need to set the same title for the aesthetics. g &lt;- guide_legend(&quot;title&quot;, nrow=2, byrow=TRUE) # define legend aes # colors cols &lt;- c(&quot;values&quot;=&quot;blue&quot;, &quot;values_ma&quot;=&quot;blue&quot;, &quot;lm1&quot;=&quot;red&quot;, &quot;values2&quot;=&quot;black&quot;, &quot;values_ma2&quot;=&quot;black&quot;, &quot;lm2&quot;=&quot;red&quot;) # if you don&#39;t want to repeat the breaks, you may use `setNames` # cols &lt;- setNames(ob = c(&quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;black&quot;, &quot;black&quot;, &quot;red&quot;), # nm = c(&quot;values&quot;, &quot;values_ma&quot;, &quot;lm1&quot;, &quot;values2&quot;, &quot;values_ma2&quot;, &quot;lm2&quot;)) # line widths sizes &lt;- c(&quot;values&quot;=0.3, &quot;values_ma&quot;=0.7, &quot;lm1&quot;=0.7, &quot;values2&quot;=0.3, &quot;values_ma2&quot;=0.7, &quot;lm2&quot;=0.7) # linetypes lines &lt;- c(&quot;values&quot;=&quot;dashed&quot;, &quot;values_ma&quot;=&quot;solid&quot;, &quot;lm1&quot;=&quot;solid&quot;, &quot;values2&quot;=&quot;dashed&quot;, &quot;values_ma2&quot;=&quot;solid&quot;, &quot;lm2&quot;=&quot;solid&quot;) # breaks: # ‚Äî if omit a series in `breaks`, the series will be droped; # ‚Äî rearrange legend order; breaks &lt;- c(&quot;values&quot;, &quot;values_ma&quot;, &quot;lm1&quot;, &quot;values2&quot;, &quot;values_ma2&quot;, &quot;lm2&quot;) # labels, must match the length of `breaks` labels &lt;- c(&quot;raw&quot;, &quot;Gaussian MA&quot;, unname(TeX( sprintf(&quot;Reg Line \\\\textbf{1961-2019}, \\\\[%.2f, %.2f, %.2f\\\\] \\\\[$Wm^{-2}/dec$\\\\]&quot;, decadal_trend[&quot;beta&quot;], decadal_trend[&quot;lower bound&quot;], decadal_trend[&quot;upper bound&quot;]) ) ), &quot;raw2&quot;, &quot;Gaussian MA2&quot;, unname(TeX( sprintf(&quot;Reg Line2 \\\\textbf{%s-2019}, \\\\[%.2f, %.2f, %.2f\\\\] \\\\[$Wm^{-2}/dec$\\\\]&quot;, bk_point+1, decadal_trend_2[&quot;beta&quot;], decadal_trend_2[&quot;lower bound&quot;], decadal_trend_2[&quot;upper bound&quot;]) ) ) ) ggplot() + scale_colour_manual(values = cols, breaks=breaks, labels = labels) + scale_linetype_manual(values=lines, breaks=breaks, labels = labels) + scale_size_manual(values=sizes, breaks=breaks, labels = labels) + theme_bw() + guides(colour=g, linetype=g, size=g # specify all aes for legend at once ) + theme(title = element_text(size=10), # title size axis.title.x = element_blank(), # axis lable legend.title = element_blank(), # remove legend title legend.text = element_text(size=8), # legend scale size legend.position = c(0.37, 0.9), legend.direction = &quot;horizontal&quot;, legend.spacing.y = unit(0, &#39;mm&#39;), # spacing between legend title and legends; legend.text.align = 0, # legend key align to the left legend.key.height = unit(0.8,&quot;line&quot;), # spacing between legend items; legend.margin = margin(t=0, b=0, unit=&quot;mm&quot;) # margins around legend box ) legend.position Can be text placement. Five possible values: ‚Äúleft‚Äù, ‚Äútop‚Äù, ‚Äúright‚Äù, ‚Äúbottom‚Äù, or \"none\" (disable legend). The argument legend.position can be also a numeric vector c(x,y). In this case it is possible to position the legend inside the plotting area. x and y are the coordinates of the legend box. Their values should be between 0 and 1. c(0,0) corresponds to the ‚Äúbottom left‚Äù and c(1,1) corresponds to the ‚Äútop right‚Äù position. legend.key.height the height of the legend key; reduce or add vertical spacing between legend items. legend.key.width the width of the legend key; legend.text.align = 0 align legend keys/text to the left; legend.box.background = element_rect(colour = \"black\", size=1) add box to legend legend.direction layout of items in legends (‚Äúhorizontal‚Äù or ‚Äúvertical‚Äù) legend.box arrangement of multiple legends (‚Äúhorizontal‚Äù or ‚Äúvertical‚Äù) guides() re-define guides, an example for override.aes; could also be used to remove some legends by specifying e.g., color=\"none\". guides(fill = FALSE, color = guide_legend(override.aes = list(fill = c(&quot;converted&quot;=&quot;#00BFC4&quot;, &quot;reported&quot;=&quot;#FF6666&quot;, obs_col=NA), color = c(&quot;converted&quot;=&quot;#00BFC4&quot;, &quot;reported&quot;=&quot;#FF6666&quot;, obs_col=&quot;#010912&quot;) ) ) ) guides(linetype = FALSE, size = FALSE, color = guide_legend( override.aes = list( linetype = lines, color = cols, size = sizes ), nrow = 2, byrow = TRUE ) ) Change legend title Specify a new legend title for the color aesthetic: labs(color=\"My new title\") guides(color=guide_legend(\"My new title\")) Putting two different legends in two columns theme(legend.box = \"horizontal\") Add legend to geom_vline Need to specify color inside aes, then use scale_color_manual to specify legends. ggplot(end_climate, aes(x=end.pre)) + geom_histogram(aes(y = after_stat(density)), binwidth=0.1, fill=&quot;#BDBCBC&quot;, color=&quot;black&quot;) + geom_vline(aes(xintercept=1.734513, color=&quot;Burke&quot;), linetype=&quot;dashed&quot;) + scale_color_manual(values=c(&quot;Burke&quot;=&quot;red&quot;), labels=c(&quot;Burke&quot;=&quot;Burke&#39;s fixed optimum&quot;)) + labs(x=&quot;Precipitation in 2019 [Meters]&quot;) + theme(legend.position = c(0.8, 0.9), legend.title = element_blank(), axis.title.y = element_blank(), ) legend alignment (irregular legends) split legends into multiple ragged rows/columns (with different length) as you desire https://stackoverflow.com/questions/27803710/ggplot2-divide-legend-into-two-columns-each-with-its-own-title Key idea: create a dummy factor level and setting its colour to white in the legend, so that it can‚Äôt be seen. so now we have regular levels. Then, we use scale_fill_manual to set the color of this blank level to ‚Äúwhite‚Äù. drop=FALSE forces ggplot to keep the blank level in the legend. Remember to¬†factor the category column. ## `factor` the category column, and specify `drop=FALSE` diamonds$cut = factor(diamonds$cut, levels=c(&quot;Fair&quot;,&quot;Good&quot;,&quot; &quot;,&quot;Very Good&quot;, &quot;Premium&quot;,&quot;Ideal&quot;)) ggplot(diamonds, aes(color, fill=cut)) + geom_bar() + scale_fill_manual(values=c(hcl(seq(15,325,length.out=5), 100, 65)[1:2], &quot;white&quot;, hcl(seq(15,325,length.out=5), 100, 65)[3:5]), drop=FALSE) + guides(fill=guide_legend(ncol=2)) + theme(legend.position=&quot;bottom&quot;) Retrieve environment variable Sys.getenv(&#39;PLOTLY_MATHJAX_PATH&#39;) # set an environment variable Sys.setenv(&#39;PLOTLY_MATHJAX_PATH&#39; = &#39;/Users/Menghan/Documents/R/MathJax&#39;) Check global setting/options options(...) set options, using name = value. getOption(x, default=NULL) get option values. default if the specified option is not set in the options list, this value is returned. This facilitates retrieving an option and checking whether it is set and setting it separately if not. &gt; options(&quot;device&quot;) $device [1] &quot;RStudioGD&quot; Check package version packageVersion(&quot;snow&quot;) Length Unit The relation between the absolute units is as follows: 1in = 2.54cm = 25.4mm = 72pt = 6pc Latex symbol in legend labels cols &lt;- c() # specify col vector here sizes &lt;- c() # specify linewidth lines &lt;- c() # linetype breaks &lt;- c() # group breaks, set legend order; ## note that the use of `unname()` is necessary labels &lt;- unname(latex2exp::TeX(c(&quot;$A_{t-k}^h$&quot;, &quot;$B_{t-k}^h$&quot;))) scale_colour_manual(values = cols, breaks=breaks, labels = labels) + scale_linetype_manual(values=lines, breaks=breaks, labels = labels) + scale_size_manual(values=sizes, breaks=breaks, labels = labels) 7.5.9 grid.gedit() https://bookdown.org/rdpeng/RProgDA/the-grid-package.html#grobs https://stackoverflow.com/questions/15059093/ggplot2-adjust-the-symbol-size-in-legends grid.gedit(..., grep = TRUE, global = TRUE) Changes the value of one of the slots of a grob and redraws the grob. ... Zero or more named arguments specifying new slot values. # To get the names of all the grobs in the ggplot grid.ls(grid.force()) # The edit - to set the size of the point in the legend to 4 mm grid.gedit(&quot;key-[-0-9]-1-1&quot;, size = unit(4, &quot;mm&quot;)) Transparency in Rstudio https://tinyheero.github.io/2015/09/15/semi-transparency-r.html Error Message: Warning message: In grid.Call.graphics(L_polygon, x$x, x$y, index) : semi-transparency is not supported on this device: reported only once per page Fix: I was able to solve this issue by switching over to use the Cairo graphics device. Make sure you first install the Cairo R package. install.packages(&quot;Cairo&quot;) Once you have that installed, place the following in your ~/.Rprofile setHook(packageEvent(&quot;grDevices&quot;, &quot;onLoad&quot;), function(...) grDevices::X11.options(type=&#39;cairo&#39;)) options(device=&#39;x11&#39;) # set default Graphic Device This makes it so that your default graphics device is set to Cairo whenever you start a new R session. Now open a new R session, and try the same plotting code (from above). You should see a plot with transparency. # get default graphic device getOption(&quot;device&quot;) What‚Äôs a graphic device? It‚Äôs the engine that renders your plot. Common graphics devices are Quartz and X11. "],["7.6-plot-raster-data.html", "7.6 plot Raster data", " 7.6 plot Raster data geom_rect() and geom_tile() do the same thing, but are parameterised differently: geom_rect() uses the locations of the four corners (xmin, xmax, ymin and ymax), while geom_tile() uses the center of the tile and its size (x, y, width, height). geom_raster(mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", ..., hjust = 0.5, vjust = 0.5, interpolate = FALSE, na.rm = FALSE, show.legend = NA,inherit.aes = TRUE) is a high performance special case for when all the tiles are the same size. # Interpolation smooths the surface &amp; is most helpful when rendering images. ggplot(faithfuld, aes(waiting, eruptions)) + geom_raster(aes(fill = density), interpolate = TRUE) plot_raster &lt;- function(r, title=NULL,low=50, high=300, unit=&quot;$W/m^2$&quot;){ # @param r is a raster # return a ggplot object mat &lt;- as.matrix(r) latMat &lt;- rep.col(seq(from=89.75,to=-89.75,by=-0.5), 720) lonMat &lt;- rep.row(seq(from=-179.75,to=179.75,by=0.5), 360) plot_data &lt;- tibble(lat=as.vector(latMat), lon=as.vector(lonMat), value=as.vector(mat) ) p &lt;- ggplot(plot_data, aes(lon, lat)) + geom_raster(aes(fill=value), interpolate = TRUE, na.rm = TRUE) + ylim(-60, 90) + # crop latitude extent scale_fill_gradientn(colours = viridis::viridis(256, option = &quot;C&quot;), limits=c(50,300), # set limit for legend space = &quot;Lab&quot;,name=TeX(unit), na.value = NA # remove gray NA values area ) + labs(title=title) + theme_minimal() + theme(axis.title.x = element_blank(), axis.title.y = element_blank()) return (p) } geom_tile(x, y, width, height) plots squares, similar to geom_raster(). geom_raster() is a high performance special case for when all the tiles are the same size. geom_rect(xmin, xmax, ymin and ymax) plots rectangles. Gradient color scales * could be either color or fill. scale_*_gradient creates a two colour gradient (low-high), scale_*_gradient2 creates a diverging colour gradient (low-mid-high), scale_*_gradientn creates a n-colour gradient. scale_*_gradientn(..., colours,values = NULL, space = \"Lab\",na.value = \"grey50\", guide = \"colourbar\",aesthetics = \"fill\", colors ) ... Arguments passed on to continuous_scale palette A palette function that when called with a numeric vector with values between 0 and 1 returns the corresponding output values (e.g., scales::area_pal()). breaks labels limits oob How to handle out of bound values default replace oob values with NA a lambda function scales::squish() for squishing out of bounds values into range. colours Vector of colours to use for n-colour gradient. values If colours should not be evenly positioned along the gradient this vector gives the position (between 0 and 1) for each colour in the colours vector. guide Type of legend. Use \"colourbar\" for continuous colour bar, or \"legend\" for discrete colour legend. Color palette in heat maps‚Äîusing scale_fill_gradient, scale_fill_gradient2 or scale_fill_gradientn. scale_fill_gradient setting a lower and a higher color to represent the values of the heat map. ggplot(df, aes(x = x, y = y, fill = value)) + geom_tile(color = &quot;black&quot;) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() If you want to add a mid color you can use scale_fill_gradient2, which includes the midargument. you can also use a custom color palette with scale_fill_gradientn, which allows passing n colors to the colors argument. ggplot(df, aes(x = x, y = y, fill = value)) + geom_tile(color = &quot;black&quot;) + scale_fill_gradient2(low = &quot;#075AFF&quot;, mid = &quot;#FFFFCC&quot;, high = &quot;#FF0000&quot;) + coord_fixed() ggplot(df, aes(x = x, y = y, fill = value)) + geom_tile(color = &quot;black&quot;) + scale_fill_gradientn(colors = hcl.colors(20, &quot;RdYlGn&quot;)) + coord_fixed() Categorical map scale_fill_manual() plot_data &lt;- plot_data %&gt;% mutate(trend_decadal = squish(trend_decadal, range = c(0.05+0.0001, 0.4) ), cat = cut(trend_decadal, seq(0.05, 0.4, 0.05)), cut = factor(cat, levels=rev(levels(cat)) ), label = sapply(as.character(levels(plot_data$cat)[plot_data$cat]), function(x) switch(x, &quot;(0.05,0.1]&quot; = 0.1, &quot;(0.1,0.15]&quot; = 0.15, &quot;(0.15,0.2]&quot; = .2, &quot;(0.2,0.25]&quot; = .25, &quot;(0.25,0.3]&quot; = .3, &quot;(0.3,0.35]&quot; = .35, &quot;(0.35,0.4]&quot; = .4) ) ) world.map &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) world.map &lt;- world.map %&gt;% left_join(plot_data, by=c(&quot;iso_a3_eh&quot;=&quot;ISO_C3&quot;)) unit &lt;- &quot;¬∫C $dec^{-1}$&quot; p_map3 &lt;- ggplot() + geom_sf(data = world.map %&gt;% filter(continent!=&quot;Antarctica&quot;), aes(fill=cut), colour=&#39;gray50&#39;, lwd=0.3 ) + coord_sf(datum = NA) + labs(title=&quot;Temperature decadal trends&quot;) + scale_fill_manual(values = myColors, breaks=levels(plot_data$cut), name=TeX(unit) ) + theme_bw() + theme(plot.title = element_text(hjust=0.1) ) p_map3 Discrete value map scale_fill_stepsn, guide_colorsteps change aesthetics. scale_*_steps creates a two colour binned gradient (low-high), scale_*_steps2creates a diverging binned colour gradient (low-mid-high), and scale_*_stepsn creates a n-colour binned gradient. Using show.limits=TRUE to specify lengend limits. https://stackoverflow.com/questions/68679342/labelling-with-scale-fill-stepsn-breaks-and-labels-are-different-lengths The last box in the legend is bigger. This is a bug, you might tweak the breaks a little bit to add/subtract a very small value: breaks = c(-3 + smallvalue, -2:2, 3 - smallvalue) Binned gradient colour scales These scales are binned variants of the gradient scale family and works in the same way. # Define your own colour ramp to extract binned colours from ggplot(df, aes(x, y)) + geom_point(aes(colour = z1)) + scale_colour_stepsn(colours = terrain.colors(10)) Set breaks for gradient scale Let mid color be white. https://stackoverflow.com/questions/14000232/2-color-heatmap-in-r-with-middle-color-anchored-to-a-specific-value test = matrix(rnorm(200), 20, 10) test[1:10, seq(1, 10, 2)] = test[1:10, seq(1, 10, 2)] + 3 test[11:20, seq(2, 10, 2)] = test[11:20, seq(2, 10, 2)] + 2 test[15:20, seq(2, 10, 2)] = test[15:20, seq(2, 10, 2)] + 4 colnames(test) = paste(&quot;Test&quot;, 1:10, sep = &quot;&quot;) rownames(test) = paste(&quot;Name&quot;, 1:20, sep = &quot;&quot;) paletteLength &lt;- 50 myColor &lt;- colorRampPalette(c(&quot;yellow&quot;, &quot;white&quot;, &quot;blue&quot;))(paletteLength) # length(breaks) == length(paletteLength) + 1 # use floor and ceiling to deal with even/odd length pallettelengths myBreaks &lt;- c(seq(min(test), 0, length.out=ceiling(paletteLength/2) + 1), seq(max(test)/paletteLength, max(test), length.out=floor(paletteLength/2))) # Plot the heatmap pheatmap(test, color=myColor, breaks=myBreaks) ## Set bins for catogarizing values; also important when plotting for gradient filling with(plot_data, min(values, na.rm=TRUE)) with(plot_data, max(values, na.rm=TRUE)) legend_low &lt;- -5 # scale limit legend_high &lt;- 5 paletteLength &lt;- 20 # number of bins; also scale break in color gradient legend diff &lt;- (legend_high-legend_low)/paletteLength # set 0 in the mid of one bin, convenient for labelling myBreaks &lt;- c(seq(0-diff/2, legend_low, by=-diff), seq(0+diff/2, legend_high, by=diff)) myBreaks &lt;- sort(myBreaks) myLabels &lt;- rollapply(myBreaks, 2, mean) plot_data &lt;- plot_data %&gt;% mutate( values_fill = cut(values, breaks = myBreaks, label = myLabels ) ) # convert factor to numeric plot_data &lt;- plot_data %&gt;% mutate( values_fill = as.numeric(levels(values_fill)[values_fill]) ) Trend Color Bar with white in the mid col_vec &lt;- c(&quot;#00007F&quot;, &quot;#7AACED&quot;, &quot;white&quot;) cl &lt;- colorRampPalette(col_vec) show_palette(cl(100)) cold &lt;- cl(100) show_palette(cold) col_vec &lt;- c(&quot;white&quot;, &quot;#FFD4D4&quot;, &quot;#FF7F7F&quot;, &quot;#FF2A2A&quot;, &quot;#FF7F00&quot;, &quot;#FFD400&quot;) cl &lt;- colorRampPalette(col_vec) show_palette(cl(10)) cl &lt;- colorRampPalette(c(&quot;white&quot;, &quot;#FFD4D4&quot;)) cl(10)[5] show_palette(cl(10)) show_palette(cl(100)) warm &lt;- cl(100) myColors &lt;- c(cold, warm) show_palette(myColors) 7.6.1 Add Patterns to Shapes Add patterns to geom_sf ggpattern::geom_sf_pattern() expect for regular aesthetic setting, you may specify patterns using Pattern Arguments. pattern Pattern name string e.g.¬†‚Äòstripe‚Äô (default), ‚Äòcrosshatch‚Äô, ‚Äòpoint‚Äô, ‚Äòcircle‚Äô, ‚Äònone‚Äô pattern_angle Orientation of the pattern in degrees. default: 30. pattern_color Colour used for strokes and points outlines. default: ‚Äòblack‚Äô pattern_fill stripes ‰∏≠Èó¥ÁöÑÂ°´ÂÖÖËâ≤„ÄÇ patter_size stroke line width. default: 1. Â¶ÇÊûúÊòØÊñúÁ∫øÁöÑËØùÔºå0.1 ÁöÑÂÆΩÂ∫¶ÊØîËæÉÂêàÈÄÇ„ÄÇ patter_spacing Spacing of the pattern as a fraction of the plot size. default: 0.05. Ë∂äÂ∞èË∂äÂØÜ„ÄÇ pattern_density Approximate fill fraction of the pattern. Usually in range [0, 1], but can be higher. default: 0.2. ÂÄºË∂äÂ§ßË∂äÂØÜ„ÄÇ Can add patterns to other types of plots too, such as boxplot, barplot, crossbar. HELP. ggpattern change pattern spacing without changing size: If you want to maintain the scale then the product of density and spacing must be kept constant. https://stackoverflow.com/a/74551836/10108921 library(ggplot2) library(ggpattern) df &lt;- data.frame(trt = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), outcome = c(0.3, 0.9, 0.6)) ggplot(df, aes(trt, outcome)) + geom_col_pattern(aes(fill = trt, pattern_density = trt, pattern_spacing = trt), pattern = &quot;pch&quot;, color = &quot;black&quot;) + scale_pattern_density_manual(values = c(0.4, 0.2, 0.1)) + scale_pattern_spacing_manual(values = c(0.025, 0.05, 0.1)) If only specify pattern_spacing, dot sizes vary with spacing. The smaller the spacing, the larger the dots. ggplot(df, aes(trt, outcome)) + geom_col_pattern(aes(fill = trt, pattern_spacing = outcome/3), pattern = &#39;pch&#39;) "],["7.7-histogram.html", "7.7 Histogram", " 7.7 Histogram geom_histogram(mapping = NULL, data = NULL, stat = \"bin\", position = \"stack\", ..., binwidth = NULL, origin = NULL, breaks = NULL, bins = NULL, na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE) binwidth The width of the bins. Can be specified as a numeric value or as a function that calculates width from unscaled x. Here, ‚Äúunscaled x‚Äù refers to the original x values in the data, before application of any scale transformation. When specifying a function along with a grouping structure, the function will be called once per group. The default is to use the number of bins in bins, covering the range of the data. stat_bin() using bins=30; this is not a good default, but the idea is to get you experimenting with different number of bins. You can also experiment modifying the binwidth with center or boundary arguments. binwidth overrides binsso you should do one change at a time. You should always override this value, exploring multiple widths to find the best to illustrate the stories in your data. center, boundary numeric values specify bin positions. One value for either center or boundary is adequate, other values will be automatically filled using binwidth. center specifies the center of one of the bins. Default figure will use center position to identify bins. boundary specifies the boundary between two bins. Boundary values are more informative. [suggest to specify one boundary value; just easier to say boundaries] Worth noting that center and boundary can be either above or below the range of the data, in this case the value provided will be shifted of a multiple number of binwidth. bins Number of bins. Overridden by binwidth. Defaults to 30. breaks Actual breaks to use. Intervals are created as left open, right closed. But specifying inside geom_histogram might show weird breaks in y-axis labels. Specifying breaks using scale_x_continuous is a better practice. p &lt;- ggplot(data=data, aes(tmp) ) + geom_histogram(fill=&quot;#BDBCBC&quot;, color=&quot;black&quot;, binwidth = 2, boundary=0) + labs(x=&quot;Average temperature [¬∫C]&quot;) p geom_histogram(aes(..density..)) surrounding the variable names with .. means to call after_stat function. It delays the mapping until later in the rendering process when summary statistics have been calculated. The expression ..density.. is deprecated; use after_stat() in stead. Most aesthetics are mapped directly from variables found in the data, called direct input (stage1). Sometimes, however, you want to delay the mapping until later stages of the data that you can map aesthetics from, and three functions to control at which stage aesthetics should be evaluated. after_stat(x) and after_scale(x) can be used inside the aes() function, used as the mapping argument in layers. after_stat(x) uses variables calculated after the transformation by the layer stat (stage 2); E.g., the height of bars in geom_histogram() can be density probability; # this shows the count frequency ggplot(faithful, aes(x = waiting)) + geom_histogram(fill=&quot;#BDBCBC&quot;, color=&quot;black&quot;) # this shows the density plot, can replace after_stat(density) with ..density.. # surrounding the variable name with two dots ggplot(faithful, aes(x = waiting)) + geom_histogram(aes(y = after_stat(density)), fill=&quot;#BDBCBC&quot;, color=&quot;black&quot;) + geom_density() # empirical density after_stat(count) show frequncy count; after_stat(ncount) count, scaled to a maximum of 1; after_stat(density) show density; after_stat(ndensity) density, scaled to a maximum of 1; after_scale uses variables calculated after the scale transformation (stage 3); see documents here. could be used to label a bar plot; Add fitted density from a distribution # fit a lognormal distribution library(MASS) fit_params &lt;- fitdistr(prices_monthly$AdjustedPrice,&quot;lognormal&quot;) fit_params$estimate ggplot(prices_monthly, aes(x=AdjustedPrice)) + geom_histogram(bins=40, aes(y=..density..), fill=&quot;#BDBCBC&quot;, color=&quot;black&quot;) + stat_function(fun=dlnorm, args=list(meanlog = fit_params$estimate[&#39;meanlog&#39;], sdlog = fit_params$estimate[&#39;sdlog&#39;]), colour = &quot;red&quot; ) + scale_x_continuous(limits=c(0, 170)) Histogram of a vector dice_results &lt;- c(1,3,2,4,5,6,5,3,2,1,6,2,6,5,6,4) ggplot(aes(x=dice_results)) + geom_bar() This returns an error: data must be a data.frame. If you don‚Äôt provide argument name explicitly, sequential rule is used ‚Äì data arg is used for aes(x=dice_results). To correct it ‚Äì use arg name explicitly: ggplot(mapping = aes(x=dice_results)) + geom_bar() Alternatively, you may use it inside geom_ functions familiy without explicit naming mapping argument since mapping is the first argument unlike in ggplot function case where data is the first function argument. ggplot() + geom_bar(aes(dice_results)) # or use the `aes` function ggplot() + aes(dice_results) + geom_bar() Vertical histogram https://stackoverflow.com/a/13334294/10108921 geom_bar and geom_col plots bar charts. geom_bar makes the height of the bar proportional to the number of cases in each group. geom_col the heights of the bars to represent values in the data geom_ribbon(data=sim_obs_quantile, aes(ymin=`17%`, ymax=`83%`), alpha=0.2, fill=\"#F8766D\") plot confidence interval (CI) in shaded areas. geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), col = \"red\", arrow = arrow(length = unit(0.3, \"cm\")) draws a straight line between points (x, y) and (xend, yend) in the plot. arrow specification for arrow heads, as created by grid::arrow(). annotate(\"segment\", x=12, y=-0.05, xend=12, yend=0, col=\"red\", arrow=arrow(length=unit(0.3, \"cm\"))) draw arrows outside the plot. "],["7.8-qq-plot.html", "7.8 QQ-plot", " 7.8 QQ-plot # using `ggplot2` sim_data &lt;- rsnorm(10000, mean = 0, sd = 1, xi = -2.5) ggplot() + geom_qq(aes(sample=sim_data)) + geom_qq_line(aes(sample=sim_data), color=&quot;red&quot;) + labs(x=&quot;Theoretical quantiles&quot;, y=&quot;Sample quantiles&quot;, title=&quot;Normal Q-Q plot&quot;) geom_qq() produce quantile-quantile plots. Sample quantiles in y-axis, theoretical quantiles in x-axis. geom_qq_line() compute the slope and intercept the line regressing sample and theoretical quantiles. Standardize to improve visualization. calculate mannually or use scale(x, center=TRUE, scale=TRUE) center and scale can be either a logical value or a numeric vector of length equal to the number of columns of x. \\[ \\frac{x-\\text{center}}{\\text{scale}} \\] # standardized qq-plot ggplot() + geom_qq(aes(sample=with(data, (HML-mean(HML))/sd(HML)))) + geom_qq_line(aes(sample=with(data, (HML-mean(HML))/sd(HML))), color=&quot;red&quot;) + labs(x=&quot;Theoretical quantiles&quot;, y=&quot;Sample quantiles&quot;, title=sprintf(&quot;Normal Q-Q plot ‚Äî HML, sd: %.2f, kurtosis: %.2f&quot;, sd(data$HML), kurtosis(data$HML))) "],["7.9-heatmap.html", "7.9 Heatmap", " 7.9 Heatmap The lattice package allows to build heatmaps thanks to the levelplot() function. # Load the lattice package library(&quot;lattice&quot;) # Dummy data x &lt;- seq(1,10, length.out=20) y &lt;- seq(1,10, length.out=20) data &lt;- expand.grid(X=x, Y=y) data$Z &lt;- runif(400, 0, 5) ## Try it out levelplot(Z ~ X*Y, data=data ,xlab=&quot;X&quot;, main=&quot;&quot;) Previous example of this document was based on a data frame at the long format. In pactice, however, a square matrix is more often used. ## S3 method for class &#39;matrix&#39; levelplot(z, data = NULL, aspect = &quot;iso&quot;, ..., xlim, ylim, row.values = seq_len(nrow(x)), column.values = seq_len(ncol(x)), main=paste(&quot;predicted radiation:&quot;,timeVals[timeLayer]), ylab=&quot;latitude&quot;, xlab=&quot;longitude&quot;, col.regions=cls(256), cuts=255, at=seq(0,450,length.out=256)) z z is a numeric response evaluated on a rectangular grid defined by row.values and column.values col.regions color vector to be used cuts The number of levels the range of z would be divided into. at A numeric vector giving breakpoints along the range of z. A wrapper for layerplot with pre-defined color palette, axis title and etc. level_plot &lt;- function(r, col_vec=c(&quot;black&quot;,&quot;red&quot;,&quot;yellow&quot;,&quot;white&quot;)){ ## for plotting 0.5*0.5 degree map # @params r: RasterLayer or matrix # @params col_vec: color vector palette for legend cl &lt;- colorRampPalette(col_vec) latVec &lt;- seq(from=-89.75,to=89.75,by=0.5) #for plotting lonVec &lt;- seq(from=-179.75,to=179.75,by=0.5) #for plotting if (class(r) == &quot;RasterLayer&quot;){ # flip raster upside down, raster reads from top left layerPlot &lt;- levelplot(t(as.matrix(r))[,seq(360,1)],row.values=lonVec,column.values=latVec,main=paste(&quot;predicted radiation:&quot;,getZ(r)),ylab=&quot;latitude&quot;,xlab=&quot;longitude&quot;,col.regions=cls(256),cuts=255,at=seq(0,450,length.out=256)) } else if(class(r) == &quot;matrix&quot;){ # levelplot reads from bottom left layerPlot &lt;- levelplot(r,row.values=lonVec,column.values=latVec,ylab=&quot;latitude&quot;,xlab=&quot;longitude&quot;,col.regions=cls(256),cuts=255,at=seq(0,450,length.out=256)) } print(layerPlot) } "],["8-time-series.html", "Chapter 8 Time Series", " Chapter 8 Time Series http://r-statistics.co/Time-Series-Analysis-With-R.html Commonly used R packages for processing Financial data: quantmod quandl tidyquant https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ00-introduction-to-tidyquant.html PerformanceAnalytics, zoo, xts PerformanceAnalytics Charts and Tables Overview PA-charts.R Github.io page by Carl and Peterson: https://timelyportfolio.github.io/PerformanceAnalytics/index.html Brian Peterson‚Äôs PA website: https://braverock.r-universe.dev/PerformanceAnalytics "],["8.1-date.html", "8.1 Date", " 8.1 Date as.POSIXct(zoo::as.yearmon(seq(1960,2014)) + 11/12, frac = 1) generate yearly date (the end of year) sequence. Sys.getlocale(\"LC_TIME\") Check your locale. The locale describes aspects of the internationalization of a program. Initially most aspects of the locale of R are set to \"C\" (which is the default for the C language and reflects North-American usage ‚Äì also known as \"POSIX\"). R uses the current ‚ÄúLC_TIME‚Äù locale when parsing or writing dates (see format.Date), to determine the appropriate words for the days of the weeks and the months. Locale affects how R processes date time representations, e.g., languages. Sys.setlocale(category = \"LC_TIME\", locale = \"C\") this set the time language to English. Same as Sys.setlocale(category = \"LC_TIME\", locale = \"en_US.UTF-8\"). locale = \"\" means set to the default locale for your system. locale also accepts locales such as ‚Äúen_US‚Äù (for the English-language locale in the Unites States) or ‚Äúfr_FR‚Äù (for the French-language locale in France). First two-letter lowercase stands for the language code (using the ISO-639 standard); followed by a two-letter uppercase country code (using the ISO-3166 standard). # examples Sys.setlocale(&quot;LC_TIME&quot;, &quot;de&quot;) # Solaris: details are OS-dependent Sys.setlocale(&quot;LC_TIME&quot;, &quot;de_DE&quot;) # Many Unix-alikes Sys.setlocale(&quot;LC_TIME&quot;, &quot;de_DE.UTF-8&quot;) # Linux, macOS, other Unix-alikes Sys.setlocale(&quot;LC_TIME&quot;, &quot;de_DE.utf8&quot;) # some Linux versions Sys.setlocale(&quot;LC_TIME&quot;, &quot;German&quot;) # Windows Every time you start a new R session, you get back to native setting. Should you want a permanent change, put .First &lt;- function() { Sys.setlocale(&quot;LC_TIME&quot;, &quot;C&quot;) } in the $(R RHOME)/etc/Rprofile.site file. Read ?Startup for how to customize R startup and the use of .First. From string/numeric to date lubridate::ymd(...), mdy(...), dmy(...): They automatically work out the format once you specify the order of the component. To use them, identify the order in which year, month, and day appear in your dates, then arrange ‚Äúy‚Äù, ‚Äúm‚Äù, and ‚Äúd‚Äù in the same order. ... a character or numeric vector of suspected dates E.x. ymd(20101215) #&gt; [1] &quot;2010-12-15&quot; mdy(&quot;4/1/17&quot;) #&gt; [1] &quot;2017-04-01&quot; dmy(&quot;31-Jan-2017&quot;) #&gt; [1] &quot;2017-01-31&quot; # create year-month-01 date series from individual components columns in df df$date &lt;- with(df, ymd(sprintf(&#39;%04d%02d%02d&#39;, YR, MON, 1))) Get year and month from date lubridate::year(date) lubridate::month(date) From date to string Convert date x to a string of your choosing format using base R: format(x, format=\"%Y-%m-%d\") strftime(x, format=\"%Y-%m-%d\") From string to date, one can use strptime, but lubridate is more flexible. Use ?strptime to check how to represent date components using strings, usually start with % and followed by a letter. Format code list This code list corresponds to user‚Äôs locale setting and platform. For instance, %b will return English if your language setting is English, return German is your language setting is German. Specification Description %d Day of the month: 01-31 %m 2 digits month: 01-12 %b Abbreviated month name in the current locale on this platform: Jan, Feb, ‚Ä¶ %B Month‚Äôs name in full: January, February etc. %y 2 digits year without century: 00-99 %Y 4 digits year. Year with century on input:00 to 68 prefixed by 20; e.g., 200569 to 99 prefixed by 19; e.g., 1968 zoo.as.yearmon(x) to convert date to yearmon. This is useful when you want to join tables by year-month combination. as.yearmon(&quot;mar07&quot;, &quot;%b%y&quot;) as.yearmon(&quot;2007-03-01&quot;) as.yearmon(&quot;2007-12&quot;) # year-quarter &gt; as.yearqtr(&quot;2019-01&quot;) [1] &quot;2019 Q1&quot; # returned Date is the fraction of the way through # the period given by frac (= 0 by default) as.Date(x) as.Date(x, frac = 1) as.POSIXct(x) as.Date(x, frac=0) frac specifies the fractional amount through the month to use so that 0 is beginning of the month. The default value of frac is 0. 1 is the end of the month. Add one month to date %m+% and %m-% add and subtract months to a date without exceeding the last day of the new month. d &lt;- ymd(&quot;2012-01-31&quot;) [1] &quot;2012-01-31 UTC&quot; d %m+% months(1) # `%m+%` avoid rollover [1] &quot;2012-02-29 UTC&quot; &gt; d + months(1) # this is invalid as Feb doesn&#39;t have 31th day [1] NA &gt; d %m+% months(-1) [1] &quot;2011-12-31&quot; &gt; d %m-% months(1) # same as last code [1] &quot;2011-12-31&quot; &gt; d %m+% years(1) [1] &quot;2013-01-31&quot; &gt; d %m+% years(1:3) # add a sequence [1] &quot;2013-01-31&quot; &quot;2014-01-31&quot; &quot;2015-01-31&quot; &gt; d %m+% days(1:3) [1] &quot;2012-02-01&quot; &quot;2012-02-02&quot; &quot;2012-02-03&quot; A vector of dates d &lt;- ymd(&quot;2014-03-31&quot;) d %m+% months(seq(3,30,3)) [1] &quot;2014-06-30&quot; &quot;2014-09-30&quot; &quot;2014-12-31&quot; &quot;2015-03-31&quot; &quot;2015-06-30&quot; [6] &quot;2015-09-30&quot; &quot;2015-12-31&quot; &quot;2016-03-31&quot; &quot;2016-06-30&quot; &quot;2016-09-30&quot; Initialize an empty vector of dates # if you use d&lt;-c(), this will create a numeric vector # cannot concatenate afterwards d &lt;- lubridate::ymd() c(d, ymd(&quot;2014-03-31&quot;)) "],["8.2-process-data.html", "8.2 Process Data", " 8.2 Process Data Download stock data using quantmod::getSymbols getSymbols loads data in the current environment using the symbol/tick of the asset. Returned data will have column names symbol.Open, symbol.High, symbol.Low, symbol.Close, symbol.Volume, and symbol.Adjusted. # Stock prices stock_prices &lt;- getSymbols(c(&quot;SPY&quot;, &quot;AAPL&quot;), src = &#39;yahoo&#39;, from=&quot;2014-12-01&quot;, # add an extra month before the start date such that to=&quot;2023-12-31&quot;, # the 1st month has non-NA return data; # Note: need to start from &quot;12-01&quot;; auto.assign=TRUE # return 2 &quot;xts&quot; object in the current env: &quot;SPY&quot; and &quot;AAPL&quot; ) &gt; stock_prices # returned value is a vector of tickers [1] &quot;SPY&quot; &quot;AAPL&quot; # Treasury Bill Tbill_1m &lt;- getSymbols(&quot;DGS1MO&quot;, src=&quot;FRED&quot;, auto.assign=FALSE) # Market index: OSEBX mkt_idx_prices &lt;- getSymbols(&quot;OSEBX.OL&quot;, from=&quot;2014-12-01&quot;, to=&quot;2023-12-31&quot;, src=&quot;yahoo&quot;, auto.assign = FALSE) mkt_idx_prices # from xts to tibble or data.frame mkt_idx_prices &lt;- data.frame(mkt_idx_prices) %&gt;% rownames_to_column(var = &quot;Date&quot;) %&gt;% # this converts Date to a string mutate(Date = ymd(Date)) # convert back to Date type mkt_idx_prices %&gt;% str() mkt_idx_prices %&gt;% head() mkt_idx_prices %&gt;% tail() # for index, adjusted is equal to closing prices with(mkt_idx_prices, all.equal(OSEBX.OL.Adjusted, OSEBX.OL.Close)) f_name &lt;- &quot;data/market/OSEBX_prices_2015-2023_daily.csv&quot; write_csv(mkt_idx_prices, f_name) mkt_idx_prices &lt;- read_csv(f_name) src data sources. Options: yahoo, google, MySQL, FRED, csv, RData, and Oanda. Defaults to yahoo. yahoo returns 6 columns: OHLC, adjusted price, and volume. google returns OHLC and volume. FRED interest rates and other economic series data for US, including ‚Ä¢ CPIAUCSL (CPI) ‚Ä¢ POP (Population) ‚Ä¢ DNDGRA3M086SBEA (Real Consumption) ‚Ä¢ INDPRO (Industrial Production) ‚Ä¢ OILPRICE ‚Ä¢ BAA ‚Ä¢ DTB3 (3 month T-bills) ‚Ä¢ DGS10 (10 year Treasuries) ‚Ä¢ UNRATE (unemployment rate) To find the series name, refer to FRED‚Äôs website for all available indicators. Oanda The Currency Site (FX and Metals) Before doing any analysis you must always check the data to ensure quality. Do not assume that because you are getting it from a source such as Yahoo! or Google that it is clean. auto.assgin Defaults to True, data are loaded silently to the current environment, i.e., the workspace. If FALSE, need to assign the returned results to a variable. Note that only one symbol at a time may be requested when auto assignment is disabled. Objects loaded by getSymbols with auto.assign=TRUE can be viewed with showSymbols and removed by a call to removeSymbols. env = globalenv() where to create objects. Defaults to the global environment. Setting env=NULL is equal to auto.assign=FALSE. Alternatively, you can create a separate environment to store the downloaded data. # create a new env called `sp500` sp500 &lt;- new.env() # save the S&amp;P 500 (symbol:^GSPC) to `sp500` getSymbols(&quot;^GSPC&quot;, env = sp500, src = &quot;yahoo&quot;, from = as.Date(&quot;1960-01-04&quot;), to = as.Date(&quot;2009-01-01&quot;)) To load the variable GSPC from the environment sp500 to a variable in the global environment (also known as the workspace), three options: # opt 1 GSPC &lt;- sp500$GSPC # opt 2 GSPC1 &lt;- get(&quot;GSPC&quot;, envir = sp500) # opt 3 GSPC2 &lt;- with(sp500, GSPC) periodicity=\"daily\" periodicity of data to query and return. Defaults to ‚Äúdaily‚Äù. Must be one of ‚Äúdaily‚Äù, ‚Äúweekly‚Äù, ‚Äúmonthly‚Äù. # this returns beginning of month data getSymbols(Symbols = &quot;AAPL&quot;, from=&quot;2010-01-01&quot;, to=&quot;2018-03-01&quot;, periodicity=&quot;monthly&quot;) Download index components data download a csv file containing all company symbols and names. nasdaq100 &lt;- read.csv(&quot;nasdaq100list.csv&quot;, stringsAsFactors = FALSE, strip.white = TRUE) dim(nasdaq100) # check dimension nasdaq100$Name[duplicated(nasdaq100$Name)] # remove duplicates Download data By using the command tryCatch we handle unusual conditions, including errors and warnings. In this case, if the data from a company are not available from yahoo finance, the message Symbol ... not downloadable! is given. (For simplicity, we only download the symbols starting with A.) nasdaq &lt;- new.env() for(i in nasdaq100$Symbol[startsWith(nasdaq100$Symbol, &quot;A&quot;)]) { cat(&quot;Downloading time series for symbol &#39;&quot;, i, &quot;&#39; ...\\n&quot;, sep = &quot;&quot;) status &lt;- tryCatch(getSymbols(i, env = nasdaq, src = &quot;yahoo&quot;, from = as.Date(&quot;2000-01-01&quot;)), error = identity) if(inherits(status, &quot;error&quot;)) cat(&quot;Symbol &#39;&quot;, i, &quot;&#39; not downloadable!\\n&quot;, sep = &quot;&quot;) } # check AAPL time series with(nasdaq, head(AAPL)) # visualize chartSeries(nasdaq$AAPL) Have a look at the quantmod homepage for further examples. See the manual of the quantmod package for the whole list of available plot and visualization functions. Download only the close price Use getSymbols()[,4] to subset the close price column. tickers &lt;- c(&quot;0011.HK&quot;, &quot;1299.HK&quot;, &quot;1083.HK&quot;, &quot;0823.HK&quot;, &quot;0669.HK&quot;, &quot;0992.HK&quot;) portfolioPrices &lt;- NULL for (Ticker in tickers) portfolioPrices &lt;- cbind(portfolioPrices, getSymbols(Ticker, from = &quot;2012-09-01&quot;, to = &quot;2022-08-31&quot;, periodicity = &quot;weekly&quot;, auto.assign=FALSE)[, 4]) colnames(portfolioPrices) &lt;- c(&quot;HSBC&quot;, &quot;AIA&quot;, &quot;TG&quot;, &quot;LinkReit&quot;, &quot;Techronic&quot;, &quot;Lenovo&quot;) portfolioPrices Technical Indicators https://bookdown.org/kochiuyu/technical-analysis-with-r-second-edition2/technical-indicators.html 8.2.1 tidyquant Useful resources: https://www.tidy-pm.com/s-2data Nice thing about tidyquant is that it works directly with tibble, making it work seamlessly with tidyverse. This means we can: Seamlessly scale data retrieval and mutations Use the pipe (%&gt;%) for chaining operations Use dplyr and tidyr: select, filter, group_by, nest/unnest, spread/gather, etc Use purrr: mapping functions with map tq_get(x, get, from, to) get trading data, such as OHLC, and return as tibble. x A single character string, a character vector or tibble representing a single (or multiple) stock symbol, metal symbol, currency combination, FRED code, etc. get A character string representing the type of data to get for x. Possible options: \"stock.prices\": Get the open, high, low, close, volume and adjusted stock prices for a stock symbol from Yahoo Finance (https://finance.yahoo.com/). Wrapper for quantmod::getSymbols(). \"dividends\": Get the dividends for a stock symbol from Yahoo Finance (https://finance.yahoo.com/). Wrapper for quantmod::getDividends(). \"splits\": Get the split ratio for a stock symbol from Yahoo Finance (https://finance.yahoo.com/). Wrapper for quantmod::getSplits(). tq_get_options() returns a list of valid get options you can choose from. Use from and to to specify the period of interest. tq_get(&quot;AAPL&quot;,get = &quot;stock.prices&quot;) # A tibble: 2,687 √ó 8 symbol date open high low close volume adjusted &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 AAPL 2014-01-02 19.8 19.9 19.7 19.8 234684800 17.3 2 AAPL 2014-01-03 19.7 19.8 19.3 19.3 392467600 16.9 3 AAPL 2014-01-06 19.2 19.5 19.1 19.4 412610800 17.0 4 AAPL 2014-01-07 19.4 19.5 19.2 19.3 317209200 16.8 5 AAPL 2014-01-08 19.2 19.5 19.2 19.4 258529600 17.0 6 AAPL 2014-01-09 19.5 19.5 19.1 19.2 279148800 16.7 7 AAPL 2014-01-10 19.3 19.3 19.0 19.0 304976000 16.6 8 AAPL 2014-01-13 18.9 19.4 18.9 19.1 378492800 16.7 9 AAPL 2014-01-14 19.2 19.5 19.2 19.5 332561600 17.0 10 AAPL 2014-01-15 19.8 20.0 19.7 19.9 391638800 17.4 # ‚Ñπ 2,677 more rows # ‚Ñπ Use `print(n = ...)` to see more rows # get Facebook data for the past five years from = today() - years(5) Stocks &lt;- tq_get(&quot;FB&quot;, get = &quot;stock.prices&quot;, from = from) Stocks Mutiple stocks # get historical data for multiple stocks. e.g. GAFA tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) tq_index(x) returns the stock symbols and various attributes for every stock in an index or exchange. Eighteen indexes and three exchanges are available. tq_index_options() returns a list of stock indexes you can choose from. tq_exchange(x) Get all stocks in a stock exchange in tibble format. tq_exchange_options() returns a list of stock exchanges you can choose from. The options are AMEX, NASDAQ and NYSE. tq_index(&quot;SP500&quot;) # A tibble: 504 √ó 8 symbol company identifier sedol weight sector shares_held local_currency &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 AAPL APPLE INC 037833100 2046251 0.0686 - 171416583 USD 2 MSFT MICROSOFT CORP 594918104 2588173 0.0654 - 88389092 USD 3 NVDA NVIDIA CORP 67066G104 2379504 0.0563 - 292528720 USD 4 AMZN AMAZON.COM INC 023135106 2000019 0.0342 - 108870351 USD 5 META META PLATFORMS INC CLASS A 30303M102 B7TL820 0.0242 - 26074570 USD 6 GOOGL ALPHABET INC CL A 02079K305 BYVY8G0 0.0198 - 69889995 USD 7 BRK-B BERKSHIRE HATHAWAY INC CL B 084670702 2073390 0.0187 - 21540450 USD 8 GOOG ALPHABET INC CL C 02079K107 BYY88Y7 0.0166 - 58143623 USD 9 LLY ELI LILLY + CO 532457108 2516152 0.0163 - 9488573 USD 10 AVGO BROADCOM INC 11135F101 BDZ78H9 0.0145 - 51827325 USD # ‚Ñπ 494 more rows # ‚Ñπ Use `print(n = ...)` to see more rows ## This takes forever to run ... sp_500 &lt;- tq_index(&quot;SP500&quot;) %&gt;% tq_get(get = &quot;stock.prices&quot;) sp_500 &gt; dim(sp_500) [1] 1310963 15 # tq_index loads data for the last 10 years sp_500$date %&gt;% unique() %&gt;% head() [1] &quot;2014-01-02&quot; &quot;2014-01-03&quot; &quot;2014-01-06&quot; &quot;2014-01-07&quot; &quot;2014-01-08&quot; &quot;2014-01-09&quot; sp_500$date %&gt;% unique() %&gt;% tail() [1] &quot;2024-08-28&quot; &quot;2024-08-29&quot; &quot;2024-08-30&quot; &quot;2024-09-03&quot; &quot;2024-09-04&quot; &quot;2024-09-05&quot; tq_transmute_fun_options() to see which functions are available Calculate monthly return. # calculate monthly return of single stock tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% tq_transmute(select = adjusted, mutate_fun = periodReturn, period = &quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) Plot closing price. tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% ggplot(aes(date, close)) + geom_line() Group by and perform operations on individual stocks from daily to monthly data sp_500 %&gt;% group_by(symbol) %&gt;% tq_transmute(select = adjusted, mutate_fun = to.monthly, indexAt = &quot;lastof&quot;) calculate monthly returns. sp_500 %&gt;% group_by(symbol) %&gt;% tq_transmute(adjusted, mutate_fun = monthlyReturn) stats::ts(data, start, frequency) base R function for time series. # quarterly data &gt; ts(1:10, start = c(1959, 2), frequency = 4) # 2nd Quarter of 1959 Qtr1 Qtr2 Qtr3 Qtr4 1959 1 2 3 1960 4 5 6 7 1961 8 9 10 # monthly data &gt; ts(cumsum(1 + round(rnorm(18), 2)), start = c(1954, 7), frequency = 12) Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 1954 1.63 2.67 3.21 4.42 4.80 6.56 1955 7.51 8.53 10.12 10.81 12.23 12.52 13.89 13.90 14.75 15.58 16.37 19.34 Difference with zoo ts objects are regularly spaced and have numeric times and are good for months and quarters whereas zoo objects can be irregularly spaced and can use most common index classes. 8.2.2 From tibble to xts xts(x, order.by) x can only contain values in a matrix or atomic vector which places constraints on the values that can be used (generally numeric, but necessarily all of a single mode, i.e., not a mix of numeric and character values) xts(FF_factor, order.by = FF_factor$Date) If Date is the 1st column FF_factor %&gt;% xts(x=.[,-1], order.by=.[[1]]) From xts to tibble using data.frame. Date will be converted to chr first, then need to convert back to Date. data.frame(price_data) %&gt;% rownames_to_column(var = &quot;Date&quot;) %&gt;% # this converts Date to a string mutate(Date = ymd(Date)) # convert back to Date type using as_tibble(). Date will remain to be Date. price_data %&gt;% as_tibble() %&gt;% add_column(Date=index(price_data), .before = 1) fortify.zoo takes a zoo object and converts it into a data frame. price_data %&gt;% fortify.zoo() Fill missing values It is very common for daily prices to have missing values. The common practice is to fill missing values using the last non-NA observation. # Last obs. carried forward na.locf(x) # Next obs. carried backward na.locf(x, fromLast = TRUE) Subset a period window(x, start='YYYY-MM-DD', end='YYYY-MM-DD') extract a subperiod window. x[between(index(x), start=ymd('YYYY-MM-DD'), end=('YYYY-MM-DD') ), ] can also be used to subset a subperiod. To get all observations in March 1970: # GSPC is xts GSPC[&quot;1970-03&quot;] It is also possible to specify a range of timestamps using ‚Äò/‚Äô as the range separator, where both endpoints are optional: e.g., # data before 1960-01-06 GSPC[&quot;/1960-01-06&quot;] # data after 2008-12-25 GSPC[&quot;2008-12-25/&quot;] Change column names This can be done easily using either names , colnames , or setNames . my_xts &lt;- with(reg_data, xts(AdjustedPrice, order.by = Date)) my_xts colnames(my_xts) &lt;- &quot;AdjustedPrice&quot; # opt1 names(my_xts) &lt;- &quot;AdjustedPrice&quot; # opt2 my_xts &lt;- my_xts %&gt;% setNames(&quot;AdjustedPrice&quot;) # opt3 can be used in a pipe sequence my_xts apply.monthly(x, FUN=colSums) apply one function periodically per column. Note that colSums is used when calculating the sum of observations per period. If FUN=sum, then sum is not only applied to each time window, but also the sum of all columns is calculated. # from daily to monthly return apply.monthly(R, Return.cumulative) Add new column to xts: xts$new_column &lt;- col_data. Not convenient to do operations on xts. First convert to data.frame, then do operations as usual. to.monthly(x, indexAt='yearmon', name=NULL, OHLC = TRUE, ...) indexAt Convert final index to new class or date. Can be set to one of the following: Option Meaning yearmon The final index will then be yearmon yearqtr The final index will then be yearqtr firstof the first time of the period lastof the last time of the period startof the starting time in the data for that period endof the ending time in the data for that period OHLC If an OHLC object should be returned. prices &lt;- prices %&gt;% na.locf() # fill missing values in daily prices prices_monthly &lt;- prices %&gt;% to.monthly(indexAt = &quot;last&quot;, OHLC = FALSE) # alternative ways to achieve the same results as the last line prices_monthly &lt;- prices %&gt;% xts::apply.monthly(last) prices_monthly &lt;- prices[xts::endpoints(prices, on=&quot;months&quot;), ] head(prices_monthly) SPY EFA IJS EEM AGG 2012-12-31 127.7356 48.20629 74.81863 39.63340 97.98471 2013-01-31 134.2744 50.00364 78.82265 39.51723 97.37608 2013-02-28 135.9876 49.35931 80.10801 38.61464 97.95142 2013-03-28 141.1512 50.00364 83.39879 38.22143 98.04794 2013-04-30 143.8630 52.51315 83.50081 38.68614 98.99760 2013-05-31 147.2596 50.92775 87.08048 36.81840 97.01658 Note that to.monthly removes rows with missing values; be cautious with that. endpoints(prices, on=\"months\") Extract index locations for an xts object that correspond to the last observation in each period specified by on. Alternatively, one could use prices %&gt;% apply.monthly(last) which takes the last day of each month in the time series. Data for all months is returned including those with NA in some of the time series. period.apply(samplexts, INDEX = endpoints(samplexts, on = \"months\"), FUN = mean, ...) Apply a function periodically. ... Additional arguments for FUN. apply.monthly(samplexts, mean) This has the same results as the code above. quantmod::monthlyReturn(x, subset=NULL, type='arithmetic', leading=TRUE, ...) Given a set of prices, return periodic returns. subset an xts/ISO8601 style subset string. type type of returns: arithmetic (discrete) or log (continuous). leading should incomplete leading period returns be returned Now we‚Äôll call PerformanceAnalytics::Return.calculate(prices_monthly, method = \"log\") to convert to returns and save as an object called asset_returns_xts. Note this will give us log returns by the method = \"log\" argument, \\(z_t = \\Delta \\ln P_t = \\ln P_t-\\ln P_{t-1}=\\ln\\frac{P_t}{P_{t-1}}\\). We could have used method = \"discrete\" to get simple returns, \\(r_t = \\frac{P_t}{P_{t-1}}-1\\). This is the default value. Relationship between \\(z_t\\) and \\(r_t\\): \\[ \\begin{align*} \\ln(1+r_t)=z_t \\end{align*} \\] asset_returns_xts &lt;- na.omit(Return.calculate(prices_monthly, method = &quot;log&quot;)) head(asset_returns_xts) SPY EFA IJS EEM AGG 2013-01-31 0.04992311 0.03660641 0.052133484 -0.002935494 -0.0062309021 2013-02-28 0.01267821 -0.01296938 0.016175381 -0.023105250 0.0058910464 2013-03-28 0.03726766 0.01296938 0.040257940 -0.010235048 0.0009849727 2013-04-30 0.01903006 0.04896773 0.001222544 0.012085043 0.0096390038 2013-05-31 0.02333571 -0.03065563 0.041976371 -0.049483592 -0.0202136957 2013-06-28 -0.01343432 -0.02715331 -0.001402974 -0.054739116 -0.0157787232 prices[endpoints(prices, on=\"months\"), ] converts daily to monthly prices. endpoints(x, on=\"month\") x an xts object on retrieve the last observation of each period. Supported periods include: ‚Äúus‚Äù (microseconds), ‚Äúmicroseconds‚Äù, ‚Äúms‚Äù (milliseconds), ‚Äúmilliseconds‚Äù, ‚Äúsecs‚Äù (seconds), ‚Äúseconds‚Äù, ‚Äúmins‚Äù (minutes), ‚Äúminutes‚Äù, ‚Äúhours‚Äù, ‚Äúdays‚Äù, ‚Äúweeks‚Äù, ‚Äúmonths‚Äù, ‚Äúquarters‚Äù, and ‚Äúyears‚Äù. Calculate returns by tidyverse lag &lt;- dplyr::lag # have to use dplyr::lag, base R lag has problems returns &lt;- prices_monthly %&gt;% group_by(ISIN) %&gt;% mutate(delta.P = c(NA, diff(AdjustedPrice)), # fill the first obs with NA lag.P = lag(AdjustedPrice), Return = delta.P/lag.P, Return2 = AdjustedPrice/lag(AdjustedPrice)-1, Return_log = log(AdjustedPrice)-lag(log(AdjustedPrice)) ) Note: Base R lag and diff works perfect with xts, but not ideal for groupped tibbles in tidyverse. Be careful whenever call lag, better to print check if you get the correct lag as the function from different packages has differing features and output. "],["8.3-portfolio-return.html", "8.3 Portfolio Return", " 8.3 Portfolio Return w &lt;- c(0.25, 0.25, 0.20, 0.20, 0.10) portfolio_returns_xts_rebalanced_monthly &lt;- Return.portfolio(asset_returns_xts, weights = w, rebalance_on = &quot;months&quot;) %&gt;% `colnames&lt;-`(&quot;returns&quot;) PerformanceAnalytics::Return.portfolio(R=Return_xts, weights=NULL, rebalance_on, verbose=FALSE) Using a time series of returns and any regular or irregular time series of weights for each asset, this function calculates the returns of a portfolio with the same periodicity of the returns data. Returns a time series of returns weighted by the weights parameter, or a list that includes intermediate calculations R An xts, vector, matrix, data frame, timeSeries or zoo object of asset returns. weights A time series or single-row matrix/vector containing asset weights, as decimal percentages, treated as beginning of period (BOP) weights. If the user does not specify weights, an equal weight portfolio is assumed. if weights is an xts object, then any value passed to rebalance_on is ignored. This is useful when you have varying assets across time. The weights index specifies the rebalancing dates, therefore a regular rebalancing frequency provided via rebalance_on is not needed and ignored. Note that weights and R should be matched by period. Irregular rebalancing can be done by specifying a time series of weights. The function uses the date index of the weights for xts-style subsetting of rebalancing periods. rebalance_on Default ‚Äúnone‚Äù; alternatively ‚Äúdaily‚Äù ‚Äúweekly‚Äù ‚Äúmonthly‚Äù ‚Äúannual‚Äù to specify calendar-period rebalancing supported by endpoints. Ignored if weights is an xts object that specifies the rebalancing dates. verboase If verbose is TRUE, return a list of intermediary calculations, such as asset contribution and asset value through time. The resultant list contains $returns, $contributions, $BOP.Weight, $EOP.Weight, $BOP.Value, and $EOP.Value. chartSeries plot an OHLC object. chartSeries(AMZN, type=&quot;candlesticks&quot;, subset=&#39;2016-05-18::2017-01-30&#39;, theme = chartTheme(&#39;white&#39;, up.col=&#39;green&#39;, dn.col=&#39;red&#39;)) type=\"candlesticks\" can be line, bar, default to candlesticks. A candle has four points of data: Open ‚Äì the first trade during the period specified by the candle High ‚Äì the highest traded price Low ‚Äì the lowest traded price Close ‚Äì the last trade during the period specified by the candle A candle has three parts: body (open/close prices), upper shadow (high price), and lower shadow (low price). The color of the body can tell them if the stock price is rising or falling. Usually, red stands for falling and green stands for rising. theme = chartTheme(\"black\") defaults to black theme. up.col up bar/candle color dn.col down bar/candle color charts.RollingPerformance(R, width=12) creates a rolling annualized returns chart, rolling annualized standard deviation chart, and a rolling annualized sharpe ratio chart. Note that if your portfolio return is on a daily basis, first convert to monthly, then run the rolling function. Otherwise, daily rolling frequency can be too computaionally intensive. Rolling window Rolling window estimation "],["8.4-plot-ts.html", "8.4 Plot TS", " 8.4 Plot TS 8.4.1 plot.xts plot(msftSbuxMonthlyPrices, main=&quot;Monthly Closing Prices&quot;, legend.loc=&quot;topleft&quot;) The default plot style in plot.xts() is a single-panel plot with multiple series. You can also create multi-panel plots by setting the optional argument multi.panel = TRUE in the call to plot.xts(). plot(msftSbuxMonthlyPrices, main = &quot;Monthly Closing Prices&quot;, multi.panel = TRUE) Add legend to multiple series using addLegend plot_data &lt;- reg_data %&gt;% select(eRi, rmrf) %&gt;% xts(order.by = the_group$Date) plot_data col_vec &lt;- c(&quot;black&quot;, &quot;red&quot;) # color series plot.xts(plot_data, col = col_vec, main = &quot;Excess Return on Asset and Market&quot;) addLegend(&quot;topright&quot;, legend.names = c(&quot;eRi&quot;, &quot;rmrf&quot;), lty = c(1, 1), lwd = c(2, 2), col = col_vec, bg = &quot;white&quot;, # legend background bty = &quot;o&quot;, # box border style, doesn&#39;t work box.col = &quot;white&quot; # box border color ) Another example plot(x = basket[,&quot;SPY.Close&quot;], xlab = &quot;Time&quot;, ylab = &quot;Cumulative Return&quot;, main = &quot;Cumulative Returns&quot;, ylim = c(0.0, 2.5), major.ticks= &quot;years&quot;, minor.ticks = FALSE, col = &quot;red&quot;) lines(x = basket[,&quot;QQQ.Close&quot;], col = &quot;darkgreen&quot;) lines(x = basket[,&quot;GDX.Close&quot;], col = &quot;goldenrod&quot;) lines(x = basket[,&quot;DBO.Close&quot;], col = &quot;darkblue&quot;) lines(x = basket[,&quot;VWO.Close&quot;], col = &quot;darkviolet&quot;) legend(x = &#39;topleft&#39;, legend = c(&quot;SPY&quot;, &quot;QQQ&quot;, &quot;GDX&quot;, &quot;DBO&quot;, &quot;VWO&quot;), lty = 1, col = myColors) main.timespan = FALSE to remove the time span label in the top right corner. plot.xts overrides figure margins set by par. If you want to change figure margins, set it inside plot.xts function. plot.xts set mar = c(3, 2, 0, 2) by default. 8.4.2 autoplot For plotting xts objects, especially with multiple columns (data series), the ggplot2 function autoplot() is especially convenient and easy: library(ggplot2) # all series in one panel autoplot(msftSbuxDailyPrices, facets = NULL) + ggtitle(&quot;Daily Closing Prices&quot;) + ylab(&quot;Closing Price Per Share&quot;) + xlab(&quot;Year&quot;) or produce a multi-panel plot call autoplot() with facets = Series ~ .: # one panel for each series autoplot(msftSbuxDailyPrices, facets = Series ~ .) + ggtitle(&quot;Daily Closing Prices&quot;) + ylab(&quot;Closing Price Per Share&quot;) + xlab(&quot;Year&quot;) More refined setting Set colors # using autoplot from earlier, I placed it into an object p &lt;- autoplot(prcomp(df), data = iris, colour = &#39;Species&#39;, shape=&#39;Species&#39;, frame=T) # then I added on scale_color_manual and scale_fill_manual with the wacky color combos that would never be publishable a + scale_fill_manual(values = c(&quot;#FF1BB3&quot;,&quot;#A7FF5B&quot;,&quot;#99554D&quot;)) + scale_color_manual(values = c(&quot;black&quot;,&quot;white&quot;,&quot;orange&quot;)) Set date breaks # auto date breaks every 6 mons autoplot(plot_data, facets = NULL) + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;)) + scale_x_date(date_breaks = &quot;6 month&quot;, date_labels = &quot;%b %y&quot;, limits=c(ymd(&quot;2014-12-30&quot;), ymd(&quot;2021-12-30&quot;) ) ) # explicit date breaks autoplot(plot_data, facets = NULL) + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;)) + scale_x_date(breaks = seq(from=index(plot_data)[1] %m-% months(1), to=tail(index(plot_data), 1), by=&quot;6 month&quot;), date_labels = &quot;%b %y&quot;, limits=c(ymd(&quot;2014-12-30&quot;), ymd(&quot;2021-12-30&quot;) ) ) Simulate a autocorrelated time series. arima.sim(model, n, sd=1) model a list with the following elements. The coefficients must be provided through the elements ar and ma. order a vector of length 3 containing the ARIMA(p, d, q) order p specifies AR order; d is the differencing order; q specifies MA order. order = c(0, 0, 0) will generate White Noise. order = c(2, 0, 0) will generate an AR(2) series. order = c(0, 0, 2) will generate an MA(2) series. ar a vector of length p containing the AR(p) coefficients ma a vector of length q containing the MA(q) coefficients n length of simulated time series sd the standard deviation of the Gaussian errors. Defaults to 1. rand.gen = rnorm can specify a function to generate the innovations. Defaults to normal distribution generator. innov = rand.gen(n, ...) can specify your own series of error here. # AR(1), with rho=0.7 ar.epsilon &lt;- arima.sim(model = list(order = c(1,0,0), ar = 0.7), n = 200, sd=20) # AR(2), with with parameters 1.5 and -.75 AR &lt;- arima.sim(model = list(order = c(2, 0, 0), ar = c(1.5, -.75)), n = 200) # MA(3), with parameters 0.5, 0.4, and 0.2 sim_ma3_05 &lt;- arima.sim(model = list(order = c(0,0,3), ma = c(0.5, 0.4, 0.2)), n=200) "],["8.5-time-series-regression.html", "8.5 Time Series Regression", " 8.5 Time Series Regression Distributed Lag model using dynlm package, which is useful for dynamic linear models and time series regressions. Lags or differences can directly be specified in the model formula. The main function used to estimate our model is the dynlm(formula, data) function. Within this function, d() can be used to specify the difference in a variable and L() can be used to compute the desired lag of the variable. d(u, 1) means to calculate the first difference in u; L(g, 0:2) denotes g of the current period and the past two periods, \\(g\\), \\(g_{-1}\\), and \\(g_{-2}\\). Note that data must be either a data frame or zoo object. xts returns an error. &gt; library(dynlm) # Finite Distributed Lag Models &gt; okun.lag2 &lt;- dynlm(d(unemp, 1) ~ L(gGDP, 0:2), data = okun2.zoo) # lag 2 &gt; okun.lag3 &lt;- dynlm(d(unemp, 1) ~ L(gGDP, 0:3), data = okun2.zoo) # lag 3 # ARDL(1, 1) &gt; okun.ardl &lt;- dynlm(d(unemp, 1) ~ L(d(unemp, 1), 1) + L(gGDP, 0:1), data = okun2.zoo) &gt; summary(okun.ardl) Time series regression with &quot;zoo&quot; data: Start = 1948 Q3, End = 2025 Q1 Call: dynlm(formula = d(unemp, 1) ~ L(d(unemp, 1), 1) + L(gGDP, 0:1), data = okun2.zoo) Residuals: Min 1Q Median 3Q Max -1.4662 -0.2198 -0.0218 0.1686 4.9875 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.42871 0.03769 11.374 &lt; 2e-16 *** L(d(unemp, 1), 1) -0.05640 0.05874 -0.960 0.337736 L(gGDP, 0:1)1 -0.43423 0.02390 -18.165 &lt; 2e-16 *** L(gGDP, 0:1)2 -0.11951 0.03575 -3.343 0.000932 *** --- Signif. codes: 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1 Residual standard error: 0.4416 on 303 degrees of freedom (0 observations deleted due to missingness) Multiple R-squared: 0.5825, Adjusted R-squared: 0.5784 F-statistic: 140.9 on 3 and 303 DF, p-value: &lt; 2.2e-16 8.5.1 Lag Polynomial Let \\(\\rho(L)=1-\\rho_1L\\) and \\(\\beta(L)=\\beta_0+\\beta_1L.\\) Now we have \\(\\psi(L)\\) such at \\[ \\rho(L)\\psi(L)=\\beta(L) . \\] We want to find \\(\\psi(L)=\\rho(L)^{-1}\\beta(L).\\) # Compute psi(L) = rho(L)^(-1) * beta(L) lag_poly_solution &lt;- function(rho1, beta0, beta1, n_terms = 5) { # Expand (1 - rho1*L)^(-1) as a power series up to n_terms # (1 - rho1*L)^(-1) = 1 + rho1*L + rho1^2*L^2 + ... + rho1^n*L^n rho_inv &lt;- sapply(0:n_terms, function(k) rho1^k) # beta(L) = beta0 + beta1*L # psi(L) = (1 + rho1*L + rho1^2*L^2 + ...)*(beta0 + beta1*L) # = beta0*(1 + rho1*L + ...) + beta1*L*(1 + rho1*L + ...) # = beta0*rho_inv + beta1*c(0, rho_inv[1:n_terms]) psi &lt;- beta0 * rho_inv psi &lt;- psi + beta1 * c(0, rho_inv[1:n_terms]) # Return coefficients: psi0 + psi1*L + psi2*L^2 + ... names(psi) &lt;- paste0(&quot;L^&quot;, 0:n_terms) return(psi) } # Example usage: psi_coef &lt;- lag_poly_solution(rho1 = 0.5, beta0 = 1, beta1 = 2, n_terms = 20) psi_coef ## L^0 L^1 L^2 L^3 L^4 L^5 ## 1.000000e+00 2.500000e+00 1.250000e+00 6.250000e-01 3.125000e-01 1.562500e-01 ## L^6 L^7 L^8 L^9 L^10 L^11 ## 7.812500e-02 3.906250e-02 1.953125e-02 9.765625e-03 4.882812e-03 2.441406e-03 ## L^12 L^13 L^14 L^15 L^16 L^17 ## 1.220703e-03 6.103516e-04 3.051758e-04 1.525879e-04 7.629395e-05 3.814697e-05 ## L^18 L^19 L^20 ## 1.907349e-05 9.536743e-06 4.768372e-06 Plot the lag polynomial coefficients: library(tidyverse) psi_df &lt;- tibble( lag = 1:21, coef = psi_coef ) # Bar plot ggplot(psi_df, aes(x = lag, y = coef)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) + labs(x = &quot;Lag&quot;, y = expression(psi[L]), title = &quot;Lag Polynomial Coefficients&quot;) + theme_minimal() "],["9-regression.html", "Chapter 9 Regression", " Chapter 9 Regression Use example: library(tidyverse) reg_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/META_monthly_factor_model_2014-2024.csv&quot;) reg_data &lt;- reg_data %&gt;% mutate(return = adjusted/lag(adjusted)-1, eRi = return-RF) ## CAPM capm_ml &lt;- lm(eRi~rmrf, data=reg_data) summary(capm_ml) ## ## Call: ## lm(formula = eRi ~ rmrf, data = reg_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40895 -0.03482 0.00141 0.03767 0.20600 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.009747 0.007645 1.275 0.205 ## rmrf 1.066640 0.166384 6.411 2.8e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08343 on 123 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.2504, Adjusted R-squared: 0.2444 ## F-statistic: 41.1 on 1 and 123 DF, p-value: 2.801e-09 Get the coefficient table with broom::tidy. library(broom) tidy(capm_ml) ## # A tibble: 2 √ó 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.009746761 0.007644518 1.275000 2.047124e-1 ## 2 rmrf 1.066640 0.1663838 6.410718 2.801085e-9 Get the variance-covariance matrix with vcov. vcov(capm_ml) ## (Intercept) rmrf ## (Intercept) 5.843866e-05 -0.0002760605 ## rmrf -2.760605e-04 0.0276835643 # verify using coef table capm_ml %&gt;% tidy() %&gt;% mutate(variance=std.error^2) ## # A tibble: 2 √ó 6 ## term estimate std.error statistic p.value variance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.009746761 0.007644518 1.275000 2.047124e-1 0.00005843866 ## 2 rmrf 1.066640 0.1663838 6.410718 2.801085e-9 0.02768356 We can manually calculate as df &lt;- (nrow(capm_ml$model)-2) # degree of freedom sigma2 &lt;- sum(capm_ml$residuals^2)/df # residual variance # 1st column of lm$model is the depend. var. X &lt;- capm_ml$model[, 2, drop=FALSE] %&gt;% as.matrix() X &lt;- cbind(1, X) # add intercept V &lt;- solve(t(X) %*% X) * sigma2 V ## rmrf ## 5.843866e-05 -0.0002760605 ## rmrf -2.760605e-04 0.0276835643 Note that cov.unscaled returns the unscaled covariance matrix, \\((X&#39;X)^{-1}\\). To get estimated covariance matrix for the coefficients, you need to multiply cov.unscaled by the estimate of the error variance. solve(t(X) %*% X) ## rmrf ## 0.008395487 -0.03965975 ## rmrf -0.039659745 3.97711043 summary(capm_ml)$cov.unscaled ## (Intercept) rmrf ## (Intercept) 0.008395487 -0.03965975 ## rmrf -0.039659745 3.97711043 all.equal(solve(t(X) %*% X), summary(capm_ml)$cov.unscaled, check.attributes = FALSE) ## [1] TRUE References: https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/vcov.R stats package: https://docs.tibco.com/pub/enterprise-runtime-for-R/6.0.1/doc/html/Language_Reference/stats/summary.lm.html "],["9.1-ols.html", "9.1 OLS", " 9.1 OLS When variable names have spaces, use as.formula(\"`Avg ret` ~ Beta\"). use + 0 or - 1 as part of the formula to suppress the intercept. ~,+, -, * and ^ as formula operators have special meanings Symbol Purpose Example In Words ~ separate LHS and RHS of formula y ~ x regress y on x + add variable to a formula y ~ x + z regress y on x and z . denotes ‚Äúeverything else‚Äù y ~ . regress y on all other variables in a data frame - remove variable from a formula y ~ . - x regress y on all other variables except x 1 denotes intercept y ~ x - 1 regress y on x without an intercept : construct interaction term y ~ x + z + x:z regress y on x, z, and the product x times z * factor crossing, shorthand for levels plus interaction y ~ x * z regress y on x, z, and the product x times z, equivalent to y ~ x + z + x:z ^ higher order interactions y ~ (x + z + w)^3 regress y on x, z, w, all two-way interactions, and the three-way interactions I() ‚Äúas-is‚Äù - override special meanings of other symbols from this table y ~ x + I(x^2) regress y on x and x squared Fun fact: R‚Äôs formula syntax originated in this 1973 paper by Wilkinson and Rogers.‚Ü©Ô∏é Tabel source: Econometrics.blog by Frank DiTraglia. In function formula, I is used to inhibit the interpretation of operators such as \"+\", \"-\", \"*\" and \"^\" as formula operators, so they are used as arithmetical operators. This is interpreted as a symbol by terms.formula. For example, y ~ I(a+b) means y regressed on the sum of a and b, while y ~ a + b means y regressed on a and b. Broom takes in regression models and print nice tibble. broom:tidy(lm, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE) Tidy summarizes information about the components of a model. lm An lm object created by stats::lm(). conf.int=FALSE Defaults to FALSE. Logical indicating whether or not to include a confidence interval in the tidied output. conf.level=0.95 The confidence level to use for the confidence interval if conf.int = TRUE. Defaults to 0.95, which corresponds to a 95 percent confidence interval. exponentiate = FALSE Defaults to FALSE. Logical indicating whether or not to exponentiate the the coefficient estimates. This is typical for logistic and multinomial regressions, but a bad idea if there is no log or logit link. # a CAPM model example &gt; reg_ml &lt;- lm(eRi~rmrf, data = company_data) &gt; reg_ml Call: lm(formula = eRi ~ rmrf, data = company_data) Coefficients: (Intercept) rmrf 0.1139 0.2804 &gt; summary(reg_ml) Call: lm(formula = eRi ~ rmrf, data = company_data) Residuals: Min 1Q Median 3Q Max -10.1312 -0.6116 0.0905 0.6072 6.8892 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.11393 0.07835 1.454 0.14720 rmrf 0.28040 0.09455 2.966 0.00331 ** --- Signif. codes: 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1 Residual standard error: 1.233 on 249 degrees of freedom (10 observations deleted due to missingness) Multiple R-squared: 0.03412, Adjusted R-squared: 0.03024 F-statistic: 8.796 on 1 and 249 DF, p-value: 0.003312 9.1.1 Get the coef estimates coef(summary(FF_ml)) get a matrix of coefficients estimates, including point estiamtes, SEs, t-values, p-values. Equivalently, you can use tidy(FF_ml) to return a tibble, which is easier to manipulate and output. # `coef()` returns a vector of coef &gt; coef(reg_ml) (Intercept) rmrf 0.1139251 0.2804040 # `summary()$coef` returns a matrix, including se, t-stat, and pval # equivalent to `coef(summary(reg_ml))` &gt; summary(reg_ml)$coef Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.1139251 0.0783512 1.454031 0.147196897 rmrf 0.2804040 0.0945456 2.965807 0.003312303 # `broom::tidy` returns a tibble, the rownames become a column named `term` # column names are more robust, lowercase and with spaces removed &gt; broom::tidy(reg_ml) # A tibble: 2 √ó 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 0.114 0.0784 1.45 0.147 2 rmrf 0.280 0.0945 2.97 0.00331 9.1.2 Covariance Matrix lmtest::coeftest(ml, vcov.=NULL) coeftest is a generic function for performing z and (quasi-)t Wald tests of estimated coefficients. coefcicomputes the corresponding Wald confidence intervals. ml The generic function coeftest currently has a default method (which works in particular for \"lm\" objects) and dedicated methods for objects of class \"glm\" (as computed by glm), \"mlm\" (as computed by lm with multivariate responses), \"survreg\" (as computed by survreg), and \"breakpointsfull\" (as computed by breakpoints.formula). The default method assumes that a coef methods exists, such that coef(x) yields the estimated coefficients. vcov. To specify the corresponding covariance matrix vcov. to be used, there are three possibilities: It is pre-computed and supplied in argument vcov. A function for extracting the covariance matrix from x is supplied, e.g., sandwich, vcovHC, vcovCL, or vcovHAC from package sandwich. vcov. is set to NULL, then it is assumed that a vcov method exists, such that vcov(x) yields a covariance matrix. library(lmtest) coeftest(ml.gdp, vcovHC(ml.gdp, type = &#39;HC0&#39;, cluster = &#39;group&#39;)) vcovHAC(lm_phillips) vcovHC(lm_phillips) lm_phillips %&gt;% coeftest(., vcov = vcovHAC(.)) lm_phillips %&gt;% coeftest(., vcov = vcovHC(., type = &#39;HC0&#39;)) The ‚Äúsandwich‚Äù is two pieces of bread defined by the expected information enclosing a meat defined by the observed information. For a linear regression, the estimating equation is: \\[ U(\\beta) = \\mathbf{X}^T\\left(Y - \\mathbf{X}^T\\beta\\right) \\] The expected information (bread) is: \\[ A = \\frac{\\partial U(\\beta)}{\\partial \\beta} = -(\\mathbf{X}^T\\mathbf{X}) \\] The observed information (meat) is: \\[ B = E\\left[U(\\beta)U(\\beta)^T\\right] = \\mathbf{X}^T(Y-\\mathbf{X}^T\\beta)(Y-\\mathbf{X}^T\\beta)^T\\mathbf{X} \\] Note the inner term is a diagonal of constant residuals when the homoscedasticity, independent data assumption is met, then the sandwich covariance estimator which is given by \\(A^{-1}BA^{-1}\\) is the usual linear regression covariance matrix \\(\\sigma^2(X^TX)^{-1}\\) where \\(\\sigma^2\\) is the variance of the residuals. However, that‚Äôs rather strict. You get a considerably broader class of estimators by relaxing the assumptions involved around the ùëõ√óùëõ residual matrix: \\[ R = (Y-\\mathbf{X}^T\\beta)(Y-\\mathbf{X}^T\\beta) \\] The ‚ÄúHC0‚Äù vcovHC estimator is consistent even when the data are not independent. So I will not say that we ‚Äúassume‚Äù the residuals are independent, but I will say that we use ‚Äúa working independent covariance structure‚Äù. Then the matrix ùëÖ is replaced by a diagonal of the residuals \\[ R_{ii} = (Y_i - \\beta \\mathbf{X}_{I.})^2, \\quad 0\\text{ elsewhere} \\] This estimator works really well except under small samples (&lt;40 is often purported). The HC1-3 are various finite sample corrections. HC3 is generally the best performing. sandwich::vcovHC(x, type=c(\"HC3\", \"const\", \"HC\", \"HC0\", \"HC1\", \"HC2\", \"HC4\", \"HC4m\", \"HC5\"), cluster ) is a function for estimating a Heteroscedasticity-robust covariance matrix of parameters for a fixed effects or random effects panel model according to the White method. x a fitted model object of class lm; type a character string specifying the estimation type. const gives the usual estimate of the homoskedastic covariance matrix of the coefficient estimates: \\[ \\begin{align} V_{\\hat{\\beta}}^0 &amp;= \\sigma^2 (X&#39;X)^{-1} \\\\ \\hat{V}_{\\hat{\\beta}}^0 &amp;= s^2 (X&#39;X)^{-1} \\\\ s^2 &amp;= \\frac{1}{n-k} \\sum^n_{i=1}\\hat{e_i}^2 \\end{align} \\] ‚Äã The estimator \\(s^2\\) is unbiased for \\(\\sigma^2\\), known as the ‚Äúbias-corrected estimator‚Äù for \\(\\sigma^2\\). \\[ E[s^2]=\\sigma^2 \\] \"HC\" (or equivalently \"HC0\") gives White‚Äôs estimator \\[ \\begin{align*} V_{\\hat{\\beta}}^{HC0} &amp;= (X&#39;X)^{-1}(X&#39;DX)(X&#39;X)^{-1} \\\\ D &amp;= E[ee&#39;|X] \\\\ \\hat{V}_{\\hat{\\beta}}^{HC0} &amp;= (X&#39;X)^{-1} \\left( \\sum^n_{i=1} x_i x_i&#39;\\hat{e_i}^2 \\right) (X&#39;X)^{-1} \\end{align*} \\] The label ‚ÄúHC‚Äù refers to ‚Äúheteroskedasticity-consistent.‚Äù ‚ÄúHC0‚Äù refers to this being the baseline hetetroskedasticity-consistent covariance matrix estimator. The ‚ÄúHC0‚Äù is consistent even when the data are not independent, e.g., in the presence of serially correlated errors. HC1 uses the unbiased estimator \\(s^2\\), scaling the moment estimator \\(\\hat{\\sigma}^2\\) by \\(n/(n-k)\\). \\[ \\hat{V}_{\\hat{\\beta}}^{HC1} = \\left (\\frac{n}{n-k} \\right) (X&#39;X)^{-1} \\left( \\sum^n_{i=1} x_i x_i&#39;\\hat{e_i}^2 \\right) (X&#39;X)^{-1} \\] HC2 uses the standardized residuals \\(\\bar{e}_i\\) HC3 uses the prediction errors \\(\\widetilde{e}_i\\) cluster Observations may be clustered by \"group\" (\"time\") to account for serial (cross-sectional) correlation. sandwich::vcovHAC(x) Heteroscedasticity and autocorrelation consistent (HAC) estimation of the covariance matrix of the coefficient estimates in a (generalized) linear regression model x, refer to Newey &amp; West (1987, 1994). There are various methods to estimate the autoregressive effect. The default method is Andrews‚Äô (weights = weightsAndrews). For HAC standard errors to be valid, we need to assume that the autocorrelations go to zero as the tiem between observations increases (a condition necessary for stationarity), and we need a large sample, but it is not necessary to make a precise assumption about the autocorrelated error model. If you have balanced panel data, HAC estimator is overkill. You should use gee from the gee package instead specifying the covariance structure to AR-1 or similar. # Naive OLS &gt; vcov(lm_phillips) (Intercept) unemp (Intercept) 0.54830829 -0.08973175 unemp -0.08973175 0.01582211 # HC estimator &gt; vcovHC(lm_phillips) (Intercept) unemp (Intercept) 0.9319139 -0.16120691 unemp -0.1612069 0.02912351 # HAC estimator &gt; vcovHAC(lm_phillips) (Intercept) unemp (Intercept) 0.23561076 -0.039847043 unemp -0.03984704 0.006986272 vcovHAC tends to generate very small variance and make all coefficient significant. Be aware of that. To get a feel for how HAC standard errors are found, consider the simple regression model \\(y_{t}=\\beta_{1}+\\beta_{2}x_{t}+e_{t}\\). The variance of the least squares estimator \\(b_2\\) can be written as \\[ \\begin{aligned} \\text{var}(b_{2}) &amp; = \\sum_{t}{w_{t}^{2}\\text{var}(e_{t})} + \\sum_{t \\neq s}{\\sum{w_{t}w_{s}\\text{cov}(e_{t}, e_{s})}} \\\\ &amp; = \\sum_{t}{w_{t}^{2}\\text{var}(e_{t})} \\Bigg[1+\\frac{\\sum_{t \\neq s}{\\sum{w_{t}w_{s}\\text{cov}(e_{t}, e_{s})}}}{\\sum_{t}{w_{t}^{2}\\text{var}(e_{t})}}\\Bigg] \\end{aligned} \\] where \\(w_{t}=(x_{t}-\\bar{x})/\\sum_{t}{(x_{t}-x_{t-1})^{2}}\\). When the errors are not correlated, \\(\\text{cov}(e_{t},e_{t-1})=0\\), and the term in the square brackets is equal to one. The resulting expression \\(\\text{var}(b_{2})=\\sum_{t}{w_{t}^{2}}\\text{var}(e_{t})\\), is the on used to find HC standard errors. If we denote the quantity in the square brackets in the above equation as \\(g\\) and denote its estimate as \\(\\hat{g}\\), then the relationship between the two estimated variances is \\[ \\begin{aligned} \\widehat{\\text{Var}}_{HAC}(b_{2}) = \\widehat{\\text{Var}}_{HC}(b_{2}) \\times \\hat{g} \\end{aligned} \\] The HAC variance estimate is equal to the HC variance estimate multiplied by an extra term that depends on the serial correlation in errors. All these calculations depends on the fact that \\(x_te_t\\) are stationary variables, where \\(x_t\\) are the regressors and \\(e_t\\) are the disturbances. Stationarity is a bit restrictive property, so check whether it holds. References: vcovHC vs vcovHAC: https://stats.stackexchange.com/a/319922 Andrews DWK (1991). ‚ÄúHeteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation.‚Äù Econometrica, 59, 817‚Äì858. Newey WK &amp; West KD (1987). ‚ÄúA Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix.‚Äù Econometrica, 55, 703‚Äì708. Newey WK &amp; West KD (1994). ‚ÄúAutomatic Lag Selection in Covariance Matrix Estimation.‚Äù Review of Economic Studies, 61, 631‚Äì653. Zeileis A (2004). ‚ÄúEconometric Computing with HC and HAC Covariance Matrix Estimators.‚Äù Journal of Statistical Software, 11(10), 1‚Äì17. 9.1.3 Model performance broom::glance(lmfit) Several measures of model accuracy are computed for the entire regression, such as \\(R^2\\) and the F-statistic. &gt; library(broom) &gt; glance(reg_ml) # A tibble: 1 √ó 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.0341 0.0302 1.23 8.80 0.00331 1 -408. 821. 832. 378. df.residual nobs &lt;int&gt; &lt;int&gt; 1 249 251 Augment data with fitted predictions and residuals augment(lmfit) print fitted values and residuals for each of the original points in the regression. &gt; augment(reg_ml) # A tibble: 251 √ó 9 .rownames eRi rmrf .fitted .resid .hat .sigma .cooksd .std.resid &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.739 0.59 0.279 0.460 0.00541 1.23 0.000381 0.374 2 2 0.785 0.61 0.285 0.500 0.00553 1.23 0.000459 0.407 3 3 1.22 0.32 0.204 1.02 0.00427 1.23 0.00147 0.828 4 4 0.0542 -0.81 -0.113 0.167 0.00883 1.24 0.0000829 0.136 5 5 0.467 -0.59 -0.0515 0.519 0.00677 1.23 0.000608 0.422 6 6 1.03 1.14 0.434 0.592 0.0104 1.23 0.00122 0.483 7 7 0.361 0.09 0.139 0.222 0.00398 1.24 0.0000652 0.181 8 8 -1.31 -1.47 -0.298 -1.01 0.0184 1.23 0.00646 -0.829 9 9 -1.02 -0.92 -0.144 -0.873 0.0101 1.23 0.00258 -0.712 10 10 -2.50 -1.32 -0.256 -2.24 0.0158 1.23 0.0270 -1.83 # ‚Ñπ 241 more rows # ‚Ñπ Use `print(n = ...)` to see more rows plot.lm(x, which=c(1,2,3,5)) https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.lm.html which refers to which plots you want to show. ‚ÄúResiduals vs Fitted‚Äù plot ‚ÄúResidual Q-Q‚Äù plot ‚ÄúScale-Location‚Äù plot ‚ÄúCook‚Äôs distance‚Äù plot ‚ÄúResiduals vs Leverage‚Äù plot ‚ÄúCook‚Äôs dist vs Lev./(1-Lev.)‚Äù plot 9.1.4 Model Summary library(modelsummary) supports saving a table directly to file, raw output in console. Different formats are supported: html, tex, markdown, png. By default, modelsummary prints the coefficient‚Äôs standard error in parentheses below the corresponding estimate. The value of this uncertainty statistic is determined by the statisticargument. The statistic argument accepts any of the column names produced by get_estimates(model). modelsummary(models, statistic = &#39;std.error&#39;) # default: SE modelsummary(models, statistic = &#39;conf.int&#39;) # CI modelsummary(models, statistic = &#39;statistic&#39;) # t stat Several statistics under the coefficients. modelsummary(models, gof_omit = &quot;.*&quot;, statistic = c(&quot;conf.int&quot;, &quot;s.e. = {std.error}&quot;, &quot;t = {statistic}&quot;, &quot;p = {p.value}&quot;)) CI after coef in the same row modelsummary(models, gof_omit = &quot;.*&quot;, estimate = &quot;{estimate} [{conf.low}, {conf.high}]&quot;, statistic = NULL) The simplest way to summarize an unsupported model is to create a modelsummary_list object. A modelsummary_list is a list with two element that conform to the broom package specification: tidy and tidy is a data.frame with at least three columns: term, estimate, and std.error. glance. glance is a data.frame with only a single row, and where each column will be displayed at the bottom of the table in the goodness-of-fit section. ti &lt;- data.frame( term = c(&quot;coef1&quot;, &quot;coef2&quot;, &quot;coef3&quot;), estimate = 1:3, std.error = c(pi, exp(1), sqrt(2))) gl &lt;- data.frame( stat1 = &quot;blah&quot;, stat2 = &quot;blah blah&quot;) mod &lt;- list( tidy = ti, glance = gl) class(mod) &lt;- &quot;modelsummary_list&quot; modelsummary(mod) Reference: https://modelsummary.com/vignettes/modelsummary_extension.html 9.1.5 Dummy variable model.matrix creates a design (or model) matrix, e.g., by expanding factors to a set of dummy variables (depending on the contrasts) and expanding interactions similarly. model.matrix(object, data = environment(object), contrasts.arg = NULL, xlev = NULL, ...) object an object of an appropriate class. For the default method, a model formula or a termsobject. contrasts.arg a list, whose entries are values (numeric matrices, functions or character strings naming functions) to be used as replacement values for the contrasts replacement function and whose names are the names of columns of data containing factors. # expand our data frame so that every factor level of `x1` is represented in a dummy column model.matrix( ~ x1 - 1, data) # merge these dummies to our original data frame data_dummy &lt;- data.frame(data[ , ! colnames(data) %in% &quot;x1&quot;], # Create dummy data model.matrix( ~ x1 - 1, data)) data_dummy "],["9.2-panel.html", "9.2 Panel", " 9.2 Panel make.pbalanced(x, balance.type = \"fill\") is useful to make a panel balanced by filling missing time periods with NA values. This is useful when you include individual specific time trends in the model. plm::plm(formula, data, effect = c(\"individual\", \"time\", \"twoways\", \"nested\"), model = c(\"within\", \"random\", \"ht\", \"between\", \"pooling\", \"fd\"), index = NULL ) index the index attribute that describes individual and time dimensions; has to be the exact order, i.e.¬†entity first, can‚Äôt reverse; It can be: a vector of two character strings which contains the names of the individual and of the time indexes; a character string which is the name of the individual index variable. In this case, the time index is created automatically and a new variable called ‚Äútime‚Äù is added, assuming consecutive and ascending time periods in the order of the original data; an integer, the number of individuals. In this case, the data need to be a balanced panel and be organized as a stacked time series (successive blocks of individuals, each block being a time series for the respective individual) assuming consecutive and ascending time periods in the order of the original data. Two new variables are added: ‚Äúid‚Äù and ‚Äútime‚Äù which contain the individual and the time indexes. 9.2.1 Dynamic Panel data(&quot;EmplUK&quot;, package = &quot;plm&quot;) form &lt;- log(emp) ~ log(wage) + log(capital) + log(output) # Arellano and Bond (1991), table 4, col. b empl_ab &lt;- pgmm( dynformula(form, list(2, 1, 0, 1)), data = EmplUK, index = c(&quot;firm&quot;, &quot;year&quot;), effect = &quot;twoways&quot;, model = &quot;twosteps&quot;, gmm.inst = ~ log(emp), lag.gmm = list(c(2, 99)) ) \\[ \\boldsymbol \\Gamma_n = \\begin {pmatrix} \\gamma_0 &amp; \\gamma_1 &amp; \\gamma_2 &amp; \\cdots &amp; \\gamma_{n - 1} \\\\ \\gamma_1 &amp; \\gamma_0 &amp; \\gamma_1 &amp; \\cdots &amp; \\gamma_{n - 2} \\\\ \\gamma_2 &amp; \\gamma_1 &amp; \\gamma_0 &amp; \\cdots &amp; \\gamma_{n - 3} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\gamma_{n - 1} &amp; \\gamma_{n - 2} &amp; \\gamma_{n - 3} &amp; \\cdots &amp; \\gamma_0 \\end {pmatrix} \\] "],["10-machine-learning.html", "Chapter 10 Machine Learning", " Chapter 10 Machine Learning Parametric models such as generalized linear regression and logistic regression has advantages and disadvantages. Strength: The effects of individual predictors on the outcome are easily understood Statistical inference, such as hypothesis testing or interval estimation, is straightforward Methods and procedures for selecting, comparing, and summarizing these models are well-established and extensively studied Disadvantages in the following scenarios: Complex, non-linear relationships between predictors and the outcome High degrees of interaction between predictors Nominal outcome variables with several categories In these situations, non-parametric or algorithmic modeling approaches have the potential to better capture the underlying trends in the data. Here we introduce three models: classification and regression trees (CART), random forests, k-nearest neighbors. Classification and regression trees (CART) are ‚Äútrained‚Äù by recursively partitioning the ùëù-dimensional space (defined by the explanatory variables) until an acceptable level of homogeneity or ‚Äúpurity‚Äù is achieved within each partition. A major issue with tree-based models is that they tend to be high variance (leading to a high propensity towards over-fitting). Random forests are a non-parametric, tree-based modeling algorithm that is built upon the idea that averaging a set of independent elements yields an outcome with lower variability than any of the individual elements in the set. This general concept should seem familiar. Thinking back to your introductory statistics course, you should remember that the sample mean, \\(\\overline{x}\\), of a dataset has substantially less variability (\\(\\frac{\\sigma}{\\sqrt{n}}\\)) than the individual data-points themselves (\\(\\sigma\\)). Q: What is Bias-Variance Trade-Off in Machine Learning? A: Bias refers to error caused by a model for solving complex problems that is over simplified, makes significant assumptions, and misses important relationships in your data. Variance error is variability of a target function‚Äôs form with respect to different training sets. Models with small variance error will not change much if you replace couple of samples in training set. Models with high variance might be affected even with small changes in training set. High variance models fit the data too well, and learns the noise in addition to the inherent patterns in the data. "],["10.1-imbalanced-datasets.html", "10.1 Imbalanced datasets", " 10.1 Imbalanced datasets The following table provides generally accepted names and ranges for different degrees of imbalance: Percentage of data belonging to minority class Degree of imbalance 20-40% of the dataset Mild 1-20% of the dataset Moderate &lt;1% of the dataset Extreme For example, consider a virus detection dataset in which the minority class represents 0.5% of the dataset and the majority class represents 99.5%. Extremely imbalanced datasets like this one are common in medicine since most subjects won‚Äôt have the virus. Imbalanced datasets sometimes don‚Äôt contain enough minority class examples to train a model properly. That is, with so few positive labels, the model trains almost exclusively on negative labels and can‚Äôt learn enough about positive labels. For example, if the batch size is 50, many batches would contain no positive labels. Often, especially for mildly imbalanced and some moderately imbalanced datasets, imbalance isn‚Äôt a problem. So, you should first try training on the original dataset. If the model works well, you‚Äôre done. If not, at least the suboptimal model provides a good baseline for future experiments. Afterwards, you can try the following techniques to overcome problems caused by imbalanced datasets. 10.1.1 Downsampling and Upweighting Downsampling (in this context) means training on a disproportionately low subset of the majority class examples. Upweighting means adding an example weight to the downsampled class equal to the factor by which you downsampled. Reference: Imbalanced datasets, Google for Developers. "],["10.2-random-forest.html", "10.2 Random Forest", " 10.2 Random Forest Averaging of independent trees The goal of bagging is to produce \\(\\boldsymbol{B}\\) separate training datasets that are independent of each other (typically ùêµ is in the hundreds). The model of interest (in this case classification and regression trees) is trained separately on each of these datasets, resulting in \\(\\boldsymbol{B}\\) different estimated ‚Äúmodels‚Äù. These are then averaged to produce a single, low-variance estimate. Bagging is a general approach, but its most well-known application is in the random forest algorithm: Construct \\(\\boldsymbol{B}\\) bootstrap samples by sampling cases from the original dataset with replacement (this results in \\(\\boldsymbol{B}\\) unique datasets that are similar to the original) Fit a classification and regression tree to each sample, but randomly choose a subset of \\(m\\) variables that can be used in the construction of that tree (this results in \\(\\boldsymbol{B}\\) unique trees that are fit to similar datasets using different sets of predictors) For a given data-point, each of the \\(\\boldsymbol{B}\\) trees in the forest contributes a prediction or ‚Äúvote‚Äù, with the majority (or average) of these votes forming the random forest‚Äôs final prediction, \\(\\hat{y}_i\\) knitr::include_graphics(&quot;images/rf.png&quot;) A downside of both the CART and random forest algorithms (as well as many other algorithmic modeling approaches) is an inability to clearly quantify the roles played by individual variables in making predictions. However, the importance of individual variables in a random forest can still be expressed using a measure known as variable importance. The random forest algorithm requires the following tuning parameters be specified in order to run: ntree - the number of bagged samples, \\(\\boldsymbol{B}\\), onto which trees will be grown mtry - the number of variables that are randomly chosen to be candidates at each split Some sort of stopping criteria for individual trees, this can be: nodesize, which sets the minimum size of terminal nodes larger nodesize leads to shallower trees smaller node size allows for deeper, more complex trees maxnodes, which sets the maximum number of terminal nodes an individual tree can have. Applications of Random Forest Some of the applications of Random Forest Algorithm are listed below: Banking: It predicts a loan applicant‚Äôs solvency. This helps lending institutions make a good decision on whether to give the customer loan or not. They are also being used to detect fraudsters. Health Care: Health professionals use random forest systems to diagnose patients. Patients are diagnosed by assessing their previous medical history. Past medical records are reviewed to establish the proper dosage for the patients. Stock Market: Financial analysts use it to identify potential markets for stocks. It also enables them to remember the behaviour of stocks. E-Commerce: Through this system, e-commerce vendors can predict the preference of customers based on past consumption behaviour. When to Avoid Using Random Forests? Random Forests Algorithms are not ideal in the following situations: Extrapolation: Random Forest regression is not ideal in the extrapolation of data. Unlike linear regression, which uses existing observations to estimate values beyond the observation range. Sparse Data: Random Forest does not produce good results when the data is sparse. In this case, the subject of features and bootstrapped sample will have an invariant space. This will lead to unproductive spills, which will affect the outcome. FAQ Q: Is RF a linear or non-linear model? A: RF can capture complex, non-linear relationships. Q: Is RF sensitive to Imbalanced Data? A: Yes. It may perform poorly if the dataset is highly imbalanced like one class is significantly more frequent than another. Q: What is the loss function? A: Entropy/gini or any other loss function you want. Q: Difference btw RF and a linear model? A: A major difference is that a decision tree does not have ‚Äúparameters‚Äù, whereas the linear models need to create a functional form and find the optimal parameters. Q: Is RF regression a ‚Äútrue‚Äù regression? A: Random forests discretize continuous variables since they are based on decision trees, which function through recursive binary partitioning. But with sufficient data and sufficient splits, a step function with many small steps can approximate a smooth function. As for whether it is a ‚Äútrue‚Äù regression, this is somewhat semantic. After all, piecewise regression is regression too, but is also not smooth. As is any regression with a categorical predictor. Source. Implementation in R ranger package offers a computation efficient function for RF. RF_ranger &lt;- ranger(formula = formula, data = data_before[idx,], probability = TRUE, importance = &quot;permutation&quot;, scale.permutation.importance = TRUE, ) # print(RF_ranger) rf.pred.test &lt;- predict(RF_ranger, data=data_before[-idx,])$predictions Parameters controlling the general process of RF: probability=FALSE: Whether to forecast a probability forest. The hyperparameters mtry, min.node.size and sample.fraction determine the degree of randomness, and should be tuned. mtry=500: Number of variables to possibly split at in each node in one tree. In plain language, it indicates how many predictor variables should be used in each tree. Default is the (rounded down) square root of the number variables. Alternatively, a single argument function returning an integer, given the number of independent variables. Range btw 1 to the number of predictors. If all predictors are used, then this corresponds in fact to bagging. min.node.size: The number of observations a terminal node should at least have. Default 1 for classification, 5 for regression, 3 for survival, and 10 for probability. For classification, this can be a vector of class-specific values. Range between 1 and 10 sample.fraction: Fraction of observations to be used in each tree. Default is 1 for sampling with replacement and 0.632 for sampling without replacement. For classification, this can be a vector of class-specific values. Smaller fractions lead to greater diversity, and thus less correlated trees which often is desirable. Range between 0.2 and 0.9 Parameters controlling what and how intermediate results are saved: keep.inbag = FALSE: Whether to save how often observations are in-bag in each tree. Set to TRUE if you want to check sample composition in each tree. importance = 'none'|'impurity'|'impurity_corrected'|'permutation': Variable importance mode. scale.permutation.importance = FALSE: Whether to scale permutation importance by standard error as in (Breiman 2001). Only applicable if 'permutation' variable importance mode selected. write.forest = TRUE: Whether to save ranger.forest object, required for prediction. Set to FALSE to reduce memory usage if no prediction intended. Set to FALSE when you do parameter tuning. Q: How to tune hyperparameters? A: Check out mlr3 package. Here is an example. Imbalance Classification You can balance your random forests using case weights. Here‚Äôs a simple example: library(ranger) # Make a dataste set.seed(43) nrow &lt;- 1000 ncol &lt;- 10 X &lt;- matrix(rnorm(nrow * ncol), ncol=ncol) CF &lt;- rnorm(ncol) Y &lt;- (X %*% CF + rnorm(nrow))[,1] Y &lt;- as.integer(Y &gt; quantile(Y, 0.90)) table(Y) # Compute weights to balance the RF w &lt;- 1/table(Y) w &lt;- w/sum(w) weights &lt;- rep(0, nrow) weights[Y == 0] &lt;- w[&#39;0&#39;] weights[Y == 1] &lt;- w[&#39;1&#39;] table(weights, Y) # Fit the RF data &lt;- data.frame(Y=factor(ifelse(Y==0, &#39;no&#39;, &#39;yes&#39;)), X) model &lt;- ranger(Y~., data, case.weights=weights) print(model) Code Source: https://stats.stackexchange.com/a/287849 Fixed proportion sampling: https://github.com/imbs-hl/ranger/issues/167 References: https://remiller1450.github.io/m257s21/Lab10_Other_Models.html "],["10.3-neural-network.html", "10.3 Neural Network", " 10.3 Neural Network Neural networks are made up objects called ‚Äúlayers‚Äù and ‚Äúneurons‚Äù and these things connect to each other in a specific way. Each layer has some number of neurons. For example, the first layer might have 10 neurons, the second might have 15, and so on. The number of layers and the number of neurons in each layer is a ‚Äúhyperparameter‚Äù, the user picks how many of each. Let‚Äôs take a look at a single neuron. \\[ v_3^{(1)} = g(\\boldsymbol{w}_3^{(1)}\\boldsymbol{x} + b_3^{(1)}) \\] The LHS, \\(v_3^{(1)}\\), will be the output. The superscript \\((1)\\) refers to the layer number; the subscript \\(3\\) refers to the neuron. An output here means just a single number. If we have say 15 neurons (subscript) for this first layer (superscript), then we will have 15 numbers come out of this first layer: \\(\\boldsymbol{v^{(1)}} = \\{v^{(1)}_{1}, v^{(1)}_{1}, ..., v^{(1)}_{15}\\}\\) where the bolded \\(v\\) means a vector. \\(\\boldsymbol{x}\\) is our input vector. \\(\\boldsymbol{w}\\) is the weight/coefficient vector. \\(b\\) is a bias or the intercept term, shifting the value of \\(\\boldsymbol{w}\\cdot\\boldsymbol{x}\\) up or down. \\(g\\) refers to a ‚Äúnon-linear‚Äù function, often called as the ‚Äúactivation function‚Äù. "],["11-stata.html", "Chapter 11 Stata", " Chapter 11 Stata Resources: User Guide: https://www.stata.com/manuals/u.pdf Tutorial: https://grodri.github.io/stata/ Quick start: https://lanekenworthy.net/stata-quick-guide/ [GSM] Getting Started with Stata for Mac help &lt;cmd_name&gt;: Get help for a command in Stata console. Overview of Documentation: [U] User‚Äôs Guide: is divided into three sections: Stata basics, Elements of Stata, and Advice. Recommended to read. Base Reference Manual: list commands alphabetically. Not designed to be read from cover to cover. The PDF documentation may be accessed from within Stata. help command_name and then click on the ‚ÄúView complete PDF manual entry‚Äù button under the command. Or in the menu bar, Help &gt; PDF Documentation to open the complete PDF documentation. The pdf documentation uses Acrobat Reader as the viewer. Tip: use finger pinch to zoom in and out. When using the zoom button or cmd +/cmd -, the text jumps around, you lose your original position. Keyboard Shortcuts Actually not of much use. Keyboard Shortcut Description ctrl + R last cmd ctrl + B next cmd cmd + shift + D Run a Do file User interface Within the Stata interface window, there are five windows: Command, Results, History, Properties, and Variables. Output appears in the Results window. E.g., . sysuse auto, clear (1978 Automobile Data) The dot (.) indicates that the current line is a Stata command. &gt; indicates that the command is not yet complete. You will see this when you have a command that spans multiple lines. While Stata can be command-driven by typing code in the Command window, it can also be used in a point-and-click manner using the menu bar. While nearly everything in Stata can be done via the menus, you‚Äôre better off typing commands into a word processing file and saving them, then copying-and-pasting them into the Stata ‚ÄúCommand‚Äù window. Buttons Log: Track and save output from the Results window. Ensures replicability. New Do-file Editor: Organize your history commands in one place, making debugging easier. You can use do-files to create a batchlike environment in which you place all the commands you want to perform in a file and then instruct Stata to do that file. Ex. You have a do file myjob.do, you can run do myjob all commands in the do file would be sourced. Do-file It is recommended to run do files as a whole. (This is different than R.) You cannot re-run commands freely in Stata. For example, if you run a command that creates a variable x, realize you made a mistake, and then fix it, you can‚Äôt simply select the command that creates x and run it again because x already exists. You could manually drop the existing version of x, but now you‚Äôre doing things in a non-reproducible way. Running the entire do file will eliminate this problem because it reloads the data from disk every time. If you find yourself getting confused by these kinds of issues, run the entire do file rather than a selection. Do-file Rule of Thumb Your .do file begins with loading a dataset and ends with saving one. use ..., clear // begin /* your code */ save ..., replace // end Never modify the raw data files. Save the results of your data cleaning in a new file. Every data file is created by a script. Convert your interactive data cleaning session to a .do file. No data file is modified by multiple scripts. Intermediate steps are saved in different files (or kept in temporary files). Keep do files short Our suggestion is that you keep your do files short enough that when you‚Äôre working on one of them you can easily wrap your head around it. You also want to keep do files short so they run as quickly as possible: working on a do file usually requires running it repeatedly, so moving any code that you consider ‚Äúdone‚Äù to a different do file will save time. Project Structure You can have a master do file which loads your small section do files sequentially and all in one. Enumerate your do files. Example: 0-master.do, 1-data-clean.do, 2-stylized-facts.do, ‚Ä¶ You can then organize them in sub-do-files: if you have different set of stylized facts, you could have: 2.1-stylized-facts-geography.do, 2.2-stylized-facts-count.do etc. . . . log file log files put everything that your do file put in the Results window. Comments // for single line comment; rest-of-line comment; /* */ for multiple line comment; enclosed comment; //# or **# add a bookmark /// line-join indicator Note that the // comment indicator and the /// indicator must be preceded by one or more blanks. See [U] 16.1.2 Comments and blank lines in do-files for more details. Continuation lines: /// /// is called the line-join indicator or line continuation marker. It makes long lines more readable. Everything after /// to the end of the current line is considered a comment. The next line joins with the current line. Therefore, /// allows you to split long lines across multiple lines in the do-file. Summary of ways to break long lines: You can change the end-of-line delimiter to ; by using #delimit, #delimit ; // change the line delimiter to semicolon summarize weight price displ headroom rep78 length turn gear_ratio if substr(company,1,4)==&quot;Ford&quot; | substr(company,1,2)==&quot;GM&quot;, detail ; gen byte ford = substr(company,1,4)==&quot;Ford&quot; ; #delimit cr // change the delimiter back to carriage return gen byte gm = substr(company,1,2)==&quot;GM&quot; Once you declear #delimit ;, all lines must end in ;. Stata treats carriage returns as no different from blanks. you can comment out the line break by using /* */ comment delimiters, or you can use the /// line-join indicator. Example replace final_result = /// sqrt(first_side^2 + second_side^2) /// if type == &quot;rectangle&quot; equivalently, you can use /* */ to break long lines: replace final_result = /* */ sqrt(first_side^2 + second_side^2) /* */ if type == &quot;rectangle&quot; N.B. There‚Äôs NO line continuation marker (///) in the command window. In the command window, the enter key sends what has been written on the line to Stata. There is no way to continue a long command on a second line, without sending the first (incomplete) line to Stata. You can add comments after ///. args a /// input parameter for a b /// input parameter for b c // input parameter for c is equivalent to args a b c cd \"directory_name\" change working directory. don‚Äôt need quotation if there is no space need quotation if the directory has spaces pwd displays the path of the current working directory. exit, clear to quit Stata. If the dataset in memory has changed since the last time it was saved, Stata will refuse to quit unless you specify clear. Abbreviation rules: Stata allows abbreviations. You can abbreviate commands, variable names, and options. // full command . summarize myvar, detail // use abbr. to achieve the same function . sum myv, d As a general rule, command, option, and variable names may be abbreviated to the shortest string of characters that uniquely identifies them. When you read the Stata manual, it uses underlines to denote the minimal abbreviation for a command or option. E.g. When you see append, it means you can use ap to denote append. describe means the shortest allowable abbreviation for describe is desc. If there is no underlining, no abbreviation is allowed. rename can be abbreviated ren, rena, renam, or it can be spelled out in its entirety. Open do files in tabs rather than in separate windows: https://www.reddit.com/r/stata/comments/1ivjegr/stata_18_mac_does_not_do_tabs_for_dofile_editor/ "],["11.1-basic-syntax.html", "11.1 Basic syntax", " 11.1 Basic syntax 11.1.1 Package management Users can add new features to Stata, and some users choose to make new features that they have written available to others via the web. The files that comprise a new feature are called a package, and a package usually consists of one or more ado-files and help files. ssc install newpkgname: Install newpkgname from ssc. The SSC (Statistical Software Components) is the premier Stata download site. ssc uninstall pkgname to uninstall pkgname ado update to update packages ssc hot [, n(#)] a list of most popular pkgs at SSC. n(#) to specify the number of pkgs listed. Stata is case-sensitive: myvar, Myvar and MYVAR are three distinct names. Semicolons (:) is treated as a line separator. It is not required, but it may be used to place two statements on the same physical line: x = 1 ; y = 2 ; The last semicolon in the above example is unnecessary but allowed. 11.1.1.1 Types and Declarations A variable‚Äôs type can be described in two perspectives: eltype: specifies the type of the elements. Default: transmorphic. orgtype: specifies the organization of the elements. Default: matrix. eltype orgtype transmorphic matrix numeric vector real rowvector complex colvector string scalar pointer [by varlist:] command [ varlist ] [=exp] [if exp] [in range] [ weight ] [, options] where square brackets distinguish optional qualifiers and options from required ones. In this diagram, varlist denotes a list of variable names, If no varlist appears, most commands assumes _all, which indicate all the variables in the dataset. command denotes a Stata command, exp denotes an algebraic expression, range denotes an observation range, weight denotes a weighting expression, and options denotes a list of options. Note the comma , which separate the command‚Äôs main body to options. by varlist repeat a cmd for each subset of the data, grouped by varlist. Ex: group by region and summarize marriage divorce sysuse census sort region by region: summarize marriage divorce Note that your have to sort before by varlist. Alternatively, you can by region, sort: summarize marriage_rate divorce_rate if exp filter observations for which exp returns true summarize marriage_rate divorce_rate if region == &quot;West&quot; &amp; (and) and | (or) to join conditions. in range restricts the scope of the cmd to be applied to a specific observation range. First observation can be denoted by f Last observation can be denoted by l Negative numbers mean ‚Äúfrom the end of the data‚Äù // summarize for observations 5 to 25 summarize marriage_rate divorce_rate in 5/25 // summarize for the last five observations summarize marriage_rate divorce_rate in -5/l 11.1.1.2 Create new variables gen variable = expression // generate new variables replace variable = expression // replace the value of existing variables generate create variables based on expressions you specified. generate newvar = oldvar + 2 generate a new variable newvar, which equals oldvar + 2 generate lngdp = ln(gdp) generate the natural log of gdp generate exp2 = exp^2 generate the square of exp egen: Extensions to generate; creates a new variable based on egen functions of existing variables. Q: What are egen functions? A: The functions are specifically written for egen. // Generate newv1 for distinct groups of v1 and v2, and create and apply value label mylabel egen newv1 = group(v1 v2), label(mylabel) // for each country, calculate the average of wpop by country_id, sort: egen pop_country = mean(wpop) gen vs.¬†egen gen used for simple algebraic transformations egen for more complexed transformations, e.g., operations based on groups of observations. They behave differently if you want to calcualte the sum per group. gen returns running sum egen returns group sum // Create variable containing the running sum of x generate runsum = sum(x) // Create variable containing a constant equal to the overall sum of x egen totalsum = total(x) encode var, gen(newvar) creates a new variable named newvar based on the string variable varname. It alphabetizes unique values in var and assigns numeric codes to each entry. encode sex, gen(gender) // nolabel drops value labels and show how the data really appear list sex gender in 1/4, nolabel // you won&#39;t see difference using the following cmd list sex gender in 1/4 sex is a string variable and takes on values female and male. encode creates a new variable gender, mapping each level in sex to a numerical value. female becomes 1 and male becomes 2. display displays strings and values of scalar expressions. display [display_directive [display_directive [...]]] list displays the values of variables. If no varlist is specified, the values of all the variables are displayed. list [varlist] [if] [in] [, options] 11.1.1.3 Refer to a range of variables How can I list, drop, and keep a consecutive set of variables without typing the names individually? list all variables starting with a certain prefix list all variables between two variables combination of the two // list all variables starting with a certain prefix . list var* // all variables starting with &quot;var&quot; // list all variables between two variables . list var1-var5 // all variables between var1 and var5 // combination of the two . list var1 var3-var5 If you want to consider reordering the variables in your dataset, order, sequential will put the variables in alphabetical order (and does mostly smart things with numeric suffixes). . order *, sequential the resulting order will be: 1. alpha 2. beta 3. gamma 4. v1 5. v2 6. v3 7. v4 order, sequential is smart enough to know that v10 comes after v9 and not between v1 and v2, which pure alphabetical order would specify. For online help, type help order in Stata, or see [D] order. 11.1.2 System Variables Expressions may also contain variables (pronounced ‚Äúunderscore variables‚Äù), which are built-in system variables that are created and updated by Stata. They are called variables because their names all begin with the underscore character, _. Var Description _n the number of the current observation. _N the total number of observations in the dataset or the number of observations in the current by() group. _pi \\(\\pi\\) [eqno]_b[varname] or [eqno]_coef[varname] value of the coefficient on varname from the most recently fitted model [eqno]_se[varname] standard error of the coefficient on varname from the most recently fit model _b[_cons] value of the intercept term 11.1.3 Matrix You enter the matrices by row, separate one element from the next by using commas (,) and one row from the next by using backslashes (\\). To create \\[ A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} \\] matrix [input] a = (1,2\\3,4) matrix list a input is optional. without input, matrix must be small, can include expressions. with input, matrix can be large, but no expressions for the elements. Menu: Data &gt; Matrices, ado language &gt; Input matrix by hand Get one element using matname[r,c] to get r row, c column element. matrix rownames and colnames reset the row and column names of an already existing matrix. matrix roweq and coleq also reset the row and column names of an already existing matrix, but if a simple name (a name without a colon) is specified, it is interpreted as an equation name. // Reset row names of matrix matrix rownames A = names matrix colnames A = names // Reset row names and interpret simple names as equation names matrix roweq A = names matrix coleq A = names A is a matrix. name can be: a simple name; var an interaction; e.g., var1#var2 a colon followed by a simple name; a colon followed by an interaction; an equation name followed by a colon, and a simple name; e.g., myeq:var an equation name, a colon, and an interaction, e.g., myeq:var1#var2 Matrix define: https://www.stata.com/manuals/pmatrixdefine.pdf#pmatrixdefine Macro functions rownames A and colnames A return a list of all the row or column subnames (with time-series operators if applicable) of A, separated by single blanks. The equation names, even if present, are not included. roweq A and coleq A return a list of all the row equation names or column equation names of A, separated by single blanks, and with each name appearing however many times it appears in the matrix. rowfullnames A and colfullnames A return a list of all the row or column names, including equation names of A, separated by single blanks. 11.1.4 Factor Variables help fvvarlist for documentation on factor variables. i.varname create indicators for each level of the variable // group=1 as base level list group i.group in 1/5 // group=3 as base level list group i3.group in 1/5 // individual fixed effects regress y i.group c.varname treat as continuous # cross, create an interaction for each combination of the variables. Spaces are not allowed in interactions. ## factorial cross, a full factorial of the variables: standalone effects for each variable and an interaction group##sex // equivalently i.group i.sex i.group#i.sex o.varname omit a variable or indicator o.age means that the continuous variable age should be omitted, and o2.group means that the indicator for group = 2 should be omitted. Interaction Expansion xi [ , prefix(string) noomit ] term(s) xi expands terms containing categorical variables into indicator (also called dummy) variable sets. xi provides a convenient way to include dummy or indicator variables when fitting a model that does NOT support factor variables, e.g., xtabond. We recommend that you use factor variables instead of xi if a command allows factor variables. By default, xi will create interaction variables starting with _I. This can be changed using the prefix(string) option. Operator Description i.varname creates dummies for categorical variable varname i.varname1*i.varname2 creates dummies for categorical variables varname1 and varname2: main effects and all interactions i.varname1*varname3 creates dummies for categorical variable varname1 and continuous variable varname3: main effects and all interactions i.varname1|varname3 creates dummies for categorical variable varname1 and continuous variable varname3: all interactions and main effect of varname3, but NO main effect of varname1 xi expands both numeric and string categorical variables. agegrp takes on values 1, 2,3, and 4. xi: logistic outcome i.agegrp xi tabulates i.agegrp creates indicator (dummy) variables for each observed value, omitting the indicator for the smallest value. This creates variables name -Iagegrp2, -Iagegrp3, and -Iagegrp4. // The expanded logistic model is logistic outcome _Iagegrp_2 _Iagegrp_3 _Iagegrp_4 Dummy variables are created automatically and are left in your dataset. You can drop them by typing drop I*. You do not have to do this; each time you use xi, any automatically generated dummies with the same prefix as the one specified in the prefix(string) option, or _I by default, are dropped and new ones are created. Use xi as a command prefix // simple effects xi: logistic outcome weight i.agegrp bp // interactions of categorical variables xi: logistic outcome weight bp i.agegrp*i.race // interactions of dummy variables with continuous variables // fits a model with indicator variables for all agegrp categories interacted with weight, plus the maineffect terms weight and i.agegrp. xi: logistic outcome bp i.agegrp*weight i.race // interaction terms without the agegrp main effect (but with the weight main effect) xi: logistic outcome bp i.agegrp|weight i.race 11.1.5 Time series varlists Three time series operators: L., D. and S.. First convert variables to time variables by using tsset, then you can use the TS operators. tsset time list L.gnp Convert to panel tsset country year // or xtset country year TS Operator Meaning L. lag \\(x_{t-1}\\) L2. 2-period lag \\(x_{t-2}\\) L(1/2). a varlist \\(x_{t-1}\\) and \\(x_{t-2}\\) F. lead \\(x_{t+1}\\) F2. 2-period lead \\(x_{t+2}\\) D. difference \\(x_{t}-x_{t-1}\\) D2. difference of difference \\((x_{t}-x_{t-1})-(x_{t-1}-x_{t-2})\\) S. ‚Äúseasonal‚Äù difference \\(x_{t}-x_{t-1}\\) S2. lag-2 seasonal difference \\(x_{t}-x_{t-2}\\) Note that D1. = S1., but D2. \\(\\ne\\) S2.. D2. refers to the difference of difference S2. refers to the two-period difference Operators may be typed in uppercase or lowercase L(1/3).(gnp cpi) // equivalently L.gnp L2.gnp L3.gnp L.cpi L2.cpi L3.cpi DS12.gnp one-period difference of the 12-period difference .do is a Stata do-file. .dta is Stata dataset file format 11.1.6 Labels Variable labels convey information about a variable, and can be a substitute for long variable names. // generally label variable variable_name &quot;variable label&quot; // use example label variable price &quot;Price in 1978 Dollars&quot; Value labels are used with categorical variables to tell you what the categories mean. First define a mapping // generally label define map_name value1 &quot;label1&quot; value2 &quot;label2&quot;... // use example label define rep_label 1 &quot;Bad&quot; 2 &quot;Average&quot; 3 &quot;Good&quot; Add value labels to existing variables // generally label values map_name // use example label values rep3 rep_label 11.1.7 Output format % indicates the start of a format specification. %9.2f means a floating-point number with 9 characters wide, including 2 digits after the decimal point. the first digit states the width of the results the second digit after the decimal point states the number of digits after the decimal point f for fixed format. Alternatively, e for scientific notation ref: [U] 12.5 Data: Formats, control how data are displayed "],["11.2-data-manipulation.html", "11.2 Data Manipulation", " 11.2 Data Manipulation 11.2.1 Import and Export Shipped datasets Stata contains some demonstration datasets in the system directories. sysuse dir: list the names of shipped datasets. sysuse lifeexp: use lifeexp Note that use lifeexp will return error. Data not found. User datasets .dta use myauto [, clear]: Load myauto.dta (Stata-format) into memory. clear it is okay to replace the data in memory, even though the current data have not been saved to disk. save myauto [, replace]: Create a Stata data type file myauto.dta replace allows Stata to overwrite existing dataset that is the output from previous attempts to run the do file. .csv import delimited myauto.csv: Import myauto.csv to Stata‚Äôs memory export delimited myauto.csv‚Äù Export to myauto.csv import delimited filename reads text (ASCII) files in which there is one observation per line and the values are separated by commas, tabs, or some other delimiter. By default, Stat will check if the file is delimited by tabs or commas based on the first line of data. export delimited filename writes data into a file in comma-separated (.csv) format by default. You can specify any separation character delimiter that you prefer. If filename is specified without an extension, .csv is assumed. If filename contains embedded spaces, enclose it in double quotes. import delimited [using] filename [, import_delimited_options] Options delimiters(\"chars\"[, collapse | asstring] ): \"chars\" specifies the delimiter \";\": uses semicolon as a delimiter; \"\\t\" uses tab, \"whitespace\" uses whitespace collapse treat multiple consecutive delimiters as just one delimiter. asstring treat chars as one delimiter. By default, each character in chars is treated as an individual delimiter. // use example import delimited auto, delim(&quot; &quot;, collapse) colrange(:3) rowrange(8) clear replace data in memory 11.2.2 Save Estimation Results estimates store model_name stores the current (active) estimation results under the name model_name. // Store estimation results as m1 for use later in the same session . estimates store m1 // to get them back . estimates restore m1 // Find out what you have stored . estimates dir estimate save saves the current active estimation results to a file with the extension .ster. // Save the current active estimation results . estimate save basemodel file basemodel.ster saved In a different session, you can reload those results: // Load the saved estimation results . estimates use basemodel // Display the results . estimates table Q: What is the difference between estimates store and estimates save? A: Once estimation results are stored, you can use other estimates commands to produce tables and reports from them. estimates table [namelist] [, options] organizes estimation results from one or more models in a single formatted table. If you type estimates table without arguments, a table of the most recent estimation coefficients will be shown. // Display a table of coefficients for stored estimates m1 and m2 estimates table m1 m2 // with SE estimates table m1 m2, se // with sample size, adjusted ùëÖ2, and stars estimates table m1 m2, stats(N r2_a) star You can add more results to show using options: stats(scalarlist) reports additional statistics in the table. Below are commonly used result identifiers: N for sample size r2_a for adjusted \\(R^2\\) r2 for \\(R^2\\) F for F-statistic chi2 for chi-squared statistic p for p-value stats(N r2_a) to show sample size and adjusted \\(R^2\\) star shows stars for significance levels. By default, star(.05 .01 .001), which uses the following significance levels: * for \\(p &lt; 0.05\\) ** for \\(p &lt; 0.01\\) *** for \\(p &lt; 0.001\\) You can change the significance levels using star(.1 .05 .01) to set the levels to 0.10, 0.05, and 0.01, respectively. N.B. the star option may not be combined with the se, t, or p option. An error will be returned if you try to combine them: . estimate table, star se t p star option star not allowed b[%fmt] how to format the coefficients. se[%fmt] show standard errors and use optional format t[%fmt] show \\(t\\) or \\(z\\) statistics and use optional format p[%fmt] show \\(p\\) values and use optional format varlabel display variable labels rather than variable names // show stars for sig. levels . estimates table, star // show se, t, and p values . estimates table, se t p All statistics are shown in order under the coefficients. If you have a long list of variables, the table can be very long. You can use keep(varlist) to keep only the variables you want to show in the table. varlist is a list of variables you want to keep in the table. A list of variables can be specified as keep(var1 var2 var3). Names are separated by spaces. Not possible to use variable ranges, e.g., keep(var1-var3) will return an error. When you have multiple equations, use eqn_name:varname to specify the variable in a specific equation. Example of a long variable list estimates table, keep(L1.logd_gdp tmp tmp2 pre pre2 tmp_pre tmp2_pre tmp_pre2 tmp2_pre2) se t p etable etable allows you to easily create a table of estimation results and export it to a variety of file types, e.g., docx, html, pdf, xlsx, tex, txt, markdown, md. // use example of etable . clear all . webuse nhanes2l (Second National Health and Nutrition Examination Survey) . quietly regress bpsystol age weight i.region . estimates store model1 . quietly regress bpsystol i.sex weight i.agegrp . estimates store model2 . quietly regress bpsystol age weight i.agegrp . estimates store model3 . etable, estimates(model1 model2 model3) showstars showstarsnote title(&quot;Table 1. Models for systolic blood pressure&quot;) export(mydoc.docx, replace) Options: showstars and showstarsnote shows stars and notes for significance levels. export allows you to specify the output format Alternative to etable: eststo. 11.2.3 Stored Results Stata commands that report results also store the results where they can be subsequently used by other commands or programs. This is documented in the Stored results section of the particular command in the reference manuals. r-class commands, such as summarize, store their results in r(); most commands are r-class. e-class commands, such as regress, store their results in e(); e-class commands are Stata‚Äôs model estimation commands. // for r-class command return list // for e-class command ereturn list Most estimation commands leave behind e(b) the coefficient vector, and e(V) the variance‚Äìcovariance matrix of the estimates (VCE) // display coef vector matrix list e(b) // assign it to a variable matrix myb = e(b) matrix list myb You can refer to e(b) and e(V) in any matrix expression: matrix c = e(b)*invsym(e(V))*e(b)‚Äô matrix list c invsym(e(V)) returns the inverse of e(V). Generally, invsym requires a a square, symmetric, and positive-definite matrix. "],["11.3-predict.html", "11.3 Predict", " 11.3 Predict Refer to [U] 20.11 Obtaining predicted values. predict calculates predictions, residuals, influence statistics, and the like after estimation. Exactly what predict can do is determined by the previous estimation command; command-specific options are documented with each estimation command. Regardless of command-specific options, the actions of predict share certain similarities across estimation commands: predict [type] newvar [if] [in] [, single_options] predict newvar1 create newvar1 containing ‚Äúpredicted values‚Äù, i.e., \\(\\hat{y}_i = \\E(y_i\\mid \\bx_i)\\) For linear regression models, \\(\\hat{y}_i = \\bx_i&#39;\\hat{\\bbeta.}\\) For probit/logit models, \\(\\hat{y}_i = F(\\bx_i&#39;\\hat{\\bbeta}),\\) where \\(F(.)\\) is the logistic or normal cumulative distribution function. predict newvar2, xb create newvar2 containing the linear prediction Option xb means calculating the linear prediction, \\(\\bx_i&#39;\\hat{\\bbeta},\\) from the fitted model. Note that in case of a linear regression model, predict fitted and predict fitted, xb will give you the same result. The difference is that for probit/logit models, predict fitted gives you the predicted probability, while predict fitted, xb gives you the logit or probit index. predict newvar2 if e(sample), xb Same as above, but only for observations used to fit the model in the previous estimation, i.e., in-sample predictions. e(sample): return \\(1\\) if the observation is in the estimation sample and \\(0\\) otherwise. predict can be used in out-of-sample predictions, which extends beyond the estimation sample. You can load a new dataset and type predict to obtain results for that sample. use dataset1 /* estimation dataset */ (fit a model) use dataset2 /* forecast dataset */ predict yhat /* fill in the predictions */ predict e, residuals will generate a variable e containing the residuals of the estimation \\[ e_i = y_i - \\hat{y}_i \\] Consider the linear prediction \\[ \\begin{split} \\hat{y}_i &amp;= \\bx_i&#39;\\hat{\\bbeta} \\\\ &amp;= \\hat{\\beta}_1x_{1i} + \\hat{\\beta}_2x_{2i} + \\cdots + \\hat{\\beta}_Kx_{Ki} . \\end{split} \\] \\(\\hat{y}_i\\) is called the predcited values for in-sample predictions forecasts for out-of-sample predictions For logit or probit, \\(\\bx_i&#39;\\hat{\\bbeta}\\) is called the logit or probit index. The predicted probability is \\(p_i=\\hat{y}_i=F(\\bx_i&#39;\\hat{\\bbeta}),\\) where \\(F(.)\\) is the logistic or normal cumulative distribution function. For probit, \\(\\hat{y}_i=\\Phi(\\bx_i&#39;\\hat{\\bbeta})\\) . \\(x_{1i},\\) \\(x_{2i},\\) \\(\\ldots,\\) \\(x_{Ki}\\) are obtained from the data currently in memory and do NOT necessarily correspond to the data on the independent variables used to fit the model (obtaining \\(\\hat{\\beta}_1,\\) \\(\\hat{\\beta}_2,\\) \\(\\ldots,\\) \\(\\hat{\\beta}_K\\)). "],["11.4-forecast.html", "11.4 Forecast", " 11.4 Forecast Refers to [U] 20.21 Dynamic forecasts and simulations for a quick overview. [TS] forecast for detailed documentation. Foreceast: out-of-sample forecast works with time-series and panel datasets, and you can obtain either dynamic or staticforecasts. Dynamic forecasts use previous periods‚Äô forecast values wherever lags appear in the model‚Äôs equations and thus allow you to obtain forecasts for multiple periods in the future. Static forecasts use previous periods‚Äô actual values wherever lags appear in the model‚Äôs equations, so if you use lags, you cannot make predictions much beyond the end of the time horizon in your dataset. However, static forecasts are useful during model development. Note: Dynamic vs Static forecasts do not indicate whether the model itself is dynamic or static. It refers to how lagged values are treated when making forecasts. Quick takeaway: Using dynamic forecasts to make predictions multiple periods into the future where you do not have observations for the lagged dependent variable. You can incorporate outside information into your forecasts, and you can specify a future path for some of the model‚Äôs variables and obtain forecasts for the other variables conditional on that path. These features allow you to produce forecasts under different scenarios, and they allow you to explore how different policy interventions would affect your forecasts. Before we are able to forecast, we must populate the exogenous variables over the entire forecast horizon before solving our model. Ê∑ªÂä†Êï∞ÊçÆ Solving our model: means obtain forecast from our model. 11.4.1 Essential Procedure Estimate the model Here we use arima model as an example. arima y2 x3 y1, ar(1) ma(1) Store the estimation results using estimate store estimate store myarima Create a forecast model using forecast create. This initialize a new model; we will call the model mymodel. forecast create mymodel The name you give the model mainly controls how output from forecast commands is labeled. More importantly, forecast create creates the internal data structures Stata uses to keep track of your model. Add all equations to the model you just created using forecast estimates. The following command adds the stored estimation results in myarima to the current model mymodel. forecast estimates myarima Compute dynamic forecasts from 2012 to 2024 forecast solve, begin(2012) end(2024) 11.4.2 Creates a new forecast model forecast create [ name ] [ , replace ] The forecast create command creates a new forecast model in Stata. You must create a model before you can add equations or solve it. You can have only one model in memory at a time. You may optionally specify a name for your model. That name will appear in the output produced by the various forecast subcommands. replace clear the existing model from memory before creating name. By default, forecast create issues an error message if another model is already in memory. Note that you can add multiple equations to a forecast model. 11.4.3 Add equations/identifies Add estimation results to a forecast model currently in memory. forecast estimates modelname [, options] modelname is the name of a stored estimation result being added; it is generated by estimates store modelname. Options: predict(p_options): call predict using p_options names(newnamelist[ , replace]): use newnamelist for the names of left-hand-side (LHS) variables in the estimation result being added, i.e., modelname. forecast estimates creates a new variable in the dataset for each element of namelist. You MUST use this option of any of the LHS variables contains time series operators, e.g., D., L.. If a variable of the same name already exists in your dataset, forecast estimates exits with an error unless you specify the replace option, in which case existing variables are overwritten. // use example forecast estimates myestimates Add estimation results stored in myestimates to the forecast model currently in memory. 11.4.3.1 Add an Identity to a forecast Model forecast identity varname = exp An identity is a nonstochastic equation that expresses an endogenous variable in the model as a function of other variables in the model. Identities often describe the behavior of endogenous variables that are based on accounting identities or adding-up conditions. // Add an identity to the forecast that states that y3 is the sum of y1 and y2 forecast identity y3=y1+y2 // create new variable newy before adding it to the forecast forecast identity newy=y1+y2, generate The difference is that if the LHS variable does not exist, you need to specify the option gen. Ex. We have a model using annual data and want to assume that our population variable pop grows at 0.75% per year. Then we can declare endogenous variable pop by using forecast identity: forecast identity pop = 1.0075*L.pop Typically, you use forecast identity to define the relationship that determines an endogenous variable that is already in your dataset. The generate option of forecast identity is useful when you wish to use a transformation of one or more endogenous variables as a right-hand-side variable in a stochastic equation that describes another endogenous variable. 11.4.3.2 Add equations that you obtained elsewhere to your model Up untill now, we have been using model output from Stata to add equations to a forecast model, i.e., using forecast estimates. You use forecast coefvector to add endogenous variables to your model that are defined by linear equations. Common use scenarios of forecast coefvector: Sometimes, you might see the estimated coefficients for an equation in an article and want to add that equation to your model. In this case, forecast coefvector allows you to add equations that are stored as coefficient vectors to a forecast model. User-written estimators that do not implement a predict command can also be included in forecast models via forecast coefvector. forecast coefvector can also be useful in situations where you want to simulate time-series data. forecast coefvector cname [, options ] cname is a Stata matrix with one row. It defines the linear equations, which are stored in a coefficient (parameter) vector. Options: variance(vname): specify parameter variance matrix of the estimated parameters. This option only has an effect if you specify the simulate() option when calling forecast solve and request sim_technique‚Äôs betas or residuals. errorvariance(ename): specify additive error term with variance matrix ename, where ename is the name of s Stata matrix. The number of rows and columns in ename must match the number of equations represented by coefficient vector cname. This option only has an effect if you specify the simulate() option when calling forecast solve and request sim_technique‚Äôs betas or residuals. names(namelist[ , replace ]): instructs forecast coefvector to use namelist as the names of the left-hand-side variables in the coefficient vector being added. By default, forecast coefvector uses the equation names on the column stripe of cname. You must use this option if any of the equation names stored with cname contains time-series operators. You use forecast coefvector to add endogenous variables to your model that are defined by linear equations, where the linear equations are stored in a coefficient (parameter) vector. // Incorporate coefficient vector of the endogenous equation of y to be used by forecast solve forecast coefvector y Ex. We want to add the following eqns to a forecast model. \\[ \\begin{split} x_t &amp;= 0.2 + 0.3 x_{t-1} - 0.8 z_t \\\\ z_t &amp;= 0.1 + 0.7 z_{t-1} + 0.3 x_t - 0.2 x_{t-1} \\end{split} \\] We first define the coefficient vector eqvector. // define a row vector matrix eqvector = (0.2, 0.3, -0.8, 0.1, 0.7, 0.3, -0.2) // add equation names and variale names // equation names are before the colon // variable names are after the colon matrix coleq eqvector = x:_cons x:L.x x:y y:_cons y:L.y y:x y:L.x matrix list eqvector We could then add the coefficient vector to a forecast model. forecast create forecast coefvector y 11.4.3.3 Declare exogenous variables forecast exogenous varlist Declaring exogenous variables with forecast exogenous is not explicitly necessary, but we nevertheless strongly encourage doing so. Stata can check the exogenous variables before solving the model and issue an appropriate error message if missing values are found, whereas troubleshooting models for which forecasting failed is more difficult after the fact. Undeclared exogenous variables that contain missing values within the forecast horizon will cause forecast solve to exit with a less-informative error message and require the user to do more work to pinpoint the problem. Summary: Endogenous variables are added to the forecast model via forecast estimates, forecast identity, and forecast coefvector. Equations added via forecast estimates are always stochastic, while equations added via forecast identity are always nonstochastic. Equations added via forecast coefvector are treated as stochastic if options variance() or errorvariance() (or both) are specified and nonstochastic if neither is specified. 11.4.3.4 forecast adjust forecast adjust adjusts a variable by add factoring, replacing, etc. forecast adjust varname = exp [if] [in] varname is the name of the endogenous variable that has been previously added to the model using forecast estimates or forecast coefvector. forecast adjust specifies an adjustment to be applied to an endogenous variable in the model. Adjustments are typically used to produce alternative forecast scenarios or to incorporate outside information into a model. // Adjust the endogenous variable y in forecast to account for the variable shock in 1990 forecast adjust y = y + shock if year==1990 11.4.4 Solve the foreceast forecast solve computes static or dynamic forecasts based on the model currently in memory. Before you can solve a model, you must first create a new model using forecast create and add equations and variables to it using forecast estimates, forecast coefvector, or forecast identity. forecast solve [, { prefix(string) | suffix(string) } options ] Options: prefix(string) and suffix(string) specify prefix/suffix for forecast variables. You may specify prefix() or suffix() but NOT both. By default, forecast values will be prefixed by f_. begin(time_constant) and end(time_constant) specify period to begin/end forecasting periods(#) specify number of periods to forecast static produce static forecasts instead of dynamic forecasts Actual values of variables are used wherever lagged values of the endogenous variables appear in the model. Static forecasts are also called one-step-ahead forecasts. By default, dynamic forecasts are produced, which use the forecast values of variables wherever lagged values of the endogenous variables appear in the model. actuals use actual values if available instead of forecasts actuals specifies how nonmissing values of endogenous variables in the forecast horizon are treated. By default, nonmissing values are ignored, and forecasts are produced for all endogenous variables. When you specify actuals, forecast sets the forecast values equal to the actual values if they are nonmissing. The forecasts for the other endogenous variables are then conditional on the known values of the endogenous variables with nonmissing data. log(log_level) loglevel takes on one of the following values on: default, provides an iteration log showing the current panel and period for which the model is being solved as well as a sequence of dots for each period indicating the number of iterations. off: suppress the iteration log. detail: a detailed iteration log including the current values of the convergence criteria for each period in each panel (in the case of panel data) for which the model is being solved. brief: produces an iteration log showing the current panel being solved but does not show which period within the current panel is being solved. simulate(sim_technique, sim_statistic sim_options) allows you to simulate your model to obtain measures of uncertainty surrounding the point forecasts produced by the model. Simulating a model involves repeatedly solving the model, each time accounting for the uncertainty associated with the error terms and the estimated coefficient vectors. sim_technique can be betas, errors, or residuals. betas: draw multivariate-normal parameter vectors ‚Üê sampling error from the estimated coefficients errors: draw additive errors from multivariate normal distribution ‚Üê uncertainty from the stochastic error terms; errors drawn from a normal distribution with mean zero and variance equal to the estimated variance of the error terms residuals: draw additive residuals based on static forecast errors; errors drawn from the pool of static-forecast residuals sim_statistic specifies a summary statistic to summarize the forecasts over all the simulations. statistic(statistic, { prefix(string) | suffix(string) }) statistic can be mean, variance, or stddev. You may specify either the prefix or the suffix that will be used to name the variables that will contain the requested statistic. statistic(stddev, prefix(sd_)) This will store the standard deviations of our forecasts in variables prefixed with sd_. sim_options includes reps(#) request that forecast solve perform # replications; default is reps(50) saving(filename, ‚Ä¶) save results to file nodots suppress replication dots. By default, one dot character is displayed for each successful replication. If during a replication convergence is not achieved, forecast solve exits with an error message. 11.4.5 Use example: forecast a panel \\[ \\%\\Delta \\text{dim}_{it} = \\beta_0 + \\beta_1 \\ln(\\text{starts}_{it}) + \\beta_2 \\text{rgspgrowth}_{it} + \\beta_3 \\text{unrate}_{it} + u_{i} + \\varepsilon_{it} \\] \\(u_{i}\\) refers to individual fixed effects. When we make forecasts for any individual panel, we may want to include it in our forecasts. This can be achieved by using forecast adjust. use https://www.stata-press.com/data/r19/statehardware, clear generate lndim = ln(dim) generate lnstarts = ln(starts) quietly xtreg D.lndim lnstarts rgspgrowth unrate if qdate &lt;= tq(2009q4), fe predict dlndim_u, u /* obtain individual fixed effects */ estimates store dim /* store estimation results */ With enough observations, we can have more confidence in the estimated panel-specific errors. If we are willing to assume that we have decent estimates of the panel-specific errors and that those panel-level effects will remain constant over the forecast horizon, then we can incorporate them into our forecasts. Because predict only provided us with estimates of the panel-level effects for the estimation sample, we need to extend them into the forecast horizon. An easy way to do that is to use egen to create a new set of variables: // extend panel fixed effects to the forecast horizon by state: egen dlndim_u2 = mean(dlndim_u) We can use forecast adjust to incorporate these terms into our forecasts. The following commands define our forecast model, including the estimated panel-specific terms: /* create forecast model */ forecast create statemodel, replace /* add equations, rename the endog variable, D.lndim, to be forecasted as dlndim */ /* since the original endog variable name includes a time series operator it is required to name, otherwise will return error */ forecast estimates dim, name(dlndim) /* add state fixed effects */ forecast adjust dlndim = dlndim + dlndim_u2 Note that our dependent variable contains a time series operator, we must use name(dlndim) option of forecast estimates to specify a valid name for the endogenous variable being added. dlndim stands for the first difference of the logarithm of dim. We are interested in the level of dim, so we need to back out dim from dlndim. ‚Üí We use forecast identity to obtain the actual dim variable. // reverse first difference, note that you refer to the endog var using the new name, dlndim, now forecast identity lndim = L.lndim + dlndim // reverse natural logarithm forecast identity dim = exp(lndim) We used forecast adjust to perform our adjustment to dlndim immediately after we added those estimation results so that we would not forget to do so. However, we could specify the adjustment at any time. Regardless of when you specify an adjustment, forecast solve performs those adjustments immediately after the variable being adjusted is computed. Finally we can solve the model. Here we obtain dynamic forecasts beginning in the first quarter of 2010: forecast solve, begin(tq(2010q1)) log(off) "],["11.5-panel-1.html", "11.5 Panel", " 11.5 Panel Declare panel data You must xtset your data before you can use other xt commands. xtset panelvar timevar [, tsoptions] declares the data to be a panel in which the order of observations is relevant. When you specify timevar, you can then use time series operators (e.g., L, D). tsoptions can be specified using unit of timevar, e.g., yearly, quarterly. delta(#) specifies the time increment between observations in timevar units. Example To set a panel dataset: // string variables not allowed in varlist; need to convert them to numeric . egen float country_id = group(iso) . egen float year_id = group(year) . xtset country_id year_id, yearly Panel variable: country_id (strongly balanced) Time variable: year_id, 1 to 59 Delta: 1 year Menu Statistics &gt; Longitudinal/panel data &gt; Setup and utilities &gt; Declare dataset to be panel data panelvar panel variable that identifies the unit timevar optional time variable that identifies the time within panels Use describe to show an overview of data structure. Sometimes numbers will get recorded as string variables, making it impossible to do almost any command. destring [varlist], {gen(newvarlist) | replace} [options] gen(newvarlist) generate new variables for each variable in varlist. replace replace string variables with numeric variables ignore(\"chars\") specifies nonnumeric characters be removed. // from logd_gdp to rad, convert to numeric, replace &quot;NA&quot; with missing destring logd_gdp-rad, replace ignore(`&quot;NA&quot;&#39;) xtreg is Stata‚Äôs feature for fitting linear models for panel data. xtreg, fe estimates the parameters of fixed-effects models: xtreg depvar [indepvars] [if] [in] [weight] , fe [FE_options] Menu: Statistics &gt; Longitudinal/panel data &gt; Linear models &gt; Linear regression (FE, RE, PA, BE, CRE) Options vce(robust) use clustered variance that allows for intragroup correlation within groups. By default, SE uses OLS estimates. "],["11.6-arellano-bond-estimator.html", "11.6 Arellano-Bond Estimator", " 11.6 Arellano-Bond Estimator If an equation did contain a lagged dependent variable, then one could use a dynamic panel-data (DPD) estimator such as xtabond, xtdpd, or xtdpdsys. DPD estimators are designed for cases where the number of observations per panel \\(T\\) is small. As shown by Nickell (1981), the bias of the standard fixed- and random-effects estimators in the presence of lagged dependent variables is of order \\(1/T\\) and is thus particularly severe when each panel has relatively few observations. Judson and Owen (1999) perform Monte Carlo experiments to examine the relative performance of different panel-data estimators in the presence of lagged dependent variables when used with panel datasets having dimensions more commonly encountered in macroeconomic applications. Based on their results, The bias of the standard fixed-effects estimator (LSDV in their notation) is not inconsequential even when \\(T=20.\\) For \\(T=30\\), the fixed-effects estimator does work as well as most alternatives. The only estimator that appreciably outperformed the standard fixed-effects estimator when \\(T=30\\) is the least-squares dummy variable corrected estimator (LSDVC in their notation). Bruno (2005) provides a Stata implementation of that estimator. The Arellano‚ÄìBond estimator is for datasets with many panels and few periods. (Technically, the large-sample properties are derived with the number of panels going to infinity and the number of periods held fixed.) The number of instruments increases quadratically in the number of periods. If your dataset is better described by a framework in which both the number of panels and the number of periods is large, then you should consider other estimators such as xtiveg or xtreg, fe. 11.6.1 xtabond Syntax The Arellano-Bond estimator may be obtained in Stata using either the xtabond or xtdpd command. xtabond fits a linear dynamic panel-data model where the unobserved unit-level effects are correlated with the lags of the dependent variable, known as the Arellano‚ÄìBond estimator. This estimator is designed for datasets with many panels and few periods, and it requires that there be no autocorrelation in the idiosyncratic errors. xtabond uses moment conditions in which lags of the dependent variable and first differences of the exogenous variables are instruments for the first-differenced equation. xtabond depvar [ indepvars ] [ if ] [ in ] [, options ] Estimation Options: noconstant: suppress the constant term. lags(#): #lags of dependent variable as covariates; default is lags(1) maxldep(#): maximum lags of dependent variable for use as instruments. Defaults to \\(T_i-p-2,\\) where \\(p\\) is the number of lags of the dependent variable to be included in the model (i.e., lags(#)). maxlags(#): maximum lags of predetermined and endogenous variables for use as instruments twostep: compute the two-step estimator instead of the one-step estimator pre(varlist): predetermined variables; can be specified more than once endogenous(varlist): endogenous variables; can be specified more than once vce(vcetype) vce(gmm) the default, uses the conventionally derived variance estimator for generalized method of moments estimation. vce(robust): uses the robust estimator. After one-step estimation, this is the Arellano‚ÄìBond robust VCE estimator. After two-step estimation, this is the Windmeijer (2005) WC-robust estimator. Quick Start Arellano‚ÄìBond estimation of y on x1 and x2 using xtset data xtabond y x1 x2 One-step estimator with robust standard errors xtabond y x1 x2, vce(robust) Two-step estimator with bias-corrected robust standard errors xtabond y x1 x2, vce(robust) twostep Arellano‚ÄìBond estimation also including 2 lagged values of y xtabond y x1 x2, lags(2) 11.6.2 Use Example Arellano and Bond (1991) apply their new estimators and test statistics to a model of dynamic labor demand that had previously been considered by Layard and Nickell (1986), using data from an unbalanced panel of firms from the United Kingdom. All variables are indexed over the firm \\(i\\) and time \\(t\\). \\[ \\begin{equation} \\tag{11.1} \\begin{split} n_{i,t} &amp;= \\alpha_1 n_{i,t-1} + \\alpha_2 n_{i,t-2} + \\bbeta&#39;(L) \\bx_{it} + \\lambda_t + \\eta_i + \\varepsilon_{i,t} \\\\ &amp;= \\alpha_1 n_{i,t-1} + \\alpha_2 n_{i,t-2} \\\\ &amp;\\phantom{=}\\quad + \\beta_1 w_{i,t} + \\beta_2 w_{i,t-1} \\\\ &amp;\\phantom{=}\\quad + \\beta_3 k_{i,t} + \\beta_4 k_{i,t-1} + \\beta_5 k_{i,t-2} \\\\ &amp;\\phantom{=}\\quad + \\beta_6 ys_{i,t} + \\beta_7 ys_{i,t-1} + \\beta_8 ys_{i,t-2} \\\\ &amp;\\phantom{=}\\quad + \\gamma_3 d_3 + \\dots + \\gamma_T d_T + \\pi\\, t + \\eta_i + \\varepsilon_{i,t}, \\end{split} \\end{equation} \\] where \\(i=1,\\ldots,n\\) denotes the firm, and \\(t=3,\\ldots,T\\) is the time series dimension. \\(n_{i,t}\\) is the natural logarithm of employment, first and second lagged were used as independent variables \\(w\\) refers to the natural logarithm of wage, up to lag 1 \\(k\\) is the natural logarithm of capital, up to lag 2 \\(ys\\) is the natural logarithm of output, up to lag 2 Variables \\(d_3,\\ldots,d_T\\) are time dummies with corresponding coefficients \\(\\gamma_3,\\ldots,\\gamma_T.\\) \\(t\\) is a time trend with coefficient \\(\\pi.\\) \\(\\eta_i\\) is the unobserved individual-specific effects. \\(\\varepsilon_{i,t}\\) is an idiosyncratic remainder component. Model (11.1) can be implemented using xtabond. // Use example of xtabond use https://www.stata-press.com/data/r19/abdata xtabond n l(0/1).w l(0/2).(k ys) yr1980-yr1984 year, lags(2) vce(robust) noconstant // Equivalent command using xtdpd, need to specify exogenous variables in div() xtdpd L(0/2).n L(0/1).w L(0/2).(k ys) yr1980-yr1984 year, noconstant div(L(0/1).w L(0/2).(k ys) yr1980-yr1984 year) dgmmiv(n) lagged dependent variables n: up to 2 lags lagged independent variables w: up to 1 lag k and ys: up to 2 lags Note that without specifying div(), the default of xtdpd is to use levels of the dependent variables as instruments for the difference equation. By contrast, xtabond by default uses the first difference of all the exogenous variables as standard instruments for the difference equation. To use the same instruments as xtabond, we need to specify div(). The xtabond output would look like the following. . xtabond n l(0/1).w l(0/2).(k ys) yr1980-yr1984 year, lags(2) vce(robust) noconstant Arellano‚ÄìBond dynamic panel-data estimation Number of obs = 611 Group variable: id Number of groups = 140 Time variable: year Obs per group: min = 4 avg = 4.364286 max = 6 Number of instruments = 41 Wald chi2(16) = 1727.45 Prob > chi2 = 0.0000 One-step results (Std. err. adjusted for clustering on id) ------------------------------------------------------------------------------ | Robust n | Coefficient std. err. z P>|z| [95% conf. interval] -------------+---------------------------------------------------------------- n | L1. | .6862261 .1445943 4.75 0.000 .4028266 .9696257 L2. | -.0853582 .0560155 -1.52 0.128 -.1951467 .0244302 | w | --. | -.6078208 .1782055 -3.41 0.001 -.9570972 -.2585445 L1. | .3926237 .1679931 2.34 0.019 .0633632 .7218842 | k | --. | .3568456 .0590203 6.05 0.000 .241168 .4725233 L1. | -.0580012 .0731797 -0.79 0.428 -.2014308 .0854284 L2. | -.0199475 .0327126 -0.61 0.542 -.0840631 .0441681 | ys | --. | .6085073 .1725313 3.53 0.000 .2703522 .9466624 L1. | -.7111651 .2317163 -3.07 0.002 -1.165321 -.2570095 L2. | .1057969 .1412021 0.75 0.454 -.1709542 .382548 | yr1980 | .0029062 .0158028 0.18 0.854 -.0280667 .0338791 yr1981 | -.0404378 .0280582 -1.44 0.150 -.0954307 .0145552 yr1982 | -.0652767 .0365451 -1.79 0.074 -.1369038 .0063503 yr1983 | -.0690928 .047413 -1.46 0.145 -.1620205 .0238348 yr1984 | -.0650302 .0576305 -1.13 0.259 -.1779839 .0479235 year | .0095545 .0102896 0.93 0.353 -.0106127 .0297217 ------------------------------------------------------------------------------ Instruments for differenced equation GMM-type: L(2/.).n Standard: D.w LD.w D.k LD.k L2D.k D.ys LD.ys L2D.ys D.yr1980 D.yr1981 D.yr1982 D.yr1983 D.yr1984 D.year The footer in the output reports the instruments used. The first line indicates that xtabond used lags from 2 on back to create the GMM-type instruments described in Arellano and Bond (1991) and Holtz-Eakin, Newey, and Rosen (1988); also see Methods and formulas in [XT] xtdpd. The second and third lines indicate that the first difference of all the exogenous variables were used as standard instruments. The following is the output of the equivalent command using xtdpd. Without specifying div(). . xtdpd L(0/2).n L(0/1).w L(0/2).(k ys) yr1980-yr1984 year, noconstant dgmmiv(n) Dynamic panel-data estimation Number of obs = 611 Group variable: id Number of groups = 140 Time variable: year Obs per group: min = 4 avg = 4.364286 max = 6 Number of instruments = 27 Wald chi2(16) = 230.72 Prob > chi2 = 0.0000 One-step results ------------------------------------------------------------------------------ n | Coefficient Std. err. z P>|z| [95% conf. interval] -------------+---------------------------------------------------------------- n | L1. | .7210638 .404417 1.78 0.075 -.0715789 1.513707 L2. | .0387296 .2178553 0.18 0.859 -.388259 .4657181 | w | --. | -.3089982 .5679262 -0.54 0.586 -1.422113 .8041166 L1. | .5261578 .5830132 0.90 0.367 -.616527 1.668843 | k | --. | 1.312798 .5597144 2.35 0.019 .2157781 2.409818 L1. | -.2839418 .293703 -0.97 0.334 -.8595891 .2917054 L2. | -.1448001 .2486989 -0.58 0.560 -.6322411 .3426409 | ys | --. | -.2204921 .8476672 -0.26 0.795 -1.881889 1.440905 L1. | -.414458 1.167557 -0.35 0.723 -2.702827 1.873911 L2. | -2.635413 1.152849 -2.29 0.022 -4.894956 -.3758697 | yr1980 | -.0267764 .0721829 -0.37 0.711 -.1682524 .1146995 yr1981 | -.1019741 .1386003 -0.74 0.462 -.3736256 .1696775 yr1982 | -.2540348 .2046535 -1.24 0.214 -.6551483 .1470787 yr1983 | -.4411154 .2789873 -1.58 0.114 -.9879205 .1056897 yr1984 | -.4675579 .3241822 -1.44 0.149 -1.102943 .1678275 year | .0319349 .0421826 0.76 0.449 -.0507414 .1146113 ------------------------------------------------------------------------------ Instruments for differenced equation GMM-type: L(2/.).n With specifying div(). . xtdpd L(0/2).n L(0/1).w L(0/2).(k ys) yr1980-yr1984 year, noconstant div(L(0/1).w L(0/2).(k ys) > yr1980-yr1984 year) dgmmiv(n) Dynamic panel-data estimation Number of obs = 611 Group variable: id Number of groups = 140 Time variable: year Obs per group: min = 4 avg = 4.364286 max = 6 Number of instruments = 41 Wald chi2(16) = 1757.07 Prob > chi2 = 0.0000 One-step results ------------------------------------------------------------------------------ n | Coefficient Std. err. z P>|z| [95% conf. interval] -------------+---------------------------------------------------------------- n | L1. | .6862261 .1486163 4.62 0.000 .3949435 .9775088 L2. | -.0853582 .0444365 -1.92 0.055 -.1724523 .0017358 | w | --. | -.6078208 .0657694 -9.24 0.000 -.7367265 -.4789151 L1. | .3926237 .1092374 3.59 0.000 .1785222 .6067251 | k | --. | .3568456 .0370314 9.64 0.000 .2842653 .4294259 L1. | -.0580012 .0583051 -0.99 0.320 -.172277 .0562747 L2. | -.0199475 .0416274 -0.48 0.632 -.1015357 .0616408 | ys | --. | .6085073 .1345412 4.52 0.000 .3448115 .8722031 L1. | -.7111651 .1844599 -3.86 0.000 -1.0727 -.3496304 L2. | .1057969 .1428568 0.74 0.459 -.1741974 .3857912 | yr1980 | .0029062 .0212705 0.14 0.891 -.0387832 .0445957 yr1981 | -.0404378 .0354707 -1.14 0.254 -.1099591 .0290836 yr1982 | -.0652767 .048209 -1.35 0.176 -.1597646 .0292111 yr1983 | -.0690928 .0627354 -1.10 0.271 -.1920521 .0538664 yr1984 | -.0650302 .0781322 -0.83 0.405 -.2181665 .0881061 year | .0095545 .0142073 0.67 0.501 -.0182912 .0374002 ------------------------------------------------------------------------------ Instruments for differenced equation GMM-type: L(2/.).n Standard: D.w LD.w D.k LD.k L2D.k D.ys LD.ys L2D.ys D.yr1980 D.yr1981 D.yr1982 D.yr1983 D.yr1984 D.year xtdpdsys implements the Arellano‚ÄìBover / Blundell‚ÄìBond system estimator, which includes the lagged differences of n (the dependent variable) as instruments for the level equation. 11.6.3 Diagnostic Tests 11.6.3.1 Test for Autocorrelation The moment conditions of these GMM estimators are valid only if there is no serial correlation in the idiosyncratic errors. Because the first difference of white noise is necessarily autocorrelated, we need only concern ourselves with second and higher autocorrelation. We can use estat abond to test for autocorrelation: . estat abond, artests(4) Arellano‚ÄìBond test for zero autocorrelation in first-differenced errors H0: No autocorrelation Order z Prob &gt; z 1 -4.6414 0.0000 2 -1.0572 0.2904 3 -.19492 0.8455 4 .04472 0.9643 11.6.3.2 Test for Overidentifying Restrictions estat sargan reports the Sargan test of overidentifying restrictions. The moment conditions are valid only if the idiosyncratic errors are i.i.d. We use the Sargan test to test the validity of the moment conditions. The null hypothesis (\\(H_0\\)) is that the overidentifying restrictions are valid, i.e., that the moment conditions are valid. . estat sargan Sargan test of overidentifying restrictions H0: Overidentifying restrictions are valid chi2(25) = 65.81806 Prob &gt; chi2 = 0.0000 Noting that the Sargan test rejects the null hypothesis that the overidentifying restrictions are valid in the model with i.i.d. errors. Now we fit the model using only the moment conditions constructed starting from the third lags as instruments of the first-differenced equation. . xtdpd L(0/2).n L(0/1).w L(0/2).(k ys) yr1980-yr1984 year, &gt; noconstant div(L(0/1).w L(0/2).(k ys) yr1980-yr1984 year) dgmmiv(n, lag(3 .)) dgmmiv(n, lag(3 .) means n is treated as endogenous. use the third and higher lags of n as GMM-type instruments for the first-differenced equation. . means ‚Äúup to the maximum available lag.‚Äù (xtdpd output omitted) . estat sargan Sargan test of overidentifying restrictions H0: Overidentifying restrictions are valid chi2(19) = 23.91962 Prob &gt; chi2 = 0.1993 The results from estat sargan no longer reject the null hypothesis that the overidentifying restrictions are valid. 11.6.4 Predetermined Covariates Sometimes we cannot assume strict exogeneity. Recall that a variable, \\(x_{it}\\), is said to be strictly exogenous if \\(\\E[ùë•_{it}\\varepsilon_{is}] = 0\\) for all \\(t\\) and \\(s\\). If \\(\\E[x_{it}\\varepsilon_{is}] \\ne 0\\) for \\(s &lt; t\\) but \\(\\E[x_{it}\\varepsilon_{is}] = 0\\) for all \\(s\\ge t,\\) the variable is said to be predetermined. Intuitively, if the error term at time \\(t\\) has some feedback on the subsequent realizations of \\(x_{it},\\) \\(x_{it}\\) is a predetermined variable. Because unforecastable errors today might affect future changes in the real wage and in the capital stock, we might suspect that the log of the real product wage and the log of the gross capital stock are predetermined instead of strictly exogenous. We also call predetermined \\(x_{it}\\) as sequential exogenous. Here we treat \\(w\\) and \\(k\\) as predetermined and use lagged levels as instruments. xtabond n l(0/1).ys yr1980-yr1984 year, lags(2) twostep pre(w, lag(1,.)) pre(k, lag(2,.)) noconstant vce(robust) We are now including GMM-type instruments from the first lag of L.w on back and from the first lag of L2.k on back. pre(w, lag(1, .)) to mean that L.w is a predetermined variable and pre(k, lag(2, .)) to mean that L2.k is a predetermined variable. 11.6.5 Endogenous Covariates We might instead suspect that \\(w\\) and \\(k\\) are endogenous in that \\(\\E[x_{it}\\varepsilon_{is}] \\ne 0\\) for \\(s \\le t\\) but \\(\\E[x_{it}\\varepsilon_{is}] = 0\\) for all \\(s &gt; t.\\) By this definition, endogenous variables differ from predetermined variables only in that the endogenous variables allow for correlation between \\(x_{it}\\) and \\(\\varepsilon_{it}\\) at time \\(t,\\) whereas Endogenous variables are treated similarly to the lagged dependent variable. Levels of the endogenous variables lagged two or more periods can serve as instruments. predetermined variables do NOT allow for contemporaneous correlation. In this example, we treat \\(w\\) and \\(k\\) as endogenous variables. xtabond n l(0/1).ys yr1980-yr1984 year, lags(2) twostep endogenous(w, lag(1,.)) endogenous(k, lag(2,.)) noconstant vce(robust) Although some estimated coefficients changed in magnitude, none changed in sign, and these results are similar to those obtained by treating \\(w\\) and \\(k\\) as predetermined. Prefix xtabond with xi: if you need to include factor variables // AB estimator with factor variables xi: xtabond logd_gdp tmp tmp2 pre pre2 /// tmp_pre tmp2_pre tmp_pre2 tmp2_pre2 /// i.year i.iso|year_id i.iso|year_id2, /// lags(1) vce(robust) i.year: time fixed effects; i.iso|year_id: country-specific linear time trends i.iso|year_id2: country-specific quadratic time trends Forecast under xtabond: https://www.stata.com/stata-news/news29-3/forecast/ 11.6.6 xtabond2 xtabond2 was written by David Roodman. More versatile than xtabond. xtabond xtabond2 Not support factor variablesCan be fixed with xi: xtabond Support factor variables xtabond2 depvar varlist [if exp] [in range] [weight] [, level(#) svmat svvar twostep robust cluster(varlist) noconstant small gmmopt [gmmopt ...] ivopt [ivopt ...]] Options: level(#) confidence level, default to level(95) gmmopt gmmstyle(varlist [, laglimits(# #) collapse orthogonal equation({diff | level | both}) passthru split]) gmmstyle specifies a set of variables to be used as bases for ‚ÄúGMM-style‚Äù instrument sets described in Holtz-Eakin, Newey, and Rosen (1988) and Arellano and Bond (1991). By default xtabond2 uses, for each time period, all available lags of the specified variables in levels dated \\(t-1\\) or earlier as instruments for the transformed equation; and uses the contemporaneous first differences as instruments in the levels equation. These defaults are appropriate for predetermined variables that are not strictly exogenous (Bond 2000). Missing values are always replaced by zeros. Since the gmmstyle() varlist allows time-series operators, there are many routes to the same specification. E.g., gmm(w, lag(2 .)), the standard treatment for an endogenous variable, is equivalent to gmm(L.w, lag(1 .)), thus gmm(L.w). ivopt ivstyle(varlist [, equation({diff | level | both}) passthru mz]) ivstyle specifies a set of variables to serve as standard instruments, with one column in the instrument matrix per variable. Normally, strictly exogenous regressors are included in ivstyle options, in order to enter the instrument matrix, as well as being listed before the main comma of the command line. The equation() suboption specifies which equation(s) should use the instruments: equation(diff): first-difference only equation(level): levels only equation(both): both, default A matching triplet using different pkgs to achieve the same results (pp42, Stata Journal article by David Roodman) xtabond n, lags(1) pre(w, lagstruct(1,.)) pre(k, endog) robust xtdpd n L.n w L.w k, dgmmiv(w k n) vce(robust) xtabond2 n L.n w L.w k, gmmstyle(L.(w n k), eq(diff)) robust https://www.statalist.org/forums/forum/general-stata-discussion/general/1548924-xtabond-vs-xtabond2-how-to-get-the-same-results 11.6.7 xtdpd Linear Dynamic panel data (DPD) estimation fits dynamic panel-data models by using the Arellano‚ÄìBond or also known as the ‚ÄúDifference GMM‚Äù estimator. Implemented in xtabond. the Arellano‚ÄìBover/Blundell‚ÄìBond system estimator aka, ‚ÄúSystem GMM‚Äù estimator. Implemented in xtdpdsys. xtdpd can fit more complex models at the cost of a more complicated syntax. That the idiosyncratic errors follow a low-order MA process and that the predetermined variables have a more complicated structure than accommodated by xtabond and xtdpdsys are two common reasons for using xtdpd instead of xtabond or xtdpdsys. Syntax A panel variable and a time variable must be specified; use xtset. xtdpd depvar [ indepvars ] [ if ] [ in ], dgmmiv(varlist [...]) [ options ] Options: dgmmiv(varlist [, lagrange(flag [llag])]) specifies GMM-type instruments for the difference equation; this is a mandatory option. Levels of the variables are used to form GMM-type instruments for the difference equation. All possible lags are used, unless lagrange(flag llag) restricts the lags: begin with flag and end with llag. div(varlist [, nodifference]) specifies additional standard instruments for the difference equation. Differences of the variables are used, unless nodifference is specified, which requests that levels of the variables be used as instruments for the difference equation. Support time-series operators, e.g., L. and D.. See help tsvarlist for more information. depvar, indepvars, and all varlists may contain time-series operators; Prefix commands: by, collect, statsby, and xi are allowed. Use help prefix commands for more information. vce(vcetype) specifies the type of standard error reported. Two options: vce(gmm): the default; the conventionally derived variance estimator for generalized method of moments estimation. vce(robust): the robust estimator. After one-step estimation, this is the Arellano‚ÄìBond robust VCE estimator. After two-step estimation, this is the Windmeijer (2005) WC-robust estimator. Refer to Use Example for an illustration of Difference GMM estimation. 11.6.8 System GMM Estimation Consider the following dynamic panel-data model: \\[ y_{it} = \\sum_{j=1}^p \\alpha_j y_{i,t-j} + \\bbeta&#39; \\bx_{it} + \\nu_i + \\varepsilon_{it} . \\] \\(\\nu_i\\) are the panel-level effects, which may be correlated with \\(\\bx_{it}\\) \\(\\varepsilon_{it}\\) are i.i.d. or come from a low-order moving-average process, with variance \\(\\sigma^2_\\varepsilon\\). Building on the work of Arellano and Bover (1995), Blundell and Bond (1998) proposed a system estimator that uses moment conditions in which lagged differences are used as instruments for the level equation in addition to the moment conditions of lagged levels as instruments for the difference equation. The additional moment conditions are valid only if the initial condition \\[ \\E[\\nu_i \\Delta y_{i2}] = 0 \\] holds for all \\(i\\); see Blundell and Bond (1998) and Blundell, Bond, and Windmeijer (2000). Relevant Options for system GMM estimation: lgmmiv(varlist [, lag(#)]) specifies GMM-type instruments for the level equation. Differences of the variables are used to form GMM-type instruments for the level equation. The default is to the first lag of the differences. If lag(#) is specified, then the #th lag of the differences will be used. liv(varlist) specifies additional standard instruments for the level equation. Levels of the variables are used as instruments for the level equation. // System estimator xtdpd L(0/1).n L(0/2).(w k) yr1980-yr1984 year, div(L(0/1).(w k) yr1980-yr1984 year) dgmmiv(n) lgmmiv(n) hascons (output omitted) . estat sargan Sargan test of overidentifying restrictions H0: Overidentifying restrictions are valid chi2(31) = 59.22907 Prob &gt; chi2 = 0.0017 We note that the Sargan test rejects the null hypothesis after fitting the model with i.i.d. errors. Now we fit the model using the additional moment conditions constructed from the second lag of n as an instrument for the level equation. xtdpd L(0/1).n L(0/2).(w k) yr1980-yr1984 year, div(L(0/1).(w k) yr1980-yr1984 year) dgmmiv(n, lag(3 .)) lgmmiv(n, lag(2)) hascons Output . xtdpd L(0/1).n L(0/2).(w k) yr1980-yr1984 year, > div(L(0/1).(w k) yr1980-yr1984 year) dgmmiv(n, lag(3 .)) lgmmiv(n, lag(2)) > hascons Dynamic panel-data estimation Number of obs = 751 Group variable: id Number of groups = 140 Time variable: year Obs per group: min = 5 avg = 5.364286 max = 7 Number of instruments = 38 Wald chi2(13) = 3680.01 Prob > chi2 = 0.0000 One-step results ------------------------------------------------------------------------------ n | Coefficient Std. err. z P>|z| [95% conf. interval] -------------+---------------------------------------------------------------- n | L1. | .9603675 .095608 10.04 0.000 .7729794 1.147756 | w | --. | -.5433987 .068835 -7.89 0.000 -.6783128 -.4084845 L1. | .4356183 .0881727 4.94 0.000 .262803 .6084336 L2. | -.2785721 .1115061 -2.50 0.012 -.4971201 -.0600241 | k | --. | .3139331 .0419054 7.49 0.000 .2317999 .3960662 L1. | -.160103 .0546915 -2.93 0.003 -.2672963 -.0529096 L2. | -.1295766 .0507752 -2.55 0.011 -.2290943 -.030059 | yr1980 | -.0200704 .0248954 -0.81 0.420 -.0688644 .0287236 yr1981 | -.0425838 .0422155 -1.01 0.313 -.1253246 .040157 yr1982 | .0048723 .0600938 0.08 0.935 -.1129093 .122654 yr1983 | .0458978 .0785687 0.58 0.559 -.1080941 .1998897 yr1984 | .0633219 .1026188 0.62 0.537 -.1378074 .2644511 year | -.0075599 .019059 -0.40 0.692 -.0449148 .029795 _cons | 16.20856 38.00619 0.43 0.670 -58.28221 90.69932 ------------------------------------------------------------------------------ Instruments for differenced equation GMM-type: L(3/.).n Standard: D.w LD.w D.k LD.k D.yr1980 D.yr1981 D.yr1982 D.yr1983 D.yr1984 D.year Instruments for level equation GMM-type: L2D.n Standard: _cons The estimate of the coefficient on L.n is now 0.96. Blundell, Bond, and Windmeijer (2000, 63‚Äì65) showthat the moment conditions in the system estimator remain informative as the true coefficient on L.n approaches unity. Holtz-Eakin, Newey, and Rosen (1988) show that because the large-sample distribution of the estimator is derived for fixed number of periods and a growing number of individuals there is no ‚Äúunit-root‚Äù problem. . estat sargan Sargan test of overidentifying restrictions H0: Overidentifying restrictions are valid chi2(24) = 27.22585 Prob &gt; chi2 = 0.2940 The results from estat sargan no longer reject the null hypothesis that the overidentifying restrictions are valid. References Arellano, Manuel, and Stephen Bond. 1991. ‚ÄúSome Tests of Specification for Panel Carlo Application to Data: Monte Carlo Evidence and an Application to Employment Equations.‚Äù Review of Economic Studies 58: 277‚Äì97. Bruno, Giovanni S. F. 2005. ‚ÄúEstimation and Inference in Dynamic Unbalanced Panel-Data Models with a Small Number of Individuals.‚Äù The Stata Journal 5: 473‚Äì500. https://www.stata-journal.com/article.html?article=st0091. Judson, Ruth A., and Ann L. Owen. 1999. ‚ÄúEstimating Dynamic Panel Data Models: A Guide for Macroeconomists.‚Äù Economics Letters 65 (October): 9‚Äì15. https://doi.org/10.1016/S0165-1765(99)00130-5. Nickell, Stephen. 1981. ‚ÄúBiases In Dynamic Models With Fixed Effects.‚Äù Econometrica 49 (6): 1417‚Äì26. "],["11.7-common-correlated-effects-models.html", "11.7 Common Correlated Effects Models", " 11.7 Common Correlated Effects Models This section provides an overview of how Stata‚Äôs xtdcce2 package implements Common Correlated Effects (CCE) models, which are useful for panel data analysis with heterogeneous coefficients and common correlated effects. Environment setup ssc install xtdcce2 // dependencies for xtdcce2 ssc install moremata ssc install xtcd2 Data has to be xtset before using xtdcce2. 11.7.1 Econometric model ARDL(1, 1) model with heterogeneous coefficients and common correlated effects (CCE) is given by: \\[ \\begin{equation} \\begin{split} y(i,t) &amp;= b0(i) + b1(i) * y(i,t-1) + b2(i) * x(i,t) + b3(i) * x(i,t-1) \\\\ &amp;\\phantom{=}\\quad + u(i,t) \\end{split} \\tag{11.2} \\end{equation} \\] where \\[ u(i,t) = g(i) * f(t) + e(i,t) \\] f(t) is an unobserved common factor loading, g(i) a heterogeneous factor loading, x(i,t) is a (1 x K) vector and b2(i) and b3(i) the coefficient vectors. It is assumed that x(i,t) is strictly exogenous. The error e(i,t) is iid. The heterogeneous coefficients b1(i), b2(i) and b3(i) are randomly distributed around a common mean. In the case of a static panel model, we have b1(i) = 0. 11.7.2 Estimation 11.7.2.1 Static Pesaran (2006) shows that the averages of the coefficients b0, b2 and b3 (for example for b2(mg) = 1/N sum(b2(i))) can be consistently estimated by adding cross sectional means of the dependent and all independent variables. The default equation in xtdcce2 is given by: \\[ \\begin{equation} y(i,t) = b0(i) + b2(i)*x(i,t) + d(i)*z(i,t) + e(i,t). \\tag{11.3} \\end{equation} \\] Note that Eq. (11.3) is a static model, the lagged dependent variable does not occur and only contemporaneous cross sectional averages are used. Including the dependent and independent variables in crosssectional() and setting cr_lags(0) leads to the same result. cr_lags(0) means that only contemporaneous cross sectional means are included. crosssectional() defines the variables to be included in z(i,t). Important to notice is, that b1(i) is set to zero. Example xtdcce2 d.log_rgdpo log_hc log_ck log_ngd , cr(_all) reportc cr(_all) means that all variables are included in the cross sectional means. It is equivalent to crosssectional(log_rgdpo log_hc log_ck log_ngd). The default number of cross sectional lags is zero (cr_lags(0)), implying only contemporaneous cross sectional averages are used. cr_lags(3) would include the lags of cross sectional means up to three. reportc reports the constant term. If not specified the constant is partialled out. 11.7.2.2 Dynamic Chudik and Pesaran (2015) extends to a dynamic panel data model (b1(i) != 0); pT lags of the cross sectional means are added to achieve consistency. The mean group estimates for b1, b2 and b3 are consistently estimated as long as N, T and pT go to infinity. This implies that the number of cross sectional units and time periods is assumed to grow with the same rate. In an empirical setting this can be interpreted as N/T being constant. A dataset with one dimension being large in comparison to the other would lead to inconsistent estimates, even if both dimension are large in numbers. Stata estimates the following dynamic CCE model: \\[ \\begin{equation} \\begin{split} y(i,t) &amp;= b0(i) + b1(i)*y(i,t-1) + b2(i)*x(i,t) \\\\ &amp;\\phantom{=}\\quad + \\sum_{s=t}^{t-pT} [d(i)*z(i,s)] + e(i,t). \\end{split} \\tag{11.4} \\end{equation} \\] Eq. (11.4) is estimated if the option¬†cr_lags() contains a positive number. z(i,s) is the cross sectional average of the variables defined in crosssectional(). Example xtdcce2 d.log_rgdpo L.log_rgdpo log_hc log_ck log_ngd , /// reportc cr(log_rgdpo log_hc log_ck log_ngd) cr_lags(3) cr_lags(3) the number of lags is set to 3. The variance of the mean group coefficient b1(mg) is estimated as: \\[ \\var(b1(mg)) = \\frac{1}{N} \\sum_{i=1}^N \\left(b1(i) - b1(mg)\\right)^2 \\] If the vector \\(pi(mg) = \\left(b0(mg), b1(mg)\\right)&#39;,\\) the variance is given by: \\[ \\var(pi(mg)) = \\frac{1}{N} \\sum_{i=1}^N \\left(pi(i) - pi(mg)\\right) \\; \\left(p(i)-pi(mg)\\right)&#39; \\] 11.7.2.3 Pooled Estimation Eqs (11.3) and (11.4) can be estimated as a pooled model where the coefficients are assumed to be equal across all cross sectional units. Hence the equations become: Pooled Pesaran \\[ \\begin{equation} y(i,t) = b0 + b2*x(i,t) + d(i)*z(i,t) + e(i,t) \\end{equation} \\] Pooled Chudik and Pesaran \\[ \\begin{equation} y(i,t) = b0 + b1*y(i,t-1) + b2*x(i,t) + \\sum_{s=t}^{t-pT} [d(i)*z(i,s)] + e(i,t). \\end{equation} \\] Variables with pooled (homogenous) coefficients are specified using the pooled(varlist) option. The constant is pooled by using the option pooledconstant. In case of a pooled estimation, the standard errors are obtained from a mean group regression. Example xtdcce2 d.log_rgdpo L.log_rgdpo log_hc log_ck log_ngd , /// reportc cr(log_rgdpo log_hc log_ck log_ngd) /// pooled(L.log_rgdpo log_hc log_ck log_ngd) cr_lags(3) pooledconstant pooled(L.log_rgdpo log_hc log_ck log_ngd) means all coefficients should be pooled. pooledconstant means the constant is pooled. 11.7.3 Long run effects 11.7.4 Boostrap xtdcce2 can bootstrap confidence intervals and standard errors. It supports two types of bootstraps: the wild bootstrap and the cross-section bootstrap. The cross-section bootstrap is the default method. The cross-section bootstrap draws with replacement from the cross-sectional dimension. That is it draws randomly cross-sectional units with their entire time series. It then estimates the model using xtdcce2. The wild bootstrap is a slower from of the wild bootstrap implemented in boottest (Roodman et. al.¬†2019). It reweighs the residuals with Rademacher weights from the initial regression, recalculates the dependent variable and then runs xtdcce2. refs: Estimating Dynamic Common Correlated Effects Models in Stata, xtdcce2 xtdcce2 GitHub Repo Arellano, Manuel, and Stephen Bond. 1991. ‚ÄúSome Tests of Specification for Panel Carlo Application to Data: Monte Carlo Evidence and an Application to Employment Equations.‚Äù Review of Economic Studies 58: 277‚Äì97. Bruno, Giovanni S. F. 2005. ‚ÄúEstimation and Inference in Dynamic Unbalanced Panel-Data Models with a Small Number of Individuals.‚Äù The Stata Journal 5: 473‚Äì500. https://www.stata-journal.com/article.html?article=st0091. Judson, Ruth A., and Ann L. Owen. 1999. ‚ÄúEstimating Dynamic Panel Data Models: A Guide for Macroeconomists.‚Äù Economics Letters 65 (October): 9‚Äì15. https://doi.org/10.1016/S0165-1765(99)00130-5. Nickell, Stephen. 1981. ‚ÄúBiases In Dynamic Models With Fixed Effects.‚Äù Econometrica 49 (6): 1417‚Äì26. "]]
